{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sandrun","text":"<p>Anonymous, ephemeral, sandboxed code execution service</p> <p>Sandrun provides secure, isolated code execution without user accounts or data persistence. Submit code, get results, everything auto-deletes. Simple, private, secure.</p>"},{"location":"#what-is-sandrun","title":"What is Sandrun?","text":"<p>Sandrun is a lightweight batch job execution system that runs untrusted code in hardware-enforced isolation. Built on Linux security primitives (namespaces, seccomp-BPF, cgroups), it provides:</p> <ul> <li>True anonymity: No signup, no authentication, no user tracking</li> <li>Complete isolation: Each job runs in its own sandbox with restricted syscalls</li> <li>Automatic cleanup: All data lives in RAM and auto-deletes after use</li> <li>Fair resource sharing: IP-based rate limiting prevents abuse while ensuring access</li> <li>Multi-language support: Python, JavaScript, Bash, Ruby, R, and more</li> <li>Production-ready: Distributed execution via pool coordinator, signed results, WebSocket streaming</li> </ul> <p>Perfect for ephemeral compute needs, privacy-preserving analysis, untrusted code execution, and distributed batch processing.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#privacy-security","title":"Privacy &amp; Security","text":"<ul> <li>\ud83d\udd12 Hardware-enforced Sandbox - Linux namespaces, seccomp-BPF syscall filtering, capability dropping</li> <li>\ud83c\udfad Anonymous by Design - No accounts, no tracking, IP-based quotas only</li> <li>\ud83d\udca8 Memory-only Execution - All data in tmpfs (RAM), never touches disk</li> <li>\ud83d\udd10 Cryptographic Verification - Ed25519 signatures for worker identity and result authenticity</li> <li>\ud83d\udeaa No Network Access - Jobs are completely airgapped during execution</li> </ul>"},{"location":"#developer-experience","title":"Developer Experience","text":"<ul> <li>\ud83d\ude80 Fast &amp; Lightweight - Native C++ implementation, &lt;10ms startup time, minimal overhead</li> <li>\ud83c\udf10 Multi-language - Python, Node.js, Bash, Ruby, R, and more</li> <li>\ud83d\udce6 Multi-file Projects - Submit tarballs with dependencies, auto-install requirements</li> <li>\u26a1 Simple REST API - Submit via HTTP POST, poll for status, download outputs</li> <li>\ud83c\udfaf Pre-built Environments - ml-basic, vision, nlp, data-science templates with common packages</li> </ul>"},{"location":"#scalability-integration","title":"Scalability &amp; Integration","text":"<ul> <li>\ud83d\udd17 Pool Coordinator - Distribute jobs across multiple workers with health checking and load balancing</li> <li>\ud83d\udce1 WebSocket Streaming - Real-time log streaming during execution</li> <li>\ud83e\udd16 MCP Server Integration - Give Claude and other LLMs safe code execution capabilities</li> <li>\ud83d\udcca Job Broker - SQLite-backed job queue for distributed compute</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Install dependencies\nsudo apt-get install libseccomp-dev libcap-dev libssl-dev\n\n# Build\ncmake -B build\ncmake --build build\n\n# Run server (requires root for namespaces)\nsudo ./build/sandrun --port 8443\n</code></pre>"},{"location":"#submit-your-first-job","title":"Submit Your First Job","text":"<pre><code># Create a simple Python script\necho 'print(\"Hello, Sandrun!\")' &gt; hello.py\n\n# Package and submit\ntar czf job.tar.gz hello.py\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F 'manifest={\"entrypoint\":\"hello.py\",\"interpreter\":\"python3\"}'\n\n# Response:\n# {\"job_id\":\"job-abc123\",\"status\":\"queued\"}\n\n# Check status\ncurl http://localhost:8443/status/job-abc123\n\n# Get logs\ncurl http://localhost:8443/logs/job-abc123\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#for-individuals","title":"For Individuals","text":"<ul> <li>Privacy-Preserving Analysis - Process sensitive data without leaving traces</li> <li>Testing Untrusted Code - Run code from the internet safely in isolation</li> <li>Quick Prototyping - Execute code snippets without local environment setup</li> <li>Learning &amp; Education - Experiment with code in a safe sandbox</li> </ul>"},{"location":"#for-teams-organizations","title":"For Teams &amp; Organizations","text":"<ul> <li>Distributed Batch Processing - Scale compute across multiple workers</li> <li>CI/CD Pipeline Integration - Execute test suites or build jobs in isolated environments</li> <li>Anonymous Job Submission - Process user-submitted code without authentication overhead</li> <li>Multi-tenant Compute - Provide code execution as a service with strong isolation</li> </ul>"},{"location":"#for-ai-llm-integration","title":"For AI &amp; LLM Integration","text":"<ul> <li>AI Code Execution - Let LLMs like Claude execute code safely via MCP protocol</li> <li>Data Analysis Workflows - Enable AI assistants to run pandas, numpy, or R code</li> <li>Automated Testing - LLMs can test their own generated code</li> </ul>"},{"location":"#real-world-examples","title":"Real-World Examples","text":"<p>Quick Python Script</p> <pre><code># Create a simple analysis script\necho 'import statistics; print(statistics.mean([1,2,3,4,5]))' &gt; analyze.py\n\n# Package and submit\ntar czf job.tar.gz analyze.py\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F 'manifest={\"entrypoint\":\"analyze.py\",\"interpreter\":\"python3\"}'\n\n# Output: {\"job_id\":\"job-abc123\",\"status\":\"queued\"}\n</code></pre> <p>Multi-file ML Project</p> <pre><code># Project structure:\n# project/\n# \u251c\u2500\u2500 train.py\n# \u251c\u2500\u2500 model.py\n# \u251c\u2500\u2500 data.csv\n# \u2514\u2500\u2500 requirements.txt\n\ntar czf project.tar.gz -C project .\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@project.tar.gz\" \\\n  -F 'manifest={\n    \"entrypoint\":\"train.py\",\n    \"interpreter\":\"python3\",\n    \"requirements\":\"requirements.txt\",\n    \"outputs\":[\"model.pkl\",\"metrics.json\"],\n    \"timeout\":600,\n    \"memory_mb\":1024\n  }'\n</code></pre> <p>Real-time Log Streaming</p> <pre><code>// Watch logs in real-time via WebSocket\nconst ws = new WebSocket('ws://localhost:8443/logs/job-abc123/stream');\nws.onmessage = (event) =&gt; {\n  console.log(event.data);  // Prints logs as they happen\n};\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[Client] --&gt;|HTTP POST| B[Sandrun Server]\n    B --&gt;|Queue| C[Job Queue]\n    C --&gt;|Execute| D[Sandbox]\n    D --&gt;|Isolated| E[Linux Namespaces]\n    D --&gt;|Filter| F[Seccomp-BPF]\n    D --&gt;|Limit| G[Cgroups]\n    D --&gt;|Results| H[tmpfs RAM]\n    H --&gt;|Download| A\n    H --&gt;|Auto-delete| I[Cleanup]\n</code></pre> <p>Core Components:</p> <ul> <li>HTTP Server: Minimal C++ HTTP/1.1 server with multipart upload support</li> <li>Job Queue: FIFO queue with IP-based rate limiting and priority handling</li> <li>Sandbox: Linux namespace isolation with seccomp syscall filtering</li> <li>Resource Manager: Cgroup-based CPU/memory limits with automatic enforcement</li> <li>Worker Identity: Ed25519 cryptographic signing for result verification</li> <li>Pool Coordinator: Distributed job routing across trusted worker nodes</li> </ul> <p>Learn more about the architecture \u2192</p>"},{"location":"#security-model","title":"Security Model","text":"<p>Security Boundaries</p> <p>Sandrun provides strong isolation for untrusted code execution but relies on the Linux kernel. Always run on updated systems with kernel security patches.</p> <p>What Sandrun Protects Against: - Filesystem access outside the sandbox - Network access from jobs - Process interference between jobs - Memory disclosure between jobs - Persistence of job data after completion</p> <p>What Sandrun Does NOT Protect Against: - Kernel vulnerabilities (keep your kernel updated!) - Timing attacks between concurrent jobs - Covert channels via CPU cache timing - Physical hardware attacks</p> <p>Read the full security model \u2192</p>"},{"location":"#performance-characteristics","title":"Performance Characteristics","text":"Metric Value Job startup time &lt;10ms Overhead per job &lt;1% CPU Memory footprint 10MB (server) + 512MB (per job) Throughput 100+ jobs/second API latency &lt;50ms (submit/status)"},{"location":"#next-steps","title":"Next Steps","text":""},{"location":"#new-users","title":"New Users","text":"<ol> <li>Getting Started Guide - Install, configure, and run your first job</li> <li>Distributed Data Pipeline Tutorial - Hands-on tutorial from basics to advanced</li> <li>API Reference - Complete REST API documentation</li> <li>Job Manifest - Configure job execution parameters</li> </ol>"},{"location":"#advanced-usage","title":"Advanced Usage","text":"<ol> <li>Trusted Pool - Distribute jobs across multiple workers</li> <li>MCP Server - Give Claude AI safe code execution</li> <li>Job Broker - SQLite-backed distributed job queue</li> </ol>"},{"location":"#developers","title":"Developers","text":"<ol> <li>Architecture - Understand the design and security model</li> <li>Building - Compile from source and contribute</li> <li>Testing - Run tests and verify functionality</li> </ol>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: github.com/yourusername/sandrun</li> <li>Issues: Report bugs, request features, ask questions</li> <li>Discussions: Share use cases and best practices</li> <li>Contributions: Pull requests welcome! See CONTRIBUTING.md</li> </ul>"},{"location":"#license","title":"License","text":"<p>Sandrun is open source under the MIT License.</p> <p>Free to use, modify, and distribute - See LICENSE for details.</p> <p>Ready to get started? Install Sandrun \u2192</p>"},{"location":"CONTRIBUTING_TO_DOCS/","title":"Sandrun Documentation","text":"<p>This directory contains the source files for Sandrun's documentation, built using MkDocs with the Material theme.</p>"},{"location":"CONTRIBUTING_TO_DOCS/#quick-start","title":"Quick Start","text":""},{"location":"CONTRIBUTING_TO_DOCS/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install mkdocs-material mkdocs-minify-plugin\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#local-development","title":"Local Development","text":"<pre><code># Serve documentation locally with live reload\nmkdocs serve\n\n# Open in browser: http://127.0.0.1:8000\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#build-documentation","title":"Build Documentation","text":"<pre><code># Build static site\nmkdocs build\n\n# Output: site/ directory\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code># One-time deploy\nmkdocs gh-deploy\n\n# Or use automated deployment via GitHub Actions\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                    # Homepage\n\u251c\u2500\u2500 getting-started.md          # Installation and first job\n\u251c\u2500\u2500 api-reference.md            # REST API documentation\n\u251c\u2500\u2500 job-manifest.md             # Job configuration reference\n\u251c\u2500\u2500 architecture.md             # System design and internals\n\u251c\u2500\u2500 faq.md                      # Frequently asked questions\n\u251c\u2500\u2500 security.md                 # Security guide and best practices\n\u251c\u2500\u2500 troubleshooting.md          # Common issues and solutions\n\u2502\n\u251c\u2500\u2500 integrations/               # Integration guides\n\u2502   \u251c\u2500\u2500 trusted-pool.md        # Pool coordinator\n\u2502   \u251c\u2500\u2500 broker.md              # Job broker\n\u2502   \u2514\u2500\u2500 mcp-server.md          # Claude AI integration\n\u2502\n\u251c\u2500\u2500 development/                # Developer guides\n\u2502   \u251c\u2500\u2500 building.md            # Build from source\n\u2502   \u2514\u2500\u2500 testing.md             # Testing guide\n\u2502\n\u2514\u2500\u2500 images/                     # Diagrams and screenshots\n    \u2514\u2500\u2500 (add your images here)\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"CONTRIBUTING_TO_DOCS/#writing-style","title":"Writing Style","text":"<ul> <li>Use clear, concise language</li> <li>Write in second person (\"you can\", not \"one can\")</li> <li>Use active voice where possible</li> <li>Include code examples for all features</li> <li>Add warnings for security-critical information</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#code-examples","title":"Code Examples","text":"<p>Use syntax highlighting:</p> <pre><code>```python\nimport requests\nresponse = requests.get('http://localhost:8443/')\n```\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#admonitions","title":"Admonitions","text":"<p>Use Material theme admonitions for important information:</p> <pre><code>!!! tip \"Performance Tip\"\n    Use environment templates to avoid installing dependencies every time.\n\n!!! warning \"Security Warning\"\n    Always keep your kernel updated for security patches.\n\n!!! example \"Example: Submit Job\"\n    ```bash\n    curl -X POST http://localhost:8443/submit \\\n      -F \"files=@job.tar.gz\"\n    ```\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#tables","title":"Tables","text":"<p>Use tables for structured data:</p> <pre><code>| Feature | Value |\n|---------|-------|\n| CPU Quota | 10 seconds/minute |\n| Memory | 512MB per job |\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#links","title":"Links","text":"<p>Internal links (relative): <pre><code>See the [API Reference](api-reference.md) for details.\n</code></pre></p> <p>External links: <pre><code>Check out [MkDocs](https://www.mkdocs.org/) for more info.\n</code></pre></p>"},{"location":"CONTRIBUTING_TO_DOCS/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create the Markdown file in <code>docs/</code></li> <li>Add to navigation in <code>mkdocs.yml</code>:    <pre><code>nav:\n  - New Page: new-page.md\n</code></pre></li> <li>Test locally: <code>mkdocs serve</code></li> <li>Commit and push</li> </ol>"},{"location":"CONTRIBUTING_TO_DOCS/#using-diagrams","title":"Using Diagrams","text":""},{"location":"CONTRIBUTING_TO_DOCS/#mermaid-diagrams","title":"Mermaid Diagrams","text":"<p>Mermaid is supported out of the box:</p> <pre><code>```mermaid\ngraph LR\n    A[Client] --&gt; B[Server]\n    B --&gt; C[Sandbox]\n```\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#static-images","title":"Static Images","text":"<ol> <li>Add images to <code>docs/images/</code></li> <li>Reference in Markdown:    <pre><code>![Architecture Diagram](images/architecture.svg)\n</code></pre></li> </ol> <p>See the repository root for <code>DIAGRAM_SUGGESTIONS.md</code> with diagram ideas.</p>"},{"location":"CONTRIBUTING_TO_DOCS/#configuration","title":"Configuration","text":"<p>Documentation configuration is in <code>mkdocs.yml</code>:</p> <ul> <li>Theme settings: Material theme customization</li> <li>Navigation: Page ordering and structure</li> <li>Plugins: Search, minify, etc.</li> <li>Extensions: Markdown extensions enabled</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#testing-documentation","title":"Testing Documentation","text":""},{"location":"CONTRIBUTING_TO_DOCS/#check-for-broken-links","title":"Check for Broken Links","text":"<pre><code># Build in strict mode (fails on warnings)\nmkdocs build --strict\n\n# Check for broken links\nfind docs/ -name \"*.md\" -exec grep -H \"](.*)\" {} \\; | grep \"http\"\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#preview-on-mobile","title":"Preview on Mobile","text":"<pre><code># Serve on all interfaces\nmkdocs serve -a 0.0.0.0:8000\n\n# Access from mobile on same network\n# http://&lt;your-ip&gt;:8000\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#automated-deployment","title":"Automated Deployment","text":"<p>Documentation is automatically deployed via GitHub Actions on push to <code>master</code>.</p> <p>See <code>.github/workflows/docs.yml</code> for configuration.</p>"},{"location":"CONTRIBUTING_TO_DOCS/#versioning","title":"Versioning","text":"<p>To add versioned documentation:</p> <pre><code># Install mike\npip install mike\n\n# Deploy version\nmike deploy v1.0 latest -u\n\n# Set default version\nmike set-default latest\n\n# Deploy to GitHub Pages\nmike deploy --push --update-aliases v1.0 latest\n</code></pre>"},{"location":"CONTRIBUTING_TO_DOCS/#seo-optimization","title":"SEO Optimization","text":"<ul> <li>Each page has a unique title (H1)</li> <li>Use descriptive headings (H2, H3)</li> <li>Include meta descriptions in frontmatter:   <pre><code>---\ndescription: Complete API reference for Sandrun REST endpoints\n---\n</code></pre></li> <li>Use descriptive link text (not \"click here\")</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#accessibility","title":"Accessibility","text":"<ul> <li>Use proper heading hierarchy (H1 \u2192 H2 \u2192 H3)</li> <li>Add alt text to images</li> <li>Ensure code examples are readable</li> <li>Test with screen readers if possible</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#contributing","title":"Contributing","text":"<p>To contribute to documentation:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b docs/your-feature</code></li> <li>Make changes and test locally</li> <li>Build in strict mode: <code>mkdocs build --strict</code></li> <li>Submit pull request</li> </ol>"},{"location":"CONTRIBUTING_TO_DOCS/#review-checklist","title":"Review Checklist","text":"<ul> <li>[ ] Spelling and grammar checked</li> <li>[ ] Code examples tested and working</li> <li>[ ] Links verified (no 404s)</li> <li>[ ] Builds without warnings: <code>mkdocs build --strict</code></li> <li>[ ] Renders correctly in browser</li> <li>[ ] Mobile-friendly (test with <code>mkdocs serve</code>)</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#getting-help","title":"Getting Help","text":"<ul> <li>MkDocs Documentation: https://www.mkdocs.org/</li> <li>Material Theme: https://squidfunk.github.io/mkdocs-material/</li> <li>Markdown Guide: https://www.markdownguide.org/</li> </ul>"},{"location":"CONTRIBUTING_TO_DOCS/#license","title":"License","text":"<p>Documentation is licensed under the same license as Sandrun (MIT).</p> <p>Happy documenting! \ud83d\udcda</p>"},{"location":"DEPLOYMENT/","title":"Deploying Sandrun Documentation","text":"<p>This guide explains how to deploy the MkDocs documentation to GitHub Pages.</p>"},{"location":"DEPLOYMENT/#github-pages-deployment-options","title":"GitHub Pages Deployment Options","text":""},{"location":"DEPLOYMENT/#option-1-automated-github-actions-recommended","title":"Option 1: Automated GitHub Actions (Recommended)","text":"<p>The repository includes a GitHub Actions workflow that automatically builds and deploys documentation on every push to master.</p> <p>Setup:</p> <ol> <li>Enable GitHub Pages in your repository settings:</li> <li>Go to Settings \u2192 Pages</li> <li>Source: \"GitHub Actions\"</li> <li> <p>Save</p> </li> <li> <p>Push your changes: <pre><code>git push origin master\n</code></pre></p> </li> <li> <p>Wait for deployment:</p> </li> <li>Go to Actions tab to watch the build</li> <li>Documentation will be live at: <code>https://yourusername.github.io/sandrun/</code></li> </ol> <p>Workflow File: <code>.github/workflows/deploy-docs.yml</code></p> <p>The workflow: - Triggers on push to master/main - Installs Python and MkDocs dependencies - Builds the site with <code>mkdocs build --strict</code> - Deploys to GitHub Pages</p>"},{"location":"DEPLOYMENT/#option-2-manual-deployment-with-mkdocs","title":"Option 2: Manual Deployment with mkdocs","text":"<p>Prerequisites: <pre><code>pip install mkdocs-material pymdown-extensions mkdocs-minify-plugin\n</code></pre></p> <p>Deploy: <pre><code># Build and deploy in one command\nmkdocs gh-deploy\n\n# This will:\n# 1. Build the site to ./site/\n# 2. Push to gh-pages branch\n# 3. Documentation goes live at https://yourusername.github.io/sandrun/\n</code></pre></p> <p>Important: Make sure GitHub Pages is configured to use the <code>gh-pages</code> branch: - Settings \u2192 Pages \u2192 Source \u2192 Branch: gh-pages</p>"},{"location":"DEPLOYMENT/#option-3-build-locally-and-deploy-manually","title":"Option 3: Build Locally and Deploy Manually","text":"<pre><code># Build the site\nmkdocs build\n\n# The static site is now in ./site/\n# You can deploy this to any static hosting:\n# - Netlify\n# - Vercel\n# - AWS S3\n# - Your own server\n</code></pre>"},{"location":"DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DEPLOYMENT/#error-no-such-file-or-directory-dir_chdir0-githubworkspacedocs","title":"Error: \"No such file or directory @ dir_chdir0 - /github/workspace/docs\"","text":"<p>Problem: GitHub Pages is trying to use Jekyll instead of MkDocs.</p> <p>Solution: The <code>.nojekyll</code> file in the <code>docs/</code> directory tells GitHub to skip Jekyll processing. Make sure:</p> <ol> <li><code>docs/.nojekyll</code> exists (empty file)</li> <li>GitHub Pages is set to use \"GitHub Actions\" as the source (not \"Deploy from a branch\")</li> </ol>"},{"location":"DEPLOYMENT/#error-theme-material-not-found","title":"Error: \"Theme 'material' not found\"","text":"<p>Problem: MkDocs Material theme not installed.</p> <p>Solution: <pre><code>pip install mkdocs-material\n</code></pre></p> <p>Or use the GitHub Actions workflow which installs it automatically.</p>"},{"location":"DEPLOYMENT/#error-documentation-failed-to-build","title":"Error: \"Documentation failed to build\"","text":"<p>Problem: Broken links or invalid markdown.</p> <p>Solution: <pre><code># Build with strict mode to see all warnings\nmkdocs build --strict\n\n# Fix any broken links or formatting issues\n</code></pre></p>"},{"location":"DEPLOYMENT/#documentation-not-updating","title":"Documentation Not Updating","text":"<p>Problem: Changes not showing on GitHub Pages.</p> <p>Solutions:</p> <ol> <li>Clear browser cache:</li> <li> <p>Hard refresh: Ctrl+F5 (Windows/Linux) or Cmd+Shift+R (Mac)</p> </li> <li> <p>Check GitHub Actions:</p> </li> <li>Go to Actions tab</li> <li>Verify the latest run succeeded</li> <li> <p>Check deployment logs</p> </li> <li> <p>Verify gh-pages branch: <pre><code>git fetch origin gh-pages\ngit log origin/gh-pages\n</code></pre></p> </li> <li> <p>Wait for GitHub Pages propagation:</p> </li> <li>Can take 1-5 minutes after deployment</li> </ol>"},{"location":"DEPLOYMENT/#custom-domain","title":"Custom Domain","text":"<p>To use a custom domain (e.g., <code>docs.sandrun.io</code>):</p> <ol> <li> <p>Create CNAME file: <pre><code>echo \"docs.sandrun.io\" &gt; docs/CNAME\ngit add docs/CNAME\ngit commit -m \"Add custom domain\"\n</code></pre></p> </li> <li> <p>Configure DNS:</p> </li> <li> <p>Add CNAME record: <code>docs</code> \u2192 <code>yourusername.github.io</code></p> </li> <li> <p>Enable in GitHub:</p> </li> <li>Settings \u2192 Pages \u2192 Custom domain: <code>docs.sandrun.io</code></li> <li>Check \"Enforce HTTPS\"</li> </ol>"},{"location":"DEPLOYMENT/#local-preview","title":"Local Preview","text":"<p>Before deploying, preview locally:</p> <pre><code># Start development server\nmkdocs serve\n\n# Open in browser\nopen http://localhost:8000\n\n# Make changes and see them live (auto-reload)\n</code></pre>"},{"location":"DEPLOYMENT/#build-configuration","title":"Build Configuration","text":"<p>Key settings in <code>mkdocs.yml</code>:</p> <pre><code>site_name: Sandrun Documentation\nsite_url: https://yourusername.github.io/sandrun/  # Update this!\n\ntheme:\n  name: material\n  palette:\n    - scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n</code></pre>"},{"location":"DEPLOYMENT/#versioned-documentation-advanced","title":"Versioned Documentation (Advanced)","text":"<p>For versioned documentation (e.g., v1.0, v2.0):</p> <pre><code># Install mike\npip install mike\n\n# Deploy version\nmike deploy 1.0 latest --update-aliases\nmike set-default latest\n\n# Push to gh-pages\ngit push origin gh-pages\n</code></pre> <p>Then users can switch between versions in the documentation.</p>"},{"location":"DEPLOYMENT/#analytics-optional","title":"Analytics (Optional)","text":"<p>To add Google Analytics:</p> <ol> <li> <p>Get Google Analytics ID (e.g., <code>G-XXXXXXXXXX</code>)</p> </li> <li> <p>Update mkdocs.yml: <pre><code>extra:\n  analytics:\n    provider: google\n    property: G-XXXXXXXXXX  # Your actual ID\n</code></pre></p> </li> <li> <p>Redeploy documentation</p> </li> </ol>"},{"location":"DEPLOYMENT/#maintenance","title":"Maintenance","text":""},{"location":"DEPLOYMENT/#updating-documentation","title":"Updating Documentation","text":"<ol> <li>Edit markdown files in <code>docs/</code></li> <li>Preview changes: <code>mkdocs serve</code></li> <li>Commit and push: <pre><code>git add docs/\ngit commit -m \"docs: Update getting started guide\"\ngit push origin master\n</code></pre></li> <li>GitHub Actions automatically deploys</li> </ol>"},{"location":"DEPLOYMENT/#updating-dependencies","title":"Updating Dependencies","text":"<p>Keep MkDocs and plugins updated:</p> <pre><code>pip install --upgrade mkdocs-material pymdown-extensions mkdocs-minify-plugin\nmkdocs build --strict  # Verify still works\n</code></pre>"},{"location":"DEPLOYMENT/#cicd-integration","title":"CI/CD Integration","text":"<p>The GitHub Actions workflow can be extended to:</p> <ul> <li>Run on pull requests (preview deployments)</li> <li>Validate links before deploying</li> <li>Generate sitemap and robots.txt</li> <li>Optimize images</li> <li>Run spell check</li> </ul> <p>Example addition to workflow:</p> <pre><code>- name: Check links\n  run: |\n    pip install linkchecker\n    linkchecker site/\n</code></pre>"},{"location":"DEPLOYMENT/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check build logs in GitHub Actions</li> <li>Verify mkdocs.yml syntax with <code>mkdocs build --strict</code></li> <li>Test locally first with <code>mkdocs serve</code></li> <li>Read MkDocs docs: https://www.mkdocs.org/</li> <li>Material theme docs: https://squidfunk.github.io/mkdocs-material/</li> </ol>"},{"location":"DEPLOYMENT/#see-also","title":"See Also","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>GitHub Pages Documentation</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete REST API documentation for Sandrun's HTTP interface.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"<p>Sandrun provides a simple REST API for anonymous code execution. All endpoints return JSON responses (except for file downloads and logs).</p>"},{"location":"api-reference/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8443\n</code></pre> <p>Protocol Support</p> <ul> <li>Production deployments should use HTTPS with valid certificates</li> <li>CORS is enabled by default for web frontend support</li> <li>WebSocket support available for log streaming</li> </ul>"},{"location":"api-reference/#authentication","title":"Authentication","text":"<p>None required! Sandrun is anonymous by design. Rate limiting is based on client IP address only.</p>"},{"location":"api-reference/#response-format","title":"Response Format","text":"<p>All successful API responses follow this structure:</p> <pre><code>{\n  \"job_id\": \"unique-identifier\",\n  \"status\": \"queued|running|completed|failed\",\n  \"...\": \"additional fields\"\n}\n</code></pre> <p>Error responses:</p> <pre><code>{\n  \"error\": \"Human-readable error message\",\n  \"details\": \"Additional context (optional)\",\n  \"retry_after\": 30  // For rate limiting (optional)\n}\n</code></pre>"},{"location":"api-reference/#quick-reference","title":"Quick Reference","text":"Method Endpoint Purpose GET <code>/</code> Server info and status POST <code>/submit</code> Submit new job GET <code>/status/{job_id}</code> Get job status and metadata GET <code>/logs/{job_id}</code> Get job stdout/stderr WS <code>/logs/{job_id}/stream</code> Stream logs in real-time GET <code>/outputs/{job_id}</code> List output files GET <code>/download/{job_id}/{path}</code> Download output file GET <code>/stats</code> Check quota and system stats GET <code>/environments</code> List available environments GET <code>/health</code> Health check (for pools)"},{"location":"api-reference/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/#get","title":"GET /","text":"<p>Get server information and status.</p> <p>Response:</p> <pre><code>{\n  \"service\": \"sandrun\",\n  \"status\": \"running\",\n  \"description\": \"Batch job execution with directory upload\",\n  \"privacy\": \"Jobs auto-delete after download\",\n  \"limits\": \"10 CPU-sec/min, 512MB RAM, 5 min timeout\"\n}\n</code></pre>"},{"location":"api-reference/#post-submit","title":"POST /submit","text":"<p>Submit a new job for execution.</p> <p>Request:</p> <p>Multipart form data: - <code>files</code>: Tarball containing job files (<code>.tar.gz</code>) - <code>manifest</code>: JSON manifest (see Job Manifest)</p> <p>Example:</p> <pre><code>curl -X POST http://localhost:8443/submit \\\n  -F \"files=@project.tar.gz\" \\\n  -F 'manifest={\"entrypoint\":\"main.py\",\"interpreter\":\"python3\"}'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"job-abc123def456\",\n  \"status\": \"queued\",\n  \"position\": 3\n}\n</code></pre> <p>Status Codes:</p> <ul> <li><code>200 OK</code> - Job accepted</li> <li><code>400 Bad Request</code> - Invalid manifest or files</li> <li><code>429 Too Many Requests</code> - Rate limit exceeded</li> </ul>"},{"location":"api-reference/#get-statusjob_id","title":"GET /status/{job_id}","text":"<p>Get job execution status and metadata.</p> <p>Response:</p> <pre><code>{\n  \"job_id\": \"job-abc123def456\",\n  \"status\": \"completed\",\n  \"execution_metadata\": {\n    \"cpu_seconds\": 1.23,\n    \"memory_peak_bytes\": 52428800,\n    \"exit_code\": 0,\n    \"environment\": \"default\"\n  },\n  \"job_hash\": \"sha256-hash-of-inputs\",\n  \"output_files\": {\n    \"result.txt\": {\n      \"path\": \"result.txt\",\n      \"size_bytes\": 1024,\n      \"sha256_hash\": \"abc123...\",\n      \"type\": \"file\"\n    }\n  },\n  \"worker_metadata\": {\n    \"worker_id\": \"base64-encoded-public-key\",\n    \"signature\": \"base64-encoded-signature\"\n  }\n}\n</code></pre> <p>Job Status Values:</p> <ul> <li><code>queued</code> - Waiting in queue</li> <li><code>running</code> - Currently executing</li> <li><code>completed</code> - Finished successfully</li> <li><code>failed</code> - Execution failed</li> </ul>"},{"location":"api-reference/#get-logsjob_id","title":"GET /logs/{job_id}","text":"<p>Get job stdout and stderr logs.</p> <p>Response:</p> <p>Plain text output from job execution.</p> <p>Example:</p> <pre><code>curl http://localhost:8443/logs/job-abc123def456\n</code></pre> <pre><code>Hello from Sandrun!\nProcessing data...\nResults saved to result.txt\n</code></pre>"},{"location":"api-reference/#websocket-logsjob_idstream","title":"WebSocket /logs/{job_id}/stream","text":"<p>Stream logs in real-time via WebSocket.</p> <p>Example (JavaScript):</p> <pre><code>const ws = new WebSocket('ws://localhost:8443/logs/job-abc123def456/stream');\n\nws.onmessage = (event) =&gt; {\n  console.log(event.data);\n};\n</code></pre>"},{"location":"api-reference/#get-outputsjob_id","title":"GET /outputs/{job_id}","text":"<p>List available output files for download.</p> <p>Response:</p> <pre><code>{\n  \"job_id\": \"job-abc123def456\",\n  \"outputs\": [\n    {\n      \"path\": \"result.txt\",\n      \"size_bytes\": 1024,\n      \"type\": \"file\"\n    },\n    {\n      \"path\": \"images/\",\n      \"type\": \"directory\",\n      \"files\": [\"plot1.png\", \"plot2.png\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"api-reference/#get-downloadjob_idfilepath","title":"GET /download/{job_id}/{filepath}","text":"<p>Download a specific output file.</p> <p>Example:</p> <pre><code>curl http://localhost:8443/download/job-abc123/result.txt -o result.txt\n</code></pre> <p>Response:</p> <p>Binary file content with appropriate <code>Content-Type</code> header.</p>"},{"location":"api-reference/#get-stats","title":"GET /stats","text":"<p>Get quota information and system statistics.</p> <p>Response:</p> <pre><code>{\n  \"your_quota\": {\n    \"used\": 2.5,\n    \"limit\": 10.0,\n    \"available\": 7.5,\n    \"active_jobs\": 1,\n    \"can_submit\": true,\n    \"reason\": \"\"\n  },\n  \"system\": {\n    \"queue_length\": 3,\n    \"active_jobs\": 5\n  }\n}\n</code></pre>"},{"location":"api-reference/#get-environments","title":"GET /environments","text":"<p>List available pre-built environments.</p> <p>Response:</p> <pre><code>{\n  \"templates\": [\n    \"default\",\n    \"ml-basic\",\n    \"vision\",\n    \"nlp\",\n    \"data-science\",\n    \"scientific\"\n  ],\n  \"stats\": {\n    \"total_templates\": 6,\n    \"cached_environments\": 3,\n    \"total_uses\": 42,\n    \"disk_usage_mb\": 512\n  }\n}\n</code></pre>"},{"location":"api-reference/#get-health","title":"GET /health","text":"<p>Health check endpoint (for pool coordinators).</p> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"worker_id\": \"base64-encoded-public-key\"\n}\n</code></pre>"},{"location":"api-reference/#rate-limits","title":"Rate Limits","text":""},{"location":"api-reference/#per-ip-limits","title":"Per-IP Limits","text":"<ul> <li>CPU Quota: 10 CPU-seconds per minute</li> <li>Concurrent Jobs: 2 jobs maximum</li> <li>Hourly Jobs: 20 jobs per hour</li> </ul>"},{"location":"api-reference/#per-job-limits","title":"Per-Job Limits","text":"<ul> <li>Memory: 512MB RAM maximum</li> <li>Timeout: 5 minutes maximum</li> <li>Storage: 100MB in tmpfs</li> </ul>"},{"location":"api-reference/#rate-limit-response","title":"Rate Limit Response","text":"<p>When rate limited:</p> <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"reason\": \"CPU quota exhausted (10.2/10.0 seconds used)\",\n  \"retry_after\": 45\n}\n</code></pre>"},{"location":"api-reference/#error-responses","title":"Error Responses","text":"<p>All errors follow this format:</p> <pre><code>{\n  \"error\": \"Error message\",\n  \"details\": \"Additional information\"\n}\n</code></pre>"},{"location":"api-reference/#common-errors","title":"Common Errors","text":"<p>400 Bad Request:</p> <pre><code>{\n  \"error\": \"Invalid manifest\",\n  \"details\": \"Missing required field: entrypoint\"\n}\n</code></pre> <p>404 Not Found:</p> <pre><code>{\n  \"error\": \"Job not found\",\n  \"details\": \"job-xyz789\"\n}\n</code></pre> <p>429 Too Many Requests:</p> <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"reason\": \"Too many concurrent jobs (2/2)\",\n  \"retry_after\": 60\n}\n</code></pre> <p>500 Internal Server Error:</p> <pre><code>{\n  \"error\": \"Internal server error\",\n  \"details\": \"Failed to create sandbox\"\n}\n</code></pre>"},{"location":"api-reference/#authentication_1","title":"Authentication","text":"<p>Sandrun is anonymous by default. No API keys or authentication required.</p> <p>For trusted pool deployments, workers authenticate via Ed25519 public keys.</p>"},{"location":"api-reference/#cors","title":"CORS","text":"<p>CORS is enabled for all origins to support web frontends.</p>"},{"location":"api-reference/#content-types","title":"Content Types","text":"<p>Request: - <code>multipart/form-data</code> for <code>/submit</code></p> <p>Response: - <code>application/json</code> for most endpoints - <code>text/plain</code> for <code>/logs/{job_id}</code> - <code>application/octet-stream</code> for <code>/download/{job_id}/{file}</code></p>"},{"location":"api-reference/#examples","title":"Examples","text":""},{"location":"api-reference/#submit-and-wait-for-completion","title":"Submit and Wait for Completion","text":"<pre><code>#!/bin/bash\n\n# Submit job\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F 'manifest={\"entrypoint\":\"main.py\",\"interpreter\":\"python3\"}')\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job ID: $JOB_ID\"\n\n# Poll until complete\nwhile true; do\n  STATUS=$(curl -s http://localhost:8443/status/$JOB_ID | jq -r '.status')\n  echo \"Status: $STATUS\"\n\n  if [ \"$STATUS\" = \"completed\" ] || [ \"$STATUS\" = \"failed\" ]; then\n    break\n  fi\n\n  sleep 2\ndone\n\n# Get logs\ncurl http://localhost:8443/logs/$JOB_ID\n</code></pre>"},{"location":"api-reference/#stream-logs-in-real-time","title":"Stream Logs in Real-Time","text":"<pre><code>import websocket\n\ndef on_message(ws, message):\n    print(message, end='')\n\nws = websocket.WebSocketApp(\n    \"ws://localhost:8443/logs/job-abc123/stream\",\n    on_message=on_message\n)\n\nws.run_forever()\n</code></pre>"},{"location":"api-reference/#download-all-outputs","title":"Download All Outputs","text":"<pre><code>#!/bin/bash\n\nJOB_ID=\"job-abc123\"\n\n# Get output list\nOUTPUTS=$(curl -s http://localhost:8443/outputs/$JOB_ID | jq -r '.outputs[].path')\n\n# Download each file\nfor OUTPUT in $OUTPUTS; do\n  curl -o \"$OUTPUT\" \"http://localhost:8443/download/$JOB_ID/$OUTPUT\"\n  echo \"Downloaded: $OUTPUT\"\ndone\n</code></pre>"},{"location":"api-reference/#client-libraries","title":"Client Libraries","text":""},{"location":"api-reference/#python","title":"Python","text":"<pre><code>from integrations.python_client.sandrun_client import SandrunClient\n\nclient = SandrunClient(\"http://localhost:8443\")\n\n# Submit and wait\nresult = client.run_and_wait(\n    code=\"print('Hello!')\",\n    interpreter=\"python3\"\n)\n\nprint(result['logs']['stdout'])\n</code></pre>"},{"location":"api-reference/#javascript","title":"JavaScript","text":"<p>See <code>integrations/web-frontend/</code> for a complete web client example.</p>"},{"location":"api-reference/#curl","title":"cURL","text":"<p>See <code>integrations/examples/curl_examples.sh</code> for command-line examples.</p>"},{"location":"api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Job Manifest - Configure job execution</li> <li>Getting Started - Setup and first job</li> <li>Integrations - Extend capabilities</li> </ul>"},{"location":"architecture/","title":"Sandrun: Anonymous Ephemeral Code Execution","text":""},{"location":"architecture/#motivation","title":"Motivation","text":""},{"location":"architecture/#the-problem","title":"The Problem","text":"<p>Modern computing is increasingly locked down. Running arbitrary code requires: - Creating accounts on cloud platforms (identity exposure) - Installing complex development environments (system contamination) - Trusting third-party services with your code (privacy violation) - Managing dependencies and credentials (operational burden)</p> <p>Meanwhile, there's a legitimate need for ephemeral computation: - Testing code snippets without contaminating your system - Running untrusted scripts safely - Sharing computational resources anonymously - Quick data processing without infrastructure</p>"},{"location":"architecture/#the-vision","title":"The Vision","text":"<p>Sandrun provides anonymous, ephemeral, sandboxed code execution with these principles:</p> <ol> <li>No Identity: No accounts, no tracking, no persistence beyond job lifetime</li> <li>True Isolation: Hardware-enforced sandboxing via Linux security primitives</li> <li>Privacy by Design: Code and data exist only in memory, auto-destroyed after use</li> <li>Fair Access: Rate limiting by IP ensures equitable resource sharing</li> <li>Simplicity: Single binary, minimal dependencies, clear security model</li> </ol>"},{"location":"architecture/#theory","title":"Theory","text":""},{"location":"architecture/#privacy-model","title":"Privacy Model","text":"<p>Threat Model: - Server admin should learn minimal information about executed code - Network observers should see only encrypted traffic - Other users should have zero visibility into your jobs - System compromise shouldn't leak historical job data</p> <p>Privacy Guarantees: - Jobs execute in isolated namespaces (PID, network, mount, IPC, UTS) - Memory is never swapped to disk (mlocked pages) - All job data stored in tmpfs (RAM only) - Automatic destruction after job completion - No logging of job contents, only resource metrics</p>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":"<p>Defense in Depth: 1. Network Layer: TLS only, no HTTP fallback 2. Application Layer: Minimal HTTP server, no dynamic content 3. Execution Layer: Separate sandboxed process per job 4. System Layer: Seccomp-BPF syscall filtering 5. Hardware Layer: CPU/memory quotas via cgroups</p> <p>Sandbox Constraints (Per Job): <pre><code>- New PID namespace (job can't see other processes)\n- New network namespace (isolated network stack)\n- New mount namespace (private filesystem view)\n- Seccomp filter (whitelist of ~50 safe syscalls)\n- No capability privileges (drops all capabilities)\n- Read-only root filesystem bind mount\n- tmpfs for /tmp and working directory\n- CPU quota: 10 seconds per minute\n- Memory limit: 512MB\n- No network access (airgapped execution)\n</code></pre></p>"},{"location":"architecture/#rate-limiting-theory","title":"Rate Limiting Theory","text":"<p>Fair Queuing with Time-Based Quotas:</p> <p>Instead of traditional rate limiting (requests/second), we implement CPU-time fairness: - Each IP gets 10 CPU-seconds per minute - Maximum 2 concurrent jobs per IP - Jobs queued when quota exhausted</p> <p>This ensures: - Long-running jobs don't block others - Burst capacity for quick scripts - Natural DOS protection - Fair resource distribution</p>"},{"location":"architecture/#practice","title":"Practice","text":""},{"location":"architecture/#implementation-stack","title":"Implementation Stack","text":"<p>Why C++: - Direct syscall access for sandboxing - Predictable memory management (no GC pauses) - Small binary size (~500KB vs 50MB+ for Python) - Native seccomp-bpf and namespace support - Compile-time security checks</p> <p>Minimal Dependencies: <pre><code>System Libraries Only:\n- libseccomp: Syscall filtering\n- libcap: Capability management  \n- pthread: Threading\n- Standard C++ library\n\nNo External Dependencies:\n- HTTP parsing: Hand-rolled minimal parser\n- JSON: Simple struct serialization\n- Crypto: Use kernel's /dev/urandom\n</code></pre></p>"},{"location":"architecture/#api-design","title":"API Design","text":"<p>Batch Job Submission: <pre><code>POST /submit\n  Content-Type: multipart/form-data\n  Body: Files (tar.gz, zip, or individual files) + manifest\n  Returns: {\"job_id\": \"uuid\", \"status\": \"queued\"}\n\nGET /status/{job_id}\n  Returns: {\n    \"status\": \"queued|running|completed|failed\",\n    \"queue_position\": 3,\n    \"metrics\": {\"cpu_seconds\": 1.23, \"memory_mb\": 128}\n  }\n\nGET /logs/{job_id}\n  Returns: {\"stdout\": \"...\", \"stderr\": \"...\"}\n\nGET /outputs/{job_id}\n  Returns: {\"files\": [\"results/data.csv\", \"plot.png\"]}\n\nGET /download/{job_id}\n  Returns: tar.gz of output files matching manifest patterns\n  Auto-deletes after retrieval\n</code></pre></p> <p>No Complexity: - No authentication/authorization - No persistent storage - No configuration - Simple multipart parsing (minimal code) - No external dependencies</p>"},{"location":"architecture/#operational-model","title":"Operational Model","text":"<p>Single Binary Deployment: <pre><code># Build\ncmake -B build &amp;&amp; cmake --build build\n\n# Run (needs CAP_SYS_ADMIN for namespaces)\nsudo setcap cap_sys_admin+ep ./sandrun\n./sandrun --port 8443 --cert cert.pem --key key.pem\n</code></pre></p> <p>Auto-Cleanup: - Jobs older than 5 minutes: Killed - Completed jobs: Deleted after 1 minute - Failed jobs: Deleted immediately - Memory usage &gt; 80%: Oldest jobs evicted</p>"},{"location":"architecture/#performance-targets","title":"Performance Targets","text":"<p>Design Goals: - Startup time: &lt;10ms per job - Overhead: &lt;1% CPU for orchestration - Memory: &lt;10MB for service, 512MB per job - Latency: &lt;50ms to start execution - Throughput: 100+ jobs/second on modern hardware</p>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":"<p>What We Explicitly Don't Protect Against: - Timing attacks between jobs (shared CPU) - Resource exhaustion within quotas - Covert channels via cache timing - Kernel vulnerabilities (requires kernel hardening)</p> <p>What We Do Protect Against: - Code injection/execution outside sandbox - Filesystem access beyond tmpfs - Network access from jobs - Process visibility across jobs - Memory disclosure between jobs - Persistence of any job data</p>"},{"location":"architecture/#architecture-frontendbackend-separation","title":"Architecture: Frontend/Backend Separation","text":""},{"location":"architecture/#backend-c-sandbox-service","title":"Backend (C++ Sandbox Service)","text":"<p>Responsibilities: - Secure code execution - Resource management - Rate limiting - Minimal HTTP API</p> <p>Why C++: - Direct syscall control for sandboxing - Predictable memory management - Minimal attack surface - No interpreter overhead</p>"},{"location":"architecture/#frontend-separate-application","title":"Frontend (Separate Application)","text":"<p>Responsibilities: - User interface - Code editing - Result visualization - API interaction</p> <p>Why Separate: - Security isolation from sandbox - Independent deployment/updates - Technology freedom (Python/JS/etc) - CDN hosting possible - Multiple frontends can coexist</p>"},{"location":"architecture/#frontend-options","title":"Frontend Options","text":"<p>1. Python + Streamlit (Recommended for beauty) <pre><code># frontend.py - Beautiful UI in minutes\nimport streamlit as st\nimport requests\n\nst.title(\"\ud83c\udfc3 Sandrun - Anonymous Code Execution\")\n\ncode = st.text_area(\"Code\", height=300)\nlang = st.selectbox(\"Language\", [\"python\", \"javascript\", \"bash\"])\n\nif st.button(\"Run\", type=\"primary\"):\n    with st.spinner(\"Executing...\"):\n        res = requests.post(API_URL, json={\n            \"code\": code,\n            \"interpreter\": lang\n        })\n        st.code(res.json()[\"output\"])\n</code></pre></p> <p>2. Static HTML + JS (Recommended for simplicity) <pre><code>&lt;!-- index.html - Single file, no build needed --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;style&gt;/* Minimal CSS */&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;textarea id=\"code\"&gt;&lt;/textarea&gt;\n    &lt;button onclick=\"runCode()\"&gt;Run&lt;/button&gt;\n    &lt;pre id=\"output\"&gt;&lt;/pre&gt;\n    &lt;script&gt;\n        async function runCode() {\n            const res = await fetch('/run', {\n                method: 'POST',\n                body: document.getElementById('code').value\n            });\n            document.getElementById('output').textContent = \n                await res.text();\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>3. CLI Tool (For developers) <pre><code># sandrun CLI\necho 'print(\"hello\")' | sandrun --lang python\nsandrun script.py\nsandrun --watch script.js  # Re-run on changes\n</code></pre></p>"},{"location":"architecture/#usage-examples","title":"Usage Examples","text":"<pre><code># Direct API Usage\ncurl -X POST https://api.sandrun.io/run \\\n  -H \"Content-Type: text/plain\" \\\n  -d 'print(\"Hello from sandbox\")'\n\n# Via Streamlit Frontend\nstreamlit run frontend.py\n\n# Via Static HTML\npython -m http.server 8080  # Serve index.html\n\n# Via CLI\nsandrun my_script.py\n</code></pre>"},{"location":"architecture/#future-considerations","title":"Future Considerations","text":"<p>Possible Enhancements (Keeping Simplicity): - WebAssembly runtime for additional isolation - Encrypted job storage with ephemeral keys - Distributed execution across multiple nodes - Optional result encryption with client-provided key</p> <p>Explicitly Not Planned: - User accounts or authentication - Persistent storage - Complex job dependencies - File upload/download beyond code - Rich API with many endpoints</p>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>Sandrun represents a return to Unix philosophy: do one thing well. It executes code anonymously and ephemerally with strong isolation. No more, no less.</p> <p>The combination of C++ implementation, Linux security primitives, and aggressive ephemeral design provides a unique sweet spot: practical privacy without cryptographic complexity, strong isolation without virtualization overhead, and fair access without user management.</p> <p>This is computing as a utility: anonymous, ephemeral, and secure.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions about Sandrun and their answers.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-sandrun","title":"What is Sandrun?","text":"<p>Sandrun is an anonymous, ephemeral, sandboxed code execution service. It allows you to run untrusted code in isolated Linux namespaces without creating accounts or leaving persistent data.</p>"},{"location":"faq/#why-would-i-use-sandrun","title":"Why would I use Sandrun?","text":"<ul> <li>Privacy: No signup, no tracking, all data auto-deletes</li> <li>Security: Hardware-enforced isolation prevents malicious code from escaping</li> <li>Simplicity: Just HTTP POST your code and get results</li> <li>Versatility: Python, JavaScript, Bash, R, Ruby, and more</li> <li>Scalability: Distribute jobs across multiple workers with pool coordinator</li> </ul>"},{"location":"faq/#is-sandrun-free","title":"Is Sandrun free?","text":"<p>Yes! Sandrun is open source under the MIT license. You can run your own instance for free, or use a public instance (subject to rate limits).</p>"},{"location":"faq/#how-does-sandrun-compare-to-docker","title":"How does Sandrun compare to Docker?","text":"Feature Sandrun Docker Startup time &lt;10ms ~1s Memory overhead ~10MB ~100MB Isolation Linux namespaces + seccomp Container with configurable isolation Persistence None (tmpfs only) Optional volumes Network access Blocked by default Configurable Use case Ephemeral code execution Application deployment <p>Sandrun is optimized for short-lived, untrusted code execution, while Docker is better for long-running applications and services.</p>"},{"location":"faq/#security-questions","title":"Security Questions","text":""},{"location":"faq/#is-it-safe-to-run-untrusted-code-in-sandrun","title":"Is it safe to run untrusted code in Sandrun?","text":"<p>Sandrun provides strong isolation using multiple layers:</p> <ol> <li>Linux namespaces (PID, network, mount, IPC, UTS)</li> <li>Seccomp-BPF syscall filtering (~60 safe syscalls only)</li> <li>Capability dropping (no privileged operations)</li> <li>Resource limits (CPU, memory, timeout via cgroups)</li> <li>tmpfs-only storage (no disk access)</li> </ol> <p>However, no system is 100% secure. Sandrun relies on the Linux kernel for isolation. Keep your kernel updated and follow security best practices.</p>"},{"location":"faq/#can-jobs-access-the-network","title":"Can jobs access the network?","text":"<p>No. Jobs run in isolated network namespaces with no external connectivity. This prevents:</p> <ul> <li>Data exfiltration</li> <li>Communication between jobs</li> <li>DDoS attacks from sandboxed code</li> </ul> <p>If your job needs network access, you'll need to modify Sandrun or use a different solution.</p>"},{"location":"faq/#can-jobs-see-other-jobs","title":"Can jobs see other jobs?","text":"<p>No. Each job runs in its own PID namespace and cannot see processes from other jobs or the host system.</p>"},{"location":"faq/#what-happens-to-my-code-after-execution","title":"What happens to my code after execution?","text":"<p>All job data is automatically deleted:</p> <ul> <li>After download: Immediate deletion</li> <li>If failed: 5 minutes retention for debugging</li> <li>If no download: 1 hour maximum retention</li> </ul> <p>Data is stored in tmpfs (RAM only) and never touches disk, so it cannot be recovered after deletion.</p>"},{"location":"faq/#can-the-server-admin-see-my-code","title":"Can the server admin see my code?","text":"<p>The server admin could potentially access job data while jobs are running, since Sandrun runs with root privileges. For maximum privacy:</p> <ul> <li>Run your own Sandrun instance</li> <li>Use encrypted tarballs (decrypt inside the sandbox)</li> <li>Use end-to-end encryption for sensitive workflows</li> </ul>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#what-languages-does-sandrun-support","title":"What languages does Sandrun support?","text":"<p>Out of the box:</p> <ul> <li>Python (python3)</li> <li>JavaScript/Node.js (node)</li> <li>Bash (bash)</li> <li>Ruby (ruby)</li> <li>R (Rscript)</li> <li>Perl (perl)</li> <li>PHP (php)</li> </ul> <p>You can add support for other languages by installing them in the environment templates or including static binaries in your job tarball.</p>"},{"location":"faq/#how-do-i-install-dependencies","title":"How do I install dependencies?","text":"<p>Use the <code>requirements</code> field in your manifest:</p> PythonNode.jsRuby <p><pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"requirements\": \"requirements.txt\"\n}\n</code></pre> Runs <code>pip install -r requirements.txt</code> before execution.</p> <p><pre><code>{\n  \"entrypoint\": \"main.js\",\n  \"requirements\": \"package.json\"\n}\n</code></pre> Runs <code>npm install</code> before execution.</p> <p><pre><code>{\n  \"entrypoint\": \"main.rb\",\n  \"requirements\": \"Gemfile\"\n}\n</code></pre> Runs <code>bundle install</code> before execution.</p>"},{"location":"faq/#what-are-environment-templates","title":"What are environment templates?","text":"<p>Pre-built environments with common packages:</p> <ul> <li>default: Minimal system tools</li> <li>ml-basic: NumPy, SciPy, pandas, scikit-learn</li> <li>vision: OpenCV, Pillow, scikit-image</li> <li>nlp: NLTK, spaCy, transformers</li> <li>data-science: Jupyter, matplotlib, seaborn</li> <li>scientific: SymPy, NetworkX, statsmodels</li> </ul> <p>Use them in your manifest: <pre><code>{\n  \"entrypoint\": \"train.py\",\n  \"environment\": \"ml-basic\"\n}\n</code></pre></p>"},{"location":"faq/#how-do-i-download-output-files","title":"How do I download output files?","text":"<ol> <li> <p>Specify outputs in manifest: <pre><code>{\n  \"outputs\": [\"results/\", \"*.png\", \"model.pkl\"]\n}\n</code></pre></p> </li> <li> <p>List available outputs: <pre><code>curl http://localhost:8443/outputs/job-abc123\n</code></pre></p> </li> <li> <p>Download specific file: <pre><code>curl http://localhost:8443/download/job-abc123/results/output.png -o output.png\n</code></pre></p> </li> </ol>"},{"location":"faq/#whats-the-maximum-job-size","title":"What's the maximum job size?","text":"<ul> <li>Tarball upload: 100MB (configurable)</li> <li>Memory per job: 512MB (configurable)</li> <li>Timeout: 5 minutes (configurable)</li> <li>CPU quota: 10 seconds per minute per IP</li> </ul>"},{"location":"faq/#can-i-run-long-running-jobs","title":"Can I run long-running jobs?","text":"<p>By default, jobs timeout after 5 minutes. For longer jobs:</p> <ol> <li> <p>Increase timeout in manifest: <pre><code>{\n  \"timeout\": 3600  // 1 hour\n}\n</code></pre></p> </li> <li> <p>Note: Long jobs consume more CPU quota, which may prevent you from submitting additional jobs.</p> </li> </ol>"},{"location":"faq/#how-do-i-check-job-status","title":"How do I check job status?","text":"<pre><code># Get status\ncurl http://localhost:8443/status/job-abc123\n\n# Poll until complete\nwhile true; do\n  STATUS=$(curl -s http://localhost:8443/status/job-abc123 | jq -r '.status')\n  echo \"Status: $STATUS\"\n  [ \"$STATUS\" = \"completed\" ] || [ \"$STATUS\" = \"failed\" ] &amp;&amp; break\n  sleep 2\ndone\n</code></pre> <p>Or use WebSocket streaming: <pre><code>const ws = new WebSocket('ws://localhost:8443/logs/job-abc123/stream');\nws.onmessage = (event) =&gt; console.log(event.data);\n</code></pre></p>"},{"location":"faq/#rate-limiting-questions","title":"Rate Limiting Questions","text":""},{"location":"faq/#what-are-the-rate-limits","title":"What are the rate limits?","text":"<p>Default limits per IP address:</p> <ul> <li>10 CPU-seconds per minute</li> <li>2 concurrent jobs maximum</li> <li>512MB RAM per job</li> <li>5 minute timeout per job</li> </ul>"},{"location":"faq/#how-is-cpu-quota-calculated","title":"How is CPU quota calculated?","text":"<p>CPU quota measures actual CPU time, not wall-clock time:</p> <ul> <li>A 2-second job using 1 CPU core = 2 CPU-seconds</li> <li>A 1-second job using 4 CPU cores = 4 CPU-seconds</li> </ul>"},{"location":"faq/#what-happens-if-i-exceed-quota","title":"What happens if I exceed quota?","text":"<p>You'll receive a 429 error: <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"reason\": \"CPU quota exhausted (10.2/10.0 seconds used)\",\n  \"retry_after\": 45\n}\n</code></pre></p> <p>Wait for the <code>retry_after</code> seconds, or for 1 hour of inactivity to reset quota.</p>"},{"location":"faq/#can-i-increase-my-rate-limits","title":"Can I increase my rate limits?","text":"<p>If running your own instance, edit the configuration in the source code. If using a public instance, you'll need to follow their policies (or run your own!).</p>"},{"location":"faq/#pool-distribution-questions","title":"Pool &amp; Distribution Questions","text":""},{"location":"faq/#what-is-the-pool-coordinator","title":"What is the pool coordinator?","text":"<p>The pool coordinator distributes jobs across multiple Sandrun workers for horizontal scaling. It provides:</p> <ul> <li>Health checking of workers</li> <li>Load balancing across available workers</li> <li>Centralized job submission</li> <li>Result proxying</li> </ul> <p>Learn more \u2192</p>"},{"location":"faq/#how-do-i-set-up-a-pool","title":"How do I set up a pool?","text":"<ol> <li>Start multiple Sandrun workers with worker keys</li> <li>Create <code>workers.json</code> with worker public keys</li> <li>Run pool coordinator with worker allowlist</li> <li>Submit jobs to coordinator instead of individual workers</li> </ol> <p>Detailed setup guide \u2192</p>"},{"location":"faq/#whats-the-difference-between-trusted-and-trustless-pools","title":"What's the difference between trusted and trustless pools?","text":"Feature Trusted Pool Trustless Pool Worker authorization Allowlist (public keys) Open (anyone) Result verification None (trust workers) Consensus + verification Economic model None Stake/slashing Complexity Simple (~200 lines) Complex Use case Private cluster Public compute <p>Most users should use the trusted pool for private deployments.</p>"},{"location":"faq/#integration-questions","title":"Integration Questions","text":""},{"location":"faq/#can-i-use-sandrun-with-claude","title":"Can I use Sandrun with Claude?","text":"<p>Yes! The MCP (Model Context Protocol) server integration allows Claude Desktop to execute code via Sandrun:</p> <ol> <li>Install MCP server: <code>pip install -e integrations/mcp-server</code></li> <li>Add to Claude Desktop config</li> <li>Ask Claude to execute code</li> </ol> <p>MCP integration guide \u2192</p>"},{"location":"faq/#how-do-i-integrate-sandrun-into-my-app","title":"How do I integrate Sandrun into my app?","text":"<p>Use the REST API directly:</p> <pre><code>import requests\n\n# Submit job\nresponse = requests.post('http://localhost:8443/submit',\n    files={'files': open('job.tar.gz', 'rb')},\n    data={'manifest': '{\"entrypoint\":\"main.py\"}'})\n\njob_id = response.json()['job_id']\n\n# Get results\nstatus = requests.get(f'http://localhost:8443/status/{job_id}').json()\n</code></pre> <p>See API Reference for full documentation.</p>"},{"location":"faq/#can-i-use-sandrun-in-cicd","title":"Can I use Sandrun in CI/CD?","text":"<p>Yes! Sandrun is great for isolated test execution:</p> <pre><code># .github/workflows/test.yml\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run tests in Sandrun\n        run: |\n          tar czf tests.tar.gz tests/\n          curl -X POST http://sandrun.example.com:8443/submit \\\n            -F \"files=@tests.tar.gz\" \\\n            -F 'manifest={\"entrypoint\":\"run_tests.sh\"}'\n</code></pre>"},{"location":"faq/#troubleshooting-questions","title":"Troubleshooting Questions","text":""},{"location":"faq/#why-is-my-job-stuck-in-queued-status","title":"Why is my job stuck in \"queued\" status?","text":"<p>Possible causes:</p> <ol> <li>No available workers (if using pool)</li> <li>Rate limit reached (2 concurrent jobs per IP)</li> <li>Server overloaded (check <code>/stats</code> endpoint)</li> </ol>"},{"location":"faq/#why-did-my-job-fail","title":"Why did my job fail?","text":"<p>Check logs for errors: <pre><code>curl http://localhost:8443/logs/job-abc123\n</code></pre></p> <p>Common causes:</p> <ul> <li>Missing files in tarball</li> <li>Syntax errors in code</li> <li>Missing dependencies</li> <li>Exceeded memory limit</li> <li>Timeout (job took too long)</li> </ul>"},{"location":"faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<ol> <li>Check GitHub Issues for existing reports</li> <li>Create new issue with:</li> <li>Sandrun version (<code>./build/sandrun --version</code>)</li> <li>Operating system and kernel version</li> <li>Steps to reproduce</li> <li>Error messages and logs</li> </ol>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ul> <li>Documentation: This site!</li> <li>GitHub Discussions: Ask questions, share use cases</li> <li>GitHub Issues: Report bugs and request features</li> <li>Source Code: Read the code in <code>src/</code></li> </ul>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-fast-is-sandrun","title":"How fast is Sandrun?","text":"<ul> <li>Job startup: &lt;10ms</li> <li>API latency: &lt;50ms</li> <li>Throughput: 100+ jobs/second (on modern hardware)</li> <li>Overhead: &lt;1% CPU for orchestration</li> </ul>"},{"location":"faq/#can-sandrun-handle-gpu-workloads","title":"Can Sandrun handle GPU workloads?","text":"<p>Yes! Use the <code>gpu</code> field in your manifest:</p> <pre><code>{\n  \"entrypoint\": \"train.py\",\n  \"gpu\": {\n    \"required\": true,\n    \"min_vram_gb\": 8,\n    \"cuda_version\": \"11.8\"\n  }\n}\n</code></pre> <p>Note: GPU support requires proper NVIDIA drivers and configuration on the host.</p>"},{"location":"faq/#how-does-sandrun-scale","title":"How does Sandrun scale?","text":"<ul> <li>Vertical: Single instance can handle 100+ jobs/second</li> <li>Horizontal: Use pool coordinator to distribute across multiple workers</li> <li>Geographic: Deploy pools in different regions</li> </ul>"},{"location":"faq/#development-questions","title":"Development Questions","text":""},{"location":"faq/#how-do-i-build-sandrun-from-source","title":"How do I build Sandrun from source?","text":"<pre><code>git clone https://github.com/yourusername/sandrun.git\ncd sandrun\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build -j$(nproc)\n</code></pre> <p>Full build guide \u2192</p>"},{"location":"faq/#how-do-i-contribute","title":"How do I contribute?","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for your changes</li> <li>Submit a pull request</li> </ol> <p>Contributing guidelines \u2192</p>"},{"location":"faq/#where-is-the-code","title":"Where is the code?","text":"<ul> <li>C++ Backend: <code>src/</code></li> <li>Integrations: <code>integrations/</code></li> <li>Tests: <code>tests/</code></li> <li>Documentation: <code>docs/</code></li> </ul> <p>Have more questions? Open a discussion on GitHub \u2192</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through installing Sandrun, running your first job, and understanding the core workflow.</p> <p>Estimated Time</p> <p>5-10 minutes to install and run your first job</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before installing Sandrun, ensure your system meets these requirements:</p>"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":"Requirement Specification Operating System Linux (Ubuntu 20.04+, Debian 11+, or equivalent) Kernel Version 4.6+ (for namespace support) RAM 2GB minimum (4GB+ recommended) Disk Space 500MB for build artifacts Root Access Required for namespace creation"},{"location":"getting-started/#check-your-system","title":"Check Your System","text":"<pre><code># Verify kernel version (should be 4.6+)\nuname -r\n\n# Check if namespaces are supported\nls /proc/self/ns/\n\n# Verify seccomp support\ncat /proc/sys/kernel/seccomp  # Should output: 2\n</code></pre> <p>Root Permissions Required</p> <p>Sandrun requires root privileges to create Linux namespaces. You'll need to run it with <code>sudo</code> or grant the CAP_SYS_ADMIN capability.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#1-install-dependencies","title":"1. Install Dependencies","text":"Ubuntu/DebianFedora/RHELArch Linux <pre><code>sudo apt-get update\nsudo apt-get install -y \\\n  build-essential \\\n  cmake \\\n  libseccomp-dev \\\n  libcap-dev \\\n  libssl-dev \\\n  pkg-config \\\n  git\n</code></pre> <pre><code>sudo dnf install -y \\\n  gcc-c++ \\\n  cmake \\\n  libseccomp-devel \\\n  libcap-devel \\\n  openssl-devel \\\n  pkgconfig \\\n  git\n</code></pre> <pre><code>sudo pacman -S \\\n  base-devel \\\n  cmake \\\n  libseccomp \\\n  libcap \\\n  openssl \\\n  pkgconf \\\n  git\n</code></pre>"},{"location":"getting-started/#2-clone-and-build","title":"2. Clone and Build","text":"<pre><code># Clone repository\ngit clone https://github.com/yourusername/sandrun.git\ncd sandrun\n\n# Configure build\ncmake -B build -DCMAKE_BUILD_TYPE=Release\n\n# Build (use -j for parallel compilation)\ncmake --build build -j$(nproc)\n\n# Verify build\n./build/sandrun --help\n</code></pre> <p>Build Output</p> <p>If successful, you should see: <pre><code>Usage: sandrun [options]\nOptions:\n  --port PORT          Server port (default: 8443)\n  --worker-key FILE    Worker private key for signing\n  --generate-key FILE  Generate new worker keypair\n  --help              Show this help message\n</code></pre></p>"},{"location":"getting-started/#3-run-server","title":"3. Run Server","text":"<pre><code># Start server (requires sudo for namespace creation)\nsudo ./build/sandrun --port 8443\n</code></pre> <p>Expected Output: <pre><code>[INFO] Sandrun v1.0.0 starting...\n[INFO] Initializing sandbox environment\n[INFO] Loading environment templates\n[INFO] Server listening on http://0.0.0.0:8443\n[INFO] Press Ctrl+C to stop\n</code></pre></p> <p>Running Without Sudo</p> <p>For production deployments, you can grant specific capabilities instead of full root: <pre><code>sudo setcap cap_sys_admin,cap_sys_chroot,cap_setuid,cap_setgid+ep ./build/sandrun\n./build/sandrun --port 8443  # No sudo needed\n</code></pre></p>"},{"location":"getting-started/#your-first-job","title":"Your First Job","text":""},{"location":"getting-started/#simple-python-script","title":"Simple Python Script","text":"<pre><code># Create script\ncat &gt; hello.py &lt;&lt;'EOF'\nprint(\"Hello from Sandrun!\")\nimport sys\nprint(f\"Python version: {sys.version}\")\nEOF\n\n# Create manifest\ncat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"hello.py\",\n  \"interpreter\": \"python3\"\n}\nEOF\n\n# Package files\ntar czf job.tar.gz hello.py\n\n# Submit job\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F \"manifest=$(cat job.json)\"\n\n# Response:\n# {\n#   \"job_id\": \"job-abc123def456\",\n#   \"status\": \"queued\"\n# }\n</code></pre>"},{"location":"getting-started/#check-job-status","title":"Check Job Status","text":"<pre><code># Get status\ncurl http://localhost:8443/status/job-abc123def456\n\n# Response:\n# {\n#   \"job_id\": \"job-abc123def456\",\n#   \"status\": \"completed\",\n#   \"execution_metadata\": {\n#     \"cpu_seconds\": 0.05,\n#     \"memory_peak_bytes\": 12582912,\n#     \"exit_code\": 0\n#   },\n#   \"output_files\": {\n#     ...\n#   }\n# }\n</code></pre>"},{"location":"getting-started/#get-logs","title":"Get Logs","text":"<pre><code># Get stdout\ncurl http://localhost:8443/logs/job-abc123def456\n\n# Response:\n# Hello from Sandrun!\n# Python version: 3.10.12 (main, ...)\n</code></pre>"},{"location":"getting-started/#multi-file-projects","title":"Multi-File Projects","text":""},{"location":"getting-started/#upload-entire-directory","title":"Upload Entire Directory","text":"<pre><code># Create project\nmkdir my-project\ncd my-project\n\ncat &gt; main.py &lt;&lt;'EOF'\nfrom utils import greet\n\ngreet(\"World\")\nEOF\n\ncat &gt; utils.py &lt;&lt;'EOF'\ndef greet(name):\n    print(f\"Hello, {name}!\")\nEOF\n\ncat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"outputs\": [\"*.txt\", \"results/\"]\n}\nEOF\n\n# Package and submit\ntar czf ../my-project.tar.gz .\ncd ..\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@my-project.tar.gz\" \\\n  -F \"manifest=$(cat my-project/job.json)\"\n</code></pre>"},{"location":"getting-started/#using-environments","title":"Using Environments","text":"<p>Sandrun supports pre-built environments with common packages:</p> <pre><code>cat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"ml_script.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\"\n}\nEOF\n</code></pre> <p>Available environments:</p> <ul> <li><code>ml-basic</code> - NumPy, SciPy, pandas</li> <li><code>vision</code> - OpenCV, Pillow, scikit-image</li> <li><code>nlp</code> - NLTK, spaCy, transformers</li> <li><code>data-science</code> - Jupyter, matplotlib, seaborn</li> <li><code>scientific</code> - SymPy, NetworkX, statsmodels</li> </ul> <p>List available environments:</p> <pre><code>curl http://localhost:8443/environments\n</code></pre>"},{"location":"getting-started/#worker-identity-optional","title":"Worker Identity (Optional)","text":"<p>Generate a worker identity for signed results:</p> <pre><code># Generate key\nsudo ./build/sandrun --generate-key /etc/sandrun/worker.pem\n\n# Output:\n# \u2705 Saved worker key to: /etc/sandrun/worker.pem\n#    Worker ID: &lt;base64-encoded-public-key&gt;\n\n# Start with identity\nsudo ./build/sandrun --port 8443 --worker-key /etc/sandrun/worker.pem\n</code></pre> <p>Jobs executed by this worker will include:</p> <pre><code>{\n  \"worker_metadata\": {\n    \"worker_id\": \"base64-encoded-public-key\",\n    \"signature\": \"base64-encoded-signature\"\n  }\n}\n</code></pre>"},{"location":"getting-started/#rate-limits","title":"Rate Limits","text":"<p>Sandrun enforces IP-based rate limits:</p> <ul> <li>10 CPU-seconds per minute per IP</li> <li>512MB RAM per job</li> <li>5 minute timeout per job</li> <li>2 concurrent jobs per IP</li> </ul> <p>Check your quota:</p> <pre><code>curl http://localhost:8443/stats\n\n# Response:\n# {\n#   \"your_quota\": {\n#     \"used\": 2.5,\n#     \"limit\": 10.0,\n#     \"available\": 7.5,\n#     \"active_jobs\": 1,\n#     \"can_submit\": true\n#   },\n#   \"system\": {\n#     \"queue_length\": 3,\n#     \"active_jobs\": 5\n#   }\n# }\n</code></pre>"},{"location":"getting-started/#understanding-rate-limits","title":"Understanding Rate Limits","text":"<p>Sandrun uses IP-based rate limiting to ensure fair resource sharing:</p>"},{"location":"getting-started/#default-limits","title":"Default Limits","text":"Limit Type Value Window CPU Quota 10 CPU-seconds Per minute Concurrent Jobs 2 jobs At a time Memory per Job 512MB Per job Timeout 5 minutes Per job"},{"location":"getting-started/#check-your-quota","title":"Check Your Quota","text":"<pre><code>curl http://localhost:8443/stats\n</code></pre> <p>Response: <pre><code>{\n  \"your_quota\": {\n    \"used\": 2.5,\n    \"limit\": 10.0,\n    \"available\": 7.5,\n    \"active_jobs\": 1,\n    \"can_submit\": true\n  },\n  \"system\": {\n    \"queue_length\": 3,\n    \"active_jobs\": 5\n  }\n}\n</code></pre></p> <p>Quota Management</p> <ul> <li>Quota resets after 1 hour of inactivity</li> <li>CPU time is measured in actual CPU seconds, not wall-clock time</li> <li>Use different IP addresses for separate quotas (if needed for testing)</li> </ul>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#permission-denied","title":"Permission Denied","text":"<p>Symptom: <pre><code>Error: Permission denied creating namespace\n</code></pre></p> <p>Solutions:</p> Use sudoGrant capabilitiesCheck user namespaces <pre><code>sudo ./build/sandrun --port 8443\n</code></pre> <pre><code>sudo setcap cap_sys_admin,cap_sys_chroot,cap_setuid,cap_setgid+ep ./build/sandrun\n./build/sandrun --port 8443\n</code></pre> <pre><code># Some systems disable user namespaces for security\ncat /proc/sys/kernel/unprivileged_userns_clone\n# Should output: 1 (enabled)\n\n# If disabled, enable it:\nsudo sysctl -w kernel.unprivileged_userns_clone=1\n</code></pre>"},{"location":"getting-started/#port-already-in-use","title":"Port Already in Use","text":"<p>Symptom: <pre><code>Error: Failed to bind to port 8443: Address already in use\n</code></pre></p> <p>Solutions:</p> <pre><code># Option 1: Use a different port\nsudo ./build/sandrun --port 9000\n\n# Option 2: Find and kill the process using the port\nsudo lsof -i :8443\nsudo kill &lt;PID&gt;\n\n# Option 3: Let the OS assign a port\nsudo ./build/sandrun --port 0  # Uses random available port\n</code></pre>"},{"location":"getting-started/#build-failures","title":"Build Failures","text":"<p>Symptom: <pre><code>CMake Error: Could not find libseccomp\n</code></pre></p> <p>Solution: <pre><code># Ensure all dependencies are installed\nsudo apt-get install -y libseccomp-dev libcap-dev libssl-dev\n\n# Clean and rebuild\nrm -rf build\ncmake -B build\ncmake --build build\n</code></pre></p>"},{"location":"getting-started/#job-failed-to-execute","title":"Job Failed to Execute","text":"<p>Symptom: Job status shows <code>\"status\": \"failed\"</code> with non-zero exit code.</p> <p>Debugging Steps:</p> <ol> <li> <p>Check logs for errors: <pre><code>curl http://localhost:8443/logs/job-abc123\n</code></pre></p> </li> <li> <p>Verify manifest syntax: <pre><code>echo '{\"entrypoint\":\"main.py\",\"interpreter\":\"python3\"}' | jq .\n</code></pre></p> </li> <li> <p>Check file permissions: <pre><code># Ensure entrypoint is included in tarball\ntar -tzf job.tar.gz\n</code></pre></p> </li> <li> <p>Test locally first: <pre><code>python3 main.py  # Test before submitting\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<p>Symptom: <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"reason\": \"CPU quota exhausted (10.2/10.0 seconds used)\",\n  \"retry_after\": 45\n}\n</code></pre></p> <p>Solutions:</p> <ul> <li>Wait for quota refresh (shown in <code>retry_after</code> field)</li> <li>Optimize your code to use less CPU time</li> <li>Split jobs into smaller chunks</li> <li>Use different IP (for testing only)</li> </ul>"},{"location":"getting-started/#cannot-access-server","title":"Cannot Access Server","text":"<p>Symptom: <pre><code>curl: (7) Failed to connect to localhost port 8443: Connection refused\n</code></pre></p> <p>Checklist:</p> <ol> <li> <p>Verify server is running: <pre><code>ps aux | grep sandrun\n</code></pre></p> </li> <li> <p>Check if port is listening: <pre><code>sudo netstat -tlnp | grep 8443\n</code></pre></p> </li> <li> <p>Check firewall rules: <pre><code>sudo ufw status\nsudo iptables -L -n | grep 8443\n</code></pre></p> </li> <li> <p>Verify server logs: <pre><code># If running in terminal, check stdout\n# Or check system logs if running as service\njournalctl -u sandrun -f\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#using-pre-built-environments","title":"Using Pre-built Environments","text":"<p>Sandrun includes pre-built Python environments with common packages for faster execution.</p>"},{"location":"getting-started/#example-ml-job-with-ml-basic-environment","title":"Example: ML Job with ml-basic Environment","text":"<pre><code># Create training script\ncat &gt; train.py &lt;&lt;'EOF'\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint(\"Packages loaded successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\n\n# Create sample data\nX = np.random.rand(100, 5)\ny = np.random.randint(0, 2, 100)\n\n# Train model\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\nprint(f\"Model trained! Accuracy: {model.score(X, y):.2f}\")\nEOF\n\n# Create manifest with environment\ncat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"train.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\"\n}\nEOF\n\n# Package and submit\ntar czf ml-job.tar.gz train.py\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@ml-job.tar.gz\" \\\n  -F \"manifest=$(cat job.json)\"\n</code></pre> <p>Benefits: - \u2705 No pip install time (packages pre-installed) - \u2705 Reproducible versions - \u2705 Cached across jobs for efficiency</p>"},{"location":"getting-started/#available-environments","title":"Available Environments","text":"Environment Packages Use Case <code>ml-basic</code> NumPy, Pandas, Scikit-learn, Matplotlib Traditional ML <code>vision</code> PyTorch, Torchvision, OpenCV, Pillow Computer vision <code>nlp</code> PyTorch, Transformers, Tokenizers NLP/language models <code>data-science</code> NumPy, Pandas, Matplotlib, Seaborn, Jupyter Data analysis <code>scientific</code> NumPy, SciPy, SymPy, Matplotlib Scientific computing <p>Check available environments: <pre><code>curl http://localhost:8443/environments\n</code></pre></p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Ready for More?</p> <p>Check out our comprehensive Distributed Data Pipeline Tutorial that takes you from basics to building a production-ready distributed processing system with:</p> <ul> <li>Multi-file projects with dependencies</li> <li>Pre-built environments for faster execution</li> <li>Distributed computing across worker pools</li> <li>Real-time WebSocket monitoring</li> <li>Cryptographic result verification</li> </ul> <p>Time: 45-60 minutes | Level: Beginner to Advanced</p>"},{"location":"getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Job Manifest - Advanced job configuration</li> <li>Integrations - Pool coordinator, MCP server</li> <li>Architecture - Understand the internals</li> </ul>"},{"location":"job-manifest/","title":"Job Manifest Specification","text":""},{"location":"job-manifest/#overview","title":"Overview","text":"<p>Each job submitted to Sandrun can include a manifest file (<code>job.json</code>) that specifies execution parameters. This allows for reproducible, queue-friendly batch processing.</p>"},{"location":"job-manifest/#manifest-format","title":"Manifest Format","text":"<pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"args\": [\"--input\", \"data.csv\"],\n  \"env\": {\n    \"PYTHONPATH\": \"./lib\"\n  },\n  \"outputs\": [\n    \"results/\",\n    \"*.png\",\n    \"logs/*.log\"\n  ],\n  \"requirements\": \"requirements.txt\",\n  \"timeout\": 300,\n  \"memory_mb\": 512,\n  \"cpu_seconds\": 10\n}\n</code></pre>"},{"location":"job-manifest/#fields","title":"Fields","text":""},{"location":"job-manifest/#entrypoint-required","title":"<code>entrypoint</code> (required)","text":"<ul> <li>Type: string</li> <li>Description: The script or binary to execute</li> <li>Examples: <code>\"main.py\"</code>, <code>\"run.sh\"</code>, <code>\"analyze.R\"</code></li> </ul>"},{"location":"job-manifest/#interpreter-optional","title":"<code>interpreter</code> (optional)","text":"<ul> <li>Type: string</li> <li>Default: Auto-detected from file extension</li> <li>Options: <code>\"python3\"</code>, <code>\"node\"</code>, <code>\"bash\"</code>, <code>\"ruby\"</code>, <code>\"Rscript\"</code></li> <li>Description: The interpreter to use for the entrypoint</li> </ul>"},{"location":"job-manifest/#environment-optional","title":"<code>environment</code> (optional)","text":"<ul> <li>Type: string</li> <li>Default: None (uses default Python environment)</li> <li>Options: <code>\"ml-basic\"</code>, <code>\"vision\"</code>, <code>\"nlp\"</code>, <code>\"data-science\"</code>, <code>\"scientific\"</code></li> <li>Description: Pre-built Python environment template with common packages</li> <li>Benefits:</li> <li>Faster execution: Packages pre-installed, no pip install time</li> <li>Reproducibility: Known package versions</li> <li>Disk efficiency: Environments cached and shared across jobs</li> <li>Available Templates:</li> <li><code>ml-basic</code>: NumPy, Pandas, Scikit-learn, Matplotlib</li> <li><code>vision</code>: PyTorch, Torchvision, OpenCV, Pillow</li> <li><code>nlp</code>: PyTorch, Transformers, Tokenizers, SentencePiece</li> <li><code>data-science</code>: NumPy, Pandas, Matplotlib, Seaborn, Jupyter, IPython</li> <li><code>scientific</code>: NumPy, SciPy, SymPy, Matplotlib</li> <li>Example:   <pre><code>{\n  \"entrypoint\": \"train.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\"\n}\n</code></pre></li> <li>Note: If specified, <code>requirements.txt</code> will still be installed on top of the template</li> </ul>"},{"location":"job-manifest/#args-optional","title":"<code>args</code> (optional)","text":"<ul> <li>Type: array of strings</li> <li>Default: <code>[]</code></li> <li>Description: Command-line arguments to pass to the entrypoint</li> <li>Example: <code>[\"--input\", \"data.csv\", \"--verbose\"]</code></li> </ul>"},{"location":"job-manifest/#env-optional","title":"<code>env</code> (optional)","text":"<ul> <li>Type: object</li> <li>Default: <code>{}</code></li> <li>Description: Environment variables to set</li> <li>Note: Cannot override system security variables</li> </ul>"},{"location":"job-manifest/#outputs-optional","title":"<code>outputs</code> (optional)","text":"<ul> <li>Type: array of strings (glob patterns)</li> <li>Default: <code>[\"*\"]</code> (everything)</li> <li>Description: Files/directories to include in output download</li> <li>Examples:</li> <li><code>\"results/\"</code> - Include entire results directory</li> <li><code>\"*.png\"</code> - All PNG files</li> <li><code>\"output.json\"</code> - Specific file</li> <li><code>\"logs/*.log\"</code> - All log files in logs directory</li> </ul>"},{"location":"job-manifest/#requirements-optional","title":"<code>requirements</code> (optional)","text":"<ul> <li>Type: string</li> <li>Description: Dependencies file to install before execution</li> <li>Supported:</li> <li>Python: <code>\"requirements.txt\"</code> \u2192 runs <code>pip install -r requirements.txt</code></li> <li>Node: <code>\"package.json\"</code> \u2192 runs <code>npm install</code></li> <li>Ruby: <code>\"Gemfile\"</code> \u2192 runs <code>bundle install</code></li> </ul>"},{"location":"job-manifest/#timeout-optional","title":"<code>timeout</code> (optional)","text":"<ul> <li>Type: integer (seconds)</li> <li>Default: 300 (5 minutes)</li> <li>Maximum: 3600 (1 hour)</li> <li>Description: Maximum execution time</li> </ul>"},{"location":"job-manifest/#memory_mb-optional","title":"<code>memory_mb</code> (optional)","text":"<ul> <li>Type: integer</li> <li>Default: 512</li> <li>Maximum: 2048</li> <li>Description: Memory limit in megabytes</li> </ul>"},{"location":"job-manifest/#cpu_seconds-optional","title":"<code>cpu_seconds</code> (optional)","text":"<ul> <li>Type: integer</li> <li>Default: 10</li> <li>Maximum: 60</li> <li>Description: CPU seconds per minute quota</li> </ul>"},{"location":"job-manifest/#gpu-optional","title":"<code>gpu</code> (optional)","text":"<ul> <li>Type: object</li> <li>Description: GPU requirements for ML/compute workloads</li> <li>Fields:</li> <li><code>required</code> (boolean): Whether GPU is required</li> <li><code>device_id</code> (integer): Specific GPU device (default: 0)</li> <li><code>min_vram_gb</code> (integer): Minimum VRAM required in GB</li> <li><code>cuda_version</code> (string): Minimum CUDA version (e.g., \"11.8\")</li> <li><code>compute_capability</code> (string): Minimum compute capability (e.g., \"7.0\")</li> <li>Example:   <pre><code>{\n  \"required\": true,\n  \"min_vram_gb\": 8,\n  \"cuda_version\": \"11.8\",\n  \"compute_capability\": \"7.0\"\n}\n</code></pre></li> </ul>"},{"location":"job-manifest/#execution-flow","title":"Execution Flow","text":"<ol> <li>Upload: User uploads directory with code and <code>job.json</code></li> <li>Queue: Job enters queue with manifest metadata</li> <li>Prepare: When job starts:</li> <li>Extract files to sandbox</li> <li>Install dependencies if specified</li> <li>Set environment variables</li> <li>Execute: Run entrypoint with args</li> <li>Capture: </li> <li>stdout \u2192 <code>stdout.log</code></li> <li>stderr \u2192 <code>stderr.log</code></li> <li>Both streamed to API for live viewing</li> <li>Package: Create output archive with files matching <code>outputs</code> patterns</li> <li>Cleanup: Delete all job data after download or timeout</li> </ol>"},{"location":"job-manifest/#examples","title":"Examples","text":""},{"location":"job-manifest/#python-data-analysis","title":"Python Data Analysis","text":"<pre><code>{\n  \"entrypoint\": \"analyze.py\",\n  \"args\": [\"--dataset\", \"sales.csv\"],\n  \"outputs\": [\"figures/\", \"report.pdf\"],\n  \"requirements\": \"requirements.txt\",\n  \"memory_mb\": 1024\n}\n</code></pre>"},{"location":"job-manifest/#nodejs-build-job","title":"Node.js Build Job","text":"<pre><code>{\n  \"entrypoint\": \"build.js\",\n  \"interpreter\": \"node\",\n  \"env\": {\n    \"NODE_ENV\": \"production\"\n  },\n  \"outputs\": [\"dist/\"],\n  \"requirements\": \"package.json\"\n}\n</code></pre>"},{"location":"job-manifest/#shell-script-pipeline","title":"Shell Script Pipeline","text":"<pre><code>{\n  \"entrypoint\": \"pipeline.sh\",\n  \"interpreter\": \"bash\",\n  \"outputs\": [\"processed/*.csv\", \"summary.txt\"],\n  \"timeout\": 600\n}\n</code></pre>"},{"location":"job-manifest/#r-statistical-analysis","title":"R Statistical Analysis","text":"<pre><code>{\n  \"entrypoint\": \"model.R\",\n  \"interpreter\": \"Rscript\",\n  \"args\": [\"--confidence\", \"0.95\"],\n  \"outputs\": [\"plots/*.png\", \"results.rds\"]\n}\n</code></pre>"},{"location":"job-manifest/#ml-training-with-gpu","title":"ML Training with GPU","text":"<pre><code>{\n  \"entrypoint\": \"train.py\",\n  \"args\": [\"--epochs\", \"10\", \"--batch-size\", \"32\"],\n  \"gpu\": {\n    \"required\": true,\n    \"min_vram_gb\": 8,\n    \"cuda_version\": \"11.8\"\n  },\n  \"outputs\": [\"checkpoints/\", \"metrics.json\"],\n  \"requirements\": \"requirements.txt\",\n  \"timeout\": 1800,\n  \"memory_mb\": 2048\n}\n</code></pre>"},{"location":"job-manifest/#stable-diffusion-inference","title":"Stable Diffusion Inference","text":"<pre><code>{\n  \"entrypoint\": \"generate.py\",\n  \"args\": [\"--prompt\", \"a beautiful sunset\", \"--steps\", \"50\"],\n  \"gpu\": {\n    \"required\": true,\n    \"min_vram_gb\": 6,\n    \"compute_capability\": \"7.0\"\n  },\n  \"outputs\": [\"images/*.png\"],\n  \"timeout\": 600\n}\n</code></pre>"},{"location":"job-manifest/#privacy-considerations","title":"Privacy Considerations","text":"<ul> <li>Manifest is deleted immediately after job parsing</li> <li>No manifest contents are logged (only metrics)</li> <li>Output patterns are applied before download (unmatched files never leave sandbox)</li> <li>All job data auto-deleted after:</li> <li>Successful download (immediate)</li> <li>Job failure (5 minutes)</li> <li>No download (1 hour)</li> </ul>"},{"location":"job-manifest/#api-endpoints","title":"API Endpoints","text":""},{"location":"job-manifest/#submit-job","title":"Submit Job","text":"<pre><code>POST /submit\nContent-Type: multipart/form-data\n\nFiles: [project files]\nManifest: job.json (optional, can be in files or separate field)\n</code></pre>"},{"location":"job-manifest/#get-status","title":"Get Status","text":"<pre><code>GET /status/{job_id}\n\nReturns:\n{\n  \"status\": \"queued|running|completed|failed\",\n  \"queue_position\": 3,\n  \"metrics\": {\n    \"cpu_seconds\": 2.34,\n    \"memory_mb\": 128,\n    \"runtime\": 45\n  }\n}\n</code></pre>"},{"location":"job-manifest/#stream-logs","title":"Stream Logs","text":"<pre><code>GET /logs/{job_id}\n\nReturns:\n{\n  \"stdout\": \"...\",\n  \"stderr\": \"...\"\n}\n</code></pre>"},{"location":"job-manifest/#list-outputs","title":"List Outputs","text":"<pre><code>GET /outputs/{job_id}\n\nReturns:\n{\n  \"files\": [\"results/analysis.csv\", \"plot.png\"]\n}\n</code></pre>"},{"location":"job-manifest/#download-outputs","title":"Download Outputs","text":"<pre><code>GET /download/{job_id}          # All outputs as tar.gz\nGET /download/{job_id}/{file}   # Specific file\n</code></pre>"},{"location":"security/","title":"Security Best Practices","text":"<p>Comprehensive security guide for deploying and using Sandrun safely.</p>"},{"location":"security/#overview","title":"Overview","text":"<p>Sandrun is designed for untrusted code execution with strong isolation. However, security is a shared responsibility between the Sandrun software and the deployment environment.</p> <p>Important Security Notice</p> <p>While Sandrun provides robust isolation, it relies on the Linux kernel. Always keep your kernel and system updated with the latest security patches.</p>"},{"location":"security/#isolation-layers","title":"Isolation Layers","text":"<p>Sandrun uses defense-in-depth with multiple isolation layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application Code (Untrusted)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Seccomp-BPF (~60 safe syscalls)   \u2502 \u2190 Syscall filtering\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Linux Namespaces                   \u2502 \u2190 Process/network/mount isolation\n\u2502  - PID, Network, Mount, IPC, UTS   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Capability Dropping                \u2502 \u2190 No privileged operations\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Cgroups                           \u2502 \u2190 Resource limits\n\u2502  - CPU quota, Memory limit         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  tmpfs (RAM-only storage)          \u2502 \u2190 No persistent storage\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Linux Kernel                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Hardware                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"security/#1-linux-namespaces","title":"1. Linux Namespaces","text":"<p>Each job runs in isolated namespaces:</p> <ul> <li>PID namespace: Job cannot see other processes</li> <li>Network namespace: No network access (airgapped)</li> <li>Mount namespace: Private filesystem view</li> <li>IPC namespace: No inter-process communication</li> <li>UTS namespace: Isolated hostname</li> </ul>"},{"location":"security/#2-seccomp-bpf-syscall-filtering","title":"2. Seccomp-BPF Syscall Filtering","text":"<p>Sandrun uses a whitelist of ~60 safe syscalls including:</p> <pre><code>// Allowed syscalls (examples)\nread, write, open, close, stat, fstat\nmmap, munmap, brk\nexit, exit_group\ngetpid, getuid, getgid\nfutex, nanosleep\n</code></pre> <p>Blocked syscalls (dangerous operations):</p> <pre><code>// Blocked syscalls\nsocket, connect, bind, listen  // Network access\nptrace, perf_event_open        // Process debugging\nreboot, kexec_load             // System control\nmodule_init, module_finit      // Kernel modules\n</code></pre>"},{"location":"security/#3-capability-dropping","title":"3. Capability Dropping","text":"<p>All Linux capabilities are dropped before execution:</p> <pre><code>CAP_SYS_ADMIN    // System administration\nCAP_NET_ADMIN    // Network administration\nCAP_SYS_MODULE   // Load kernel modules\n// ... and 35+ more capabilities\n</code></pre>"},{"location":"security/#4-resource-limits","title":"4. Resource Limits","text":"<p>Cgroups enforce hard limits:</p> <ul> <li>CPU: 10 seconds per minute per IP</li> <li>Memory: 512MB per job</li> <li>Timeout: 5 minutes maximum</li> <li>Processes: 100 processes per job</li> </ul>"},{"location":"security/#5-tmpfs-only-storage","title":"5. tmpfs-only Storage","text":"<p>All job data stored in RAM (tmpfs):</p> <ul> <li>No disk writes: Data never touches persistent storage</li> <li>Automatic cleanup: Memory released on job completion</li> <li>No swap: Memory pages locked to prevent disk swap</li> </ul>"},{"location":"security/#threat-model","title":"Threat Model","text":""},{"location":"security/#what-sandrun-protects-against","title":"What Sandrun Protects Against","text":""},{"location":"security/#filesystem-escape","title":"\u2705 Filesystem Escape","text":"<p>Jobs cannot access files outside their sandbox directory.</p> <pre><code># These will fail in sandbox:\nopen('/etc/passwd', 'r')           # Permission denied\nopen('/home/user/private.txt', 'r') # Not visible\n</code></pre>"},{"location":"security/#network-access","title":"\u2705 Network Access","text":"<p>Jobs cannot make network connections.</p> <pre><code># These will fail in sandbox:\nimport socket\nsocket.socket()  # Operation not permitted\n\nimport requests\nrequests.get('http://evil.com')  # No network\n</code></pre>"},{"location":"security/#process-interference","title":"\u2705 Process Interference","text":"<p>Jobs cannot see or interact with other processes.</p> <pre><code># These will fail in sandbox:\nimport os\nos.system('ps aux')  # Only sees own processes\nos.kill(1, 9)        # Cannot kill PID 1\n</code></pre>"},{"location":"security/#resource-exhaustion","title":"\u2705 Resource Exhaustion","text":"<p>Jobs cannot consume unlimited resources.</p> <pre><code># These will be killed:\nwhile True:\n    data.append([0] * 1000000)  # Exceeds memory limit\n\nimport os\nos.fork()  # Exceeds process limit\n</code></pre>"},{"location":"security/#data-persistence","title":"\u2705 Data Persistence","text":"<p>Job data automatically deleted after use.</p>"},{"location":"security/#what-sandrun-does-not-protect-against","title":"What Sandrun Does NOT Protect Against","text":""},{"location":"security/#kernel-vulnerabilities","title":"\u274c Kernel Vulnerabilities","text":"<p>If the Linux kernel has a vulnerability, sandbox escape is possible.</p> <p>Mitigation: Keep kernel updated with security patches.</p>"},{"location":"security/#timing-attacks","title":"\u274c Timing Attacks","text":"<p>Jobs sharing CPU cores could leak information via timing.</p> <p>Mitigation: Use dedicated hardware for sensitive workloads.</p>"},{"location":"security/#covert-channels","title":"\u274c Covert Channels","text":"<p>Advanced attacks (cache timing, speculative execution) could leak data.</p> <p>Mitigation: Not a concern for most use cases. Use dedicated hardware if needed.</p>"},{"location":"security/#physical-access","title":"\u274c Physical Access","text":"<p>Attacker with physical access can bypass all software security.</p> <p>Mitigation: Standard physical security measures.</p>"},{"location":"security/#deployment-best-practices","title":"Deployment Best Practices","text":""},{"location":"security/#system-hardening","title":"System Hardening","text":""},{"location":"security/#1-keep-system-updated","title":"1. Keep System Updated","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get upgrade -y\n\n# Check kernel version\nuname -r  # Should be recent version\n\n# Check for security updates\nsudo unattended-upgrade --dry-run\n</code></pre>"},{"location":"security/#2-use-security-modules","title":"2. Use Security Modules","text":"<p>Enable AppArmor or SELinux for additional protection:</p> <pre><code># AppArmor (Ubuntu/Debian)\nsudo apt-get install apparmor apparmor-utils\nsudo aa-status\n\n# SELinux (RHEL/Fedora)\nsestatus\n# Should show: SELinux status: enabled\n</code></pre>"},{"location":"security/#3-configure-firewall","title":"3. Configure Firewall","text":"<pre><code># Allow only necessary ports\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\nsudo ufw allow 8443/tcp  # Sandrun API\nsudo ufw enable\n</code></pre>"},{"location":"security/#4-disable-unnecessary-services","title":"4. Disable Unnecessary Services","text":"<pre><code># List running services\nsystemctl list-units --type=service --state=running\n\n# Disable unnecessary services\nsudo systemctl disable bluetooth\nsudo systemctl disable cups\n</code></pre>"},{"location":"security/#network-security","title":"Network Security","text":""},{"location":"security/#use-tls-in-production","title":"Use TLS in Production","text":"<pre><code># Generate self-signed certificate (testing)\nopenssl req -x509 -newkey rsa:4096 -nodes \\\n  -keyout key.pem -out cert.pem -days 365\n\n# Start Sandrun with TLS\nsudo ./build/sandrun --port 8443 \\\n  --cert cert.pem --key key.pem\n</code></pre> <p>Production TLS</p> <p>Use Let's Encrypt or a commercial CA for production certificates. <pre><code>sudo certbot certonly --standalone -d sandrun.example.com\nsudo ./build/sandrun --port 8443 \\\n  --cert /etc/letsencrypt/live/sandrun.example.com/fullchain.pem \\\n  --key /etc/letsencrypt/live/sandrun.example.com/privkey.pem\n</code></pre></p>"},{"location":"security/#reverse-proxy-setup","title":"Reverse Proxy Setup","text":"<p>Use nginx or Caddy for TLS termination:</p> <pre><code># /etc/nginx/sites-available/sandrun\nserver {\n    listen 443 ssl http2;\n    server_name sandrun.example.com;\n\n    ssl_certificate /etc/letsencrypt/live/sandrun.example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/sandrun.example.com/privkey.pem;\n\n    # Security headers\n    add_header Strict-Transport-Security \"max-age=31536000\" always;\n    add_header X-Frame-Options DENY always;\n    add_header X-Content-Type-Options nosniff always;\n\n    location / {\n        proxy_pass http://localhost:8443;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    # WebSocket support\n    location /logs/ {\n        proxy_pass http://localhost:8443;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre>"},{"location":"security/#access-control","title":"Access Control","text":""},{"location":"security/#ip-allowlisting","title":"IP Allowlisting","text":"<pre><code># Restrict access to specific IPs\nlocation /submit {\n    allow 192.168.1.0/24;\n    allow 10.0.0.0/8;\n    deny all;\n\n    proxy_pass http://localhost:8443;\n}\n</code></pre>"},{"location":"security/#api-authentication-custom","title":"API Authentication (Custom)","text":"<p>While Sandrun doesn't include built-in authentication, you can add it via reverse proxy:</p> <pre><code># Basic auth with nginx\nlocation / {\n    auth_basic \"Sandrun API\";\n    auth_basic_user_file /etc/nginx/.htpasswd;\n    proxy_pass http://localhost:8443;\n}\n</code></pre> <p>Or use OAuth2 proxy: <pre><code># OAuth2 Proxy for Google/GitHub auth\ndocker run -p 4180:4180 \\\n  -e OAUTH2_PROXY_CLIENT_ID=xxx \\\n  -e OAUTH2_PROXY_CLIENT_SECRET=yyy \\\n  quay.io/oauth2-proxy/oauth2-proxy\n</code></pre></p>"},{"location":"security/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"security/#system-monitoring","title":"System Monitoring","text":"<pre><code># Monitor resource usage\nhtop\n\n# Check Sandrun logs\njournalctl -u sandrun -f\n\n# Monitor failed jobs\ntail -f /var/log/sandrun/errors.log\n</code></pre>"},{"location":"security/#intrusion-detection","title":"Intrusion Detection","text":"<pre><code># Install AIDE (file integrity monitoring)\nsudo apt-get install aide\nsudo aideinit\nsudo aide --check\n\n# Install fail2ban (rate limit enforcement)\nsudo apt-get install fail2ban\n</code></pre>"},{"location":"security/#prometheus-metrics-custom-integration","title":"Prometheus Metrics (Custom Integration)","text":"<p>Add metrics endpoint to track:</p> <ul> <li>Jobs submitted per minute</li> <li>CPU quota usage per IP</li> <li>Memory usage per job</li> <li>Job failure rates</li> <li>Queue depth</li> </ul>"},{"location":"security/#worker-identity-verification","title":"Worker Identity &amp; Verification","text":"<p>For pool deployments, use worker identity for result verification:</p>"},{"location":"security/#1-generate-worker-keypair","title":"1. Generate Worker Keypair","text":"<pre><code>sudo ./build/sandrun --generate-key /etc/sandrun/worker.pem\n\n# Output:\n# \u2705 Saved worker key to: /etc/sandrun/worker.pem\n#    Worker ID: &lt;base64-public-key&gt;\n</code></pre>"},{"location":"security/#2-secure-private-key","title":"2. Secure Private Key","text":"<pre><code># Restrict permissions\nsudo chmod 600 /etc/sandrun/worker.pem\nsudo chown root:root /etc/sandrun/worker.pem\n\n# Use dedicated key storage\n# - Hardware Security Module (HSM)\n# - AWS KMS\n# - HashiCorp Vault\n</code></pre>"},{"location":"security/#3-verify-results","title":"3. Verify Results","text":"<pre><code>import base64\nimport nacl.signing\n\n# Get job result\nresult = requests.get(f'http://pool:9000/status/{job_id}').json()\n\nworker_id = result['worker_metadata']['worker_id']\nsignature = result['worker_metadata']['signature']\njob_hash = result['job_hash']\n\n# Verify signature\nverify_key = nacl.signing.VerifyKey(base64.b64decode(worker_id))\ntry:\n    verify_key.verify(job_hash.encode(), base64.b64decode(signature))\n    print(\"\u2705 Result verified!\")\nexcept nacl.exceptions.BadSignatureError:\n    print(\"\u274c Invalid signature - result tampered!\")\n</code></pre>"},{"location":"security/#code-execution-safety","title":"Code Execution Safety","text":""},{"location":"security/#input-validation","title":"Input Validation","text":"<p>Always validate user inputs before submitting to Sandrun:</p> <pre><code>import json\nimport tarfile\n\ndef validate_manifest(manifest_json):\n    \"\"\"Validate manifest before submission\"\"\"\n    try:\n        manifest = json.loads(manifest_json)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON\")\n\n    # Required fields\n    if 'entrypoint' not in manifest:\n        raise ValueError(\"Missing entrypoint\")\n\n    # Validate timeout\n    timeout = manifest.get('timeout', 300)\n    if timeout &gt; 3600 or timeout &lt; 1:\n        raise ValueError(\"Invalid timeout\")\n\n    # Validate memory\n    memory_mb = manifest.get('memory_mb', 512)\n    if memory_mb &gt; 2048 or memory_mb &lt; 64:\n        raise ValueError(\"Invalid memory limit\")\n\n    return manifest\n\ndef validate_tarball(tarball_path):\n    \"\"\"Validate tarball before submission\"\"\"\n    try:\n        with tarfile.open(tarball_path, 'r:gz') as tar:\n            # Check file count\n            members = tar.getmembers()\n            if len(members) &gt; 1000:\n                raise ValueError(\"Too many files\")\n\n            # Check total size\n            total_size = sum(m.size for m in members)\n            if total_size &gt; 100 * 1024 * 1024:  # 100MB\n                raise ValueError(\"Tarball too large\")\n\n            # Check for path traversal\n            for member in members:\n                if member.name.startswith('/') or '..' in member.name:\n                    raise ValueError(\"Invalid file path\")\n\n    except tarfile.TarError:\n        raise ValueError(\"Invalid tarball\")\n</code></pre>"},{"location":"security/#sanitize-outputs","title":"Sanitize Outputs","text":"<p>Filter sensitive information from job outputs:</p> <pre><code>import re\n\ndef sanitize_logs(logs):\n    \"\"\"Remove sensitive data from logs\"\"\"\n    # Remove potential secrets\n    logs = re.sub(r'API_KEY=\\S+', 'API_KEY=***', logs)\n    logs = re.sub(r'password=\\S+', 'password=***', logs)\n    logs = re.sub(r'token:\\s*\\S+', 'token: ***', logs)\n\n    # Remove IP addresses\n    logs = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', 'x.x.x.x', logs)\n\n    return logs\n</code></pre>"},{"location":"security/#incident-response","title":"Incident Response","text":""},{"location":"security/#detecting-compromises","title":"Detecting Compromises","text":"<p>Warning signs of potential compromise:</p> <ol> <li> <p>Unusual resource usage <pre><code># Check CPU usage\ntop -bn1 | head -20\n\n# Check memory\nfree -h\n\n# Check network connections (should be minimal)\nnetstat -tulpn\n</code></pre></p> </li> <li> <p>Suspicious job submissions</p> </li> <li>Very large tarballs</li> <li>Jobs with unusual interpreters</li> <li> <p>Rapid job submissions from single IP</p> </li> <li> <p>System logs <pre><code># Check kernel logs\ndmesg | tail -100\n\n# Check auth logs\ntail -f /var/log/auth.log\n\n# Check for failed namespace creation\njournalctl -k | grep namespace\n</code></pre></p> </li> </ol>"},{"location":"security/#response-procedures","title":"Response Procedures","text":"<p>If you suspect compromise:</p> <ol> <li> <p>Immediate Actions <pre><code># Stop Sandrun\nsudo systemctl stop sandrun\n\n# Kill all sandbox processes\nsudo pkill -9 -f sandrun\n\n# Check for running jobs\nps aux | grep sandbox\n</code></pre></p> </li> <li> <p>Investigation <pre><code># Collect system state\nps auxf &gt; /tmp/processes.txt\nnetstat -tulpn &gt; /tmp/netstat.txt\nlsof &gt; /tmp/open_files.txt\n\n# Check for kernel exploits\ndmesg &gt; /tmp/kernel.log\n</code></pre></p> </li> <li> <p>Recovery <pre><code># Update system\nsudo apt-get update\nsudo apt-get upgrade -y\n\n# Rebuild Sandrun from source\ngit pull\nrm -rf build\ncmake -B build\ncmake --build build\n\n# Restart with fresh configuration\nsudo systemctl start sandrun\n</code></pre></p> </li> </ol>"},{"location":"security/#security-checklist","title":"Security Checklist","text":"<p>Use this checklist for production deployments:</p> <ul> <li>[ ] System fully updated (kernel, packages)</li> <li>[ ] Firewall configured (only necessary ports open)</li> <li>[ ] TLS enabled with valid certificate</li> <li>[ ] Reverse proxy configured with security headers</li> <li>[ ] Monitoring and alerting configured</li> <li>[ ] Backup and recovery procedures documented</li> <li>[ ] Worker keys secured (if using pool)</li> <li>[ ] Access logs enabled and reviewed regularly</li> <li>[ ] Incident response plan documented</li> <li>[ ] Regular security audits scheduled</li> </ul>"},{"location":"security/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Found a security vulnerability? Do not open a public issue.</p> <ol> <li>Email security@sandrun.example.com</li> <li>Include:</li> <li>Description of vulnerability</li> <li>Steps to reproduce</li> <li>Impact assessment</li> <li> <p>Suggested fix (if any)</p> </li> <li> <p>Wait for response before public disclosure</p> </li> </ol> <p>We follow responsible disclosure and will:</p> <ul> <li>Acknowledge within 48 hours</li> <li>Provide fix timeline</li> <li>Credit reporters (if desired)</li> <li>Coordinate disclosure</li> </ul> <p>Questions about security? Contact us \u2192</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues and error messages.</p>"},{"location":"troubleshooting/#quick-diagnosis","title":"Quick Diagnosis","text":"<p>Start here to quickly identify your issue:</p> <pre><code># Check if Sandrun is running\nps aux | grep sandrun\n\n# Check if port is accessible\ncurl http://localhost:8443/\n\n# Check system resources\nfree -h\ndf -h\n\n# Check kernel support\ncat /proc/sys/kernel/seccomp  # Should be 2\nls /proc/self/ns/               # Should list namespaces\n\n# Check recent logs\njournalctl -u sandrun -n 50\n</code></pre>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#cmake-cannot-find-dependencies","title":"CMake Cannot Find Dependencies","text":"<p>Symptom: <pre><code>CMake Error: Could not find libseccomp\n</code></pre></p> <p>Solution:</p> Ubuntu/DebianFedora/RHELBuild from source <pre><code>sudo apt-get install -y \\\n  libseccomp-dev \\\n  libcap-dev \\\n  libssl-dev \\\n  pkg-config\n</code></pre> <pre><code>sudo dnf install -y \\\n  libseccomp-devel \\\n  libcap-devel \\\n  openssl-devel\n</code></pre> <pre><code># libseccomp\ngit clone https://github.com/seccomp/libseccomp\ncd libseccomp\n./autogen.sh\n./configure\nmake\nsudo make install\n</code></pre>"},{"location":"troubleshooting/#build-fails-with-compiler-errors","title":"Build Fails with Compiler Errors","text":"<p>Symptom: <pre><code>error: 'optional' is not a member of 'std'\n</code></pre></p> <p>Solution:</p> <pre><code># Check GCC version (need 7.0+)\ngcc --version\n\n# Update if needed (Ubuntu)\nsudo apt-get install g++-9\nexport CXX=g++-9\n\n# Clean and rebuild\nrm -rf build\ncmake -B build -DCMAKE_CXX_COMPILER=g++-9\ncmake --build build\n</code></pre>"},{"location":"troubleshooting/#cmake-version-too-old","title":"CMake Version Too Old","text":"<p>Symptom: <pre><code>CMake 3.10 or higher is required. You are running version 2.8\n</code></pre></p> <p>Solution:</p> <pre><code># Install latest CMake\nwget https://github.com/Kitware/CMake/releases/download/v3.25.0/cmake-3.25.0-linux-x86_64.sh\nchmod +x cmake-3.25.0-linux-x86_64.sh\nsudo ./cmake-3.25.0-linux-x86_64.sh --prefix=/usr/local --skip-license\n\n# Verify\ncmake --version\n</code></pre>"},{"location":"troubleshooting/#server-startup-issues","title":"Server Startup Issues","text":""},{"location":"troubleshooting/#permission-denied-creating-namespace","title":"Permission Denied Creating Namespace","text":"<p>Symptom: <pre><code>[ERROR] Failed to create namespace: Operation not permitted\n</code></pre></p> <p>Solutions:</p> Use sudoGrant capabilitiesCheck namespace support <pre><code>sudo ./build/sandrun --port 8443\n</code></pre> <pre><code>sudo setcap cap_sys_admin,cap_sys_chroot,cap_setuid,cap_setgid+ep ./build/sandrun\n./build/sandrun --port 8443\n</code></pre> <pre><code># Check if unprivileged user namespaces are enabled\ncat /proc/sys/kernel/unprivileged_userns_clone\n\n# Enable if disabled\nsudo sysctl -w kernel.unprivileged_userns_clone=1\n\n# Make permanent\necho 'kernel.unprivileged_userns_clone=1' | \\\n  sudo tee -a /etc/sysctl.conf\nsudo sysctl -p\n</code></pre>"},{"location":"troubleshooting/#port-already-in-use","title":"Port Already in Use","text":"<p>Symptom: <pre><code>[ERROR] Failed to bind to port 8443: Address already in use\n</code></pre></p> <p>Solutions:</p> <pre><code># Option 1: Find what's using the port\nsudo lsof -i :8443\nsudo netstat -tulpn | grep 8443\n\n# Option 2: Kill the process\nsudo kill &lt;PID&gt;\n\n# Option 3: Use different port\nsudo ./build/sandrun --port 9000\n\n# Option 4: Stop existing Sandrun instance\nsudo systemctl stop sandrun\n# or\nsudo pkill -f sandrun\n</code></pre>"},{"location":"troubleshooting/#seccomp-not-supported","title":"Seccomp Not Supported","text":"<p>Symptom: <pre><code>[ERROR] Seccomp not supported on this kernel\n</code></pre></p> <p>Solution:</p> <pre><code># Check seccomp support\ncat /proc/sys/kernel/seccomp\n# Should output: 2\n\n# If 0, rebuild kernel with CONFIG_SECCOMP=y\n# Or use a distribution with seccomp enabled\n\n# Check kernel version (need 4.6+)\nuname -r\n\n# Update kernel if needed\nsudo apt-get install linux-generic-hwe-20.04\nsudo reboot\n</code></pre>"},{"location":"troubleshooting/#cannot-open-worker-key-file","title":"Cannot Open Worker Key File","text":"<p>Symptom: <pre><code>[ERROR] Failed to open worker key: /etc/sandrun/worker.pem: No such file or directory\n</code></pre></p> <p>Solution:</p> <pre><code># Generate new worker key\nsudo mkdir -p /etc/sandrun\nsudo ./build/sandrun --generate-key /etc/sandrun/worker.pem\n\n# Or skip worker key if not using pools\nsudo ./build/sandrun --port 8443  # No --worker-key flag\n</code></pre>"},{"location":"troubleshooting/#job-submission-issues","title":"Job Submission Issues","text":""},{"location":"troubleshooting/#invalid-manifest","title":"Invalid Manifest","text":"<p>Symptom: <pre><code>{\n  \"error\": \"Invalid manifest\",\n  \"details\": \"Missing required field: entrypoint\"\n}\n</code></pre></p> <p>Solution:</p> <pre><code># Validate JSON syntax\necho '{\"entrypoint\":\"main.py\"}' | jq .\n\n# Minimum valid manifest\ncat &gt; manifest.json &lt;&lt;EOF\n{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\"\n}\nEOF\n\n# Submit with manifest\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F \"manifest=$(cat manifest.json)\"\n</code></pre>"},{"location":"troubleshooting/#tarball-too-large","title":"Tarball Too Large","text":"<p>Symptom: <pre><code>{\n  \"error\": \"Upload too large\",\n  \"details\": \"Maximum size: 100MB\"\n}\n</code></pre></p> <p>Solutions:</p> <pre><code># Check tarball size\nls -lh job.tar.gz\n\n# Compress better\ntar czf job.tar.gz --best my_project/\n\n# Remove unnecessary files\ntar czf job.tar.gz \\\n  --exclude='*.pyc' \\\n  --exclude='__pycache__' \\\n  --exclude='.git' \\\n  --exclude='node_modules' \\\n  my_project/\n\n# Split into multiple jobs if needed\n</code></pre>"},{"location":"troubleshooting/#files-missing-in-tarball","title":"Files Missing in Tarball","text":"<p>Symptom: Job fails with <code>FileNotFoundError: main.py</code></p> <p>Solution:</p> <pre><code># List tarball contents\ntar -tzf job.tar.gz\n\n# Ensure entrypoint is included\ntar -tzf job.tar.gz | grep main.py\n\n# Create tarball correctly\ncd project_directory\ntar czf ../job.tar.gz .\n# Not: tar czf job.tar.gz project_directory\n</code></pre>"},{"location":"troubleshooting/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<p>Symptom: <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"reason\": \"CPU quota exhausted (10.2/10.0 seconds used)\",\n  \"retry_after\": 45\n}\n</code></pre></p> <p>Solutions:</p> <pre><code># Check your quota\ncurl http://localhost:8443/stats\n\n# Wait for quota to reset\nsleep 60\n\n# Optimize code to use less CPU\n# Split long jobs into smaller chunks\n# Use more efficient algorithms\n</code></pre>"},{"location":"troubleshooting/#job-execution-issues","title":"Job Execution Issues","text":""},{"location":"troubleshooting/#job-failed-with-exit-code-1","title":"Job Failed with Exit Code 1","text":"<p>Symptom: <pre><code>{\n  \"status\": \"failed\",\n  \"exit_code\": 1\n}\n</code></pre></p> <p>Diagnosis:</p> <pre><code># Get full logs\ncurl http://localhost:8443/logs/job-abc123\n\n# Common causes:\n# - Syntax error in code\n# - Missing dependency\n# - File not found\n# - Permission error\n</code></pre> <p>Solutions:</p> <pre><code># Test locally first\ncd project_directory\npython3 main.py  # Test before submitting\n\n# Add debugging\ncat &gt; main.py &lt;&lt;EOF\nimport sys\nprint(\"Python version:\", sys.version)\nprint(\"Working directory:\", os.getcwd())\nprint(\"Files:\", os.listdir('.'))\n# ... your code ...\nEOF\n</code></pre>"},{"location":"troubleshooting/#job-killed-exit-code-137","title":"Job Killed (Exit Code 137)","text":"<p>Symptom: <pre><code>{\n  \"status\": \"failed\",\n  \"exit_code\": 137,\n  \"details\": \"Killed by signal 9\"\n}\n</code></pre></p> <p>Causes:</p> <ul> <li>Out of memory (exceeded 512MB limit)</li> <li>Timeout (exceeded 5 minute limit)</li> </ul> <p>Solutions:</p> <pre><code># Increase memory limit in manifest\ncat &gt; manifest.json &lt;&lt;EOF\n{\n  \"entrypoint\": \"main.py\",\n  \"memory_mb\": 1024\n}\nEOF\n\n# Increase timeout\ncat &gt; manifest.json &lt;&lt;EOF\n{\n  \"entrypoint\": \"main.py\",\n  \"timeout\": 600\n}\nEOF\n\n# Optimize memory usage\n# - Use generators instead of lists\n# - Process data in chunks\n# - Delete large objects when done\n</code></pre>"},{"location":"troubleshooting/#permission-denied-inside-sandbox","title":"Permission Denied Inside Sandbox","text":"<p>Symptom: <pre><code>PermissionError: [Errno 13] Permission denied: '/etc/passwd'\n</code></pre></p> <p>Explanation:</p> <p>This is expected behavior. The sandbox restricts access to:</p> <ul> <li>Host filesystem (only job directory accessible)</li> <li>Network (completely blocked)</li> <li>System files (<code>/etc</code>, <code>/proc</code>, <code>/sys</code> read-only)</li> </ul> <p>Solutions:</p> <pre><code># Copy needed files into job directory\ncp /etc/hosts my_project/hosts\ntar czf job.tar.gz my_project/\n\n# In your code, use relative paths\nwith open('hosts', 'r') as f:  # Not /etc/hosts\n    data = f.read()\n</code></pre>"},{"location":"troubleshooting/#import-errors-missing-dependencies","title":"Import Errors (Missing Dependencies)","text":"<p>Symptom: <pre><code>ModuleNotFoundError: No module named 'numpy'\n</code></pre></p> <p>Solutions:</p> Use requirements.txtUse pre-built environmentInclude dependencies in tarball <pre><code># Create requirements.txt\ncat &gt; requirements.txt &lt;&lt;EOF\nnumpy==1.24.0\npandas==1.5.0\nEOF\n\n# Add to manifest\ncat &gt; manifest.json &lt;&lt;EOF\n{\n  \"entrypoint\": \"main.py\",\n  \"requirements\": \"requirements.txt\"\n}\nEOF\n</code></pre> <pre><code># List available environments\ncurl http://localhost:8443/environments\n\n# Use in manifest\ncat &gt; manifest.json &lt;&lt;EOF\n{\n  \"entrypoint\": \"main.py\",\n  \"environment\": \"ml-basic\"\n}\nEOF\n</code></pre> <pre><code># Install to local directory\npip install -t ./libs numpy pandas\n\n# In your code\nimport sys\nsys.path.insert(0, './libs')\nimport numpy\n</code></pre>"},{"location":"troubleshooting/#job-stuck-in-queued-status","title":"Job Stuck in \"queued\" Status","text":"<p>Symptom: Job never starts executing.</p> <p>Diagnosis:</p> <pre><code># Check system stats\ncurl http://localhost:8443/stats\n\n# Response shows:\n# \"queue_length\": 10  # Many queued jobs\n# \"active_jobs\": 2    # System busy\n</code></pre> <p>Causes:</p> <ul> <li>Too many concurrent jobs (2 per IP limit)</li> <li>System overloaded</li> <li>No available workers (if using pool)</li> </ul> <p>Solutions:</p> <pre><code># Wait for active jobs to complete\n# Or cancel queued jobs if needed\n\n# Check worker health (pool deployments)\ncurl http://pool:9000/pool\n</code></pre>"},{"location":"troubleshooting/#cannot-download-output-files","title":"Cannot Download Output Files","text":"<p>Symptom: <pre><code>{\n  \"error\": \"Job not found or expired\"\n}\n</code></pre></p> <p>Causes:</p> <ul> <li>Job auto-deleted after 1 hour</li> <li>Already downloaded (immediate deletion)</li> <li>Job failed (deleted after 5 minutes)</li> </ul> <p>Solutions:</p> <pre><code># Download immediately after completion\n# Check status first\nSTATUS=$(curl -s http://localhost:8443/status/job-abc123 | jq -r '.status')\n\nif [ \"$STATUS\" = \"completed\" ]; then\n  curl http://localhost:8443/download/job-abc123/output.txt -o output.txt\nfi\n\n# Use WebSocket streaming to monitor completion\n</code></pre>"},{"location":"troubleshooting/#pool-coordinator-issues","title":"Pool Coordinator Issues","text":""},{"location":"troubleshooting/#no-available-workers","title":"No Available Workers","text":"<p>Symptom: <pre><code>WARNING:__main__:No available workers for job pool-xxx\n</code></pre></p> <p>Diagnosis:</p> <pre><code># Check pool status\ncurl http://pool:9000/pool\n\n# Response shows:\n# \"healthy_workers\": 0\n</code></pre> <p>Causes:</p> <ul> <li>All workers offline</li> <li>Workers failed health check</li> <li>Workers at max capacity</li> </ul> <p>Solutions:</p> <pre><code># Check worker health directly\ncurl http://worker1:8443/health\n\n# Start more workers\nsudo ./build/sandrun --port 8443 --worker-key /etc/sandrun/worker.pem\n\n# Check worker logs\njournalctl -u sandrun -f\n\n# Verify workers.json configuration\ncat workers.json\n# Ensure worker IDs and endpoints are correct\n</code></pre>"},{"location":"troubleshooting/#worker-authentication-failed","title":"Worker Authentication Failed","text":"<p>Symptom: <pre><code>ERROR:__main__:Health check failed for worker: Invalid worker_id\n</code></pre></p> <p>Solution:</p> <pre><code># Regenerate worker key\nsudo ./build/sandrun --generate-key /etc/sandrun/worker.pem\n\n# Copy output Worker ID\n# Update workers.json with new worker_id\n\n# Restart worker\nsudo systemctl restart sandrun\n</code></pre>"},{"location":"troubleshooting/#jobs-stuck-in-pool-queue","title":"Jobs Stuck in Pool Queue","text":"<p>Symptom: Jobs never dispatched to workers.</p> <p>Diagnosis:</p> <pre><code># Check pool logs\njournalctl -u pool-coordinator -f\n\n# Check worker endpoints\nfor worker in worker1 worker2 worker3; do\n  echo \"Testing $worker:\"\n  curl http://$worker:8443/health\ndone\n</code></pre> <p>Solutions:</p> <pre><code># Verify network connectivity\nping worker1\ntelnet worker1 8443\n\n# Check firewall rules\nsudo iptables -L -n\n\n# Ensure workers are reachable from coordinator\n# Update workers.json with correct endpoints\n</code></pre>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-job-execution","title":"Slow Job Execution","text":"<p>Diagnosis:</p> <pre><code># Check system load\nuptime\ntop\n\n# Check I/O wait\niostat -x 1\n\n# Check memory pressure\nfree -h\nvmstat 1\n</code></pre> <p>Solutions:</p> <pre><code># Increase system resources\n# Add more RAM\n# Use faster CPU\n# Add more workers for horizontal scaling\n\n# Optimize job code\n# Use compiled languages for CPU-intensive tasks\n# Minimize disk I/O\n# Use efficient algorithms\n</code></pre>"},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Diagnosis:</p> <pre><code># Check memory usage\nfree -h\n\n# Check tmpfs usage\ndf -h /dev/shm\nmount | grep tmpfs\n\n# Check per-process memory\nps aux --sort=-%mem | head -20\n</code></pre> <p>Solutions:</p> <pre><code># Increase system RAM\n# Reduce concurrent job limit\n# Reduce per-job memory limit\n# Optimize job code for memory efficiency\n\n# Clean up old jobs manually if needed\nsudo systemctl restart sandrun\n</code></pre>"},{"location":"troubleshooting/#debugging-tools","title":"Debugging Tools","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># Build with debug symbols\ncmake -B build -DCMAKE_BUILD_TYPE=Debug\ncmake --build build\n\n# Run with verbose logging\nsudo ./build/sandrun --port 8443 --verbose\n\n# Or set environment variable\nexport SANDRUN_LOG_LEVEL=debug\nsudo -E ./build/sandrun --port 8443\n</code></pre>"},{"location":"troubleshooting/#use-gdb-for-crashes","title":"Use GDB for Crashes","text":"<pre><code># Build with debug symbols\ncmake -B build -DCMAKE_BUILD_TYPE=Debug\ncmake --build build\n\n# Run under GDB\nsudo gdb ./build/sandrun\n(gdb) run --port 8443\n# Wait for crash\n(gdb) backtrace\n(gdb) info locals\n</code></pre>"},{"location":"troubleshooting/#memory-leak-detection","title":"Memory Leak Detection","text":"<pre><code># Run with Valgrind\nsudo valgrind \\\n  --leak-check=full \\\n  --show-leak-kinds=all \\\n  --track-origins=yes \\\n  --verbose \\\n  --log-file=valgrind.log \\\n  ./build/sandrun --port 8443\n\n# Analyze results\ncat valgrind.log\n</code></pre>"},{"location":"troubleshooting/#network-debugging","title":"Network Debugging","text":"<pre><code># Monitor HTTP traffic\nsudo tcpdump -i any -nn -A port 8443\n\n# Or use Wireshark\nsudo wireshark\n\n# Test with verbose curl\ncurl -v http://localhost:8443/\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li> <p>Check logs: <pre><code>journalctl -u sandrun -n 100\ndmesg | tail -50\n</code></pre></p> </li> <li> <p>Gather system info: <pre><code>./build/sandrun --version\nuname -a\ncat /etc/os-release\n</code></pre></p> </li> <li> <p>Search existing issues: GitHub Issues</p> </li> <li> <p>Ask for help:</p> </li> <li>GitHub Discussions</li> <li> <p>Include: OS, kernel version, error messages, logs</p> </li> <li> <p>Report bugs:</p> </li> <li>File an issue</li> <li>Include: steps to reproduce, expected vs actual behavior</li> </ol> <p>Still need help? Open an issue \u2192</p>"},{"location":"development/building/","title":"Building Sandrun","text":"<p>Guide for building and developing Sandrun.</p>"},{"location":"development/building/#prerequisites","title":"Prerequisites","text":""},{"location":"development/building/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: Linux (Ubuntu 20.04+, Debian 11+)</li> <li>Kernel: 4.6+ (for namespace support)</li> <li>RAM: 2GB minimum</li> <li>Disk: 500MB for build artifacts</li> </ul>"},{"location":"development/building/#dependencies","title":"Dependencies","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y \\\n  build-essential \\\n  cmake \\\n  libseccomp-dev \\\n  libcap-dev \\\n  libssl-dev \\\n  pkg-config \\\n  git\n\n# Optional: for testing\nsudo apt-get install -y \\\n  libgtest-dev \\\n  lcov \\\n  python3-pip\n\n# Optional: for documentation\npip3 install mkdocs-material\n</code></pre>"},{"location":"development/building/#building","title":"Building","text":""},{"location":"development/building/#standard-build","title":"Standard Build","text":"<pre><code># Configure\ncmake -B build\n\n# Build\ncmake --build build\n\n# Output: build/sandrun\n</code></pre>"},{"location":"development/building/#debug-build","title":"Debug Build","text":"<pre><code># Configure with debug symbols\ncmake -B build -DCMAKE_BUILD_TYPE=Debug\n\n# Build\ncmake --build build\n\n# Run with GDB\nsudo gdb ./build/sandrun\n</code></pre>"},{"location":"development/building/#release-build","title":"Release Build","text":"<pre><code># Configure with optimizations\ncmake -B build -DCMAKE_BUILD_TYPE=Release\n\n# Build\ncmake --build build\n\n# Output is optimized and stripped\n</code></pre>"},{"location":"development/building/#build-options","title":"Build Options","text":"<pre><code># Enable all warnings\ncmake -B build -DCMAKE_CXX_FLAGS=\"-Wall -Wextra -Wpedantic\"\n\n# Enable address sanitizer (debugging memory issues)\ncmake -B build -DCMAKE_CXX_FLAGS=\"-fsanitize=address\"\n\n# Static analysis\ncmake -B build -DCMAKE_CXX_CLANG_TIDY=clang-tidy\n</code></pre>"},{"location":"development/building/#running-tests","title":"Running Tests","text":""},{"location":"development/building/#unit-tests","title":"Unit Tests","text":"<pre><code># Build tests\ncmake -B build\ncmake --build build\n\n# Run unit tests\n./build/tests/unit_tests\n\n# Run with verbose output\n./build/tests/unit_tests --gtest_color=yes --gtest_output=xml:test_results.xml\n</code></pre>"},{"location":"development/building/#integration-tests","title":"Integration Tests","text":"<pre><code># Integration tests may require sudo\nsudo ./build/tests/integration_tests\n</code></pre>"},{"location":"development/building/#test-coverage","title":"Test Coverage","text":"<pre><code># Build with coverage\ncmake -B build-coverage -DCMAKE_BUILD_TYPE=Debug -DCMAKE_CXX_FLAGS=\"--coverage\"\ncmake --build build-coverage\n\n# Run tests\n./build-coverage/tests/unit_tests\n\n# Generate coverage report\nlcov --directory build-coverage --capture --output-file coverage.info\nlcov --remove coverage.info '/usr/*' '*/tests/*' --output-file coverage.info\ngenhtml coverage.info --output-directory coverage_report\n\n# View report\nfirefox coverage_report/index.html\n</code></pre>"},{"location":"development/building/#test-script","title":"Test Script","text":"<pre><code># Run all tests and generate coverage\n./scripts/run_tests.sh\n</code></pre>"},{"location":"development/building/#development-workflow","title":"Development Workflow","text":""},{"location":"development/building/#code-style","title":"Code Style","text":"<p>Follow Google C++ Style Guide:</p> <ul> <li>2-space indentation</li> <li>Snake_case for variables</li> <li>PascalCase for classes</li> <li>UPPER_CASE for constants</li> <li><code>#pragma once</code> for header guards</li> </ul>"},{"location":"development/building/#static-analysis","title":"Static Analysis","text":"<pre><code># clang-tidy\nclang-tidy src/*.cpp -- -std=c++17\n\n# cppcheck\ncppcheck --enable=all src/\n</code></pre>"},{"location":"development/building/#format-code","title":"Format Code","text":"<pre><code># clang-format\nfind src/ -name \"*.cpp\" -o -name \"*.h\" | xargs clang-format -i\n</code></pre>"},{"location":"development/building/#project-structure","title":"Project Structure","text":"<pre><code>sandrun/\n\u251c\u2500\u2500 CMakeLists.txt           Build configuration\n\u251c\u2500\u2500 src/                     C++ source code\n\u2502   \u251c\u2500\u2500 main.cpp            Entry point\n\u2502   \u251c\u2500\u2500 sandbox.{h,cpp}     Sandbox implementation\n\u2502   \u251c\u2500\u2500 http_server.{h,cpp} HTTP server\n\u2502   \u251c\u2500\u2500 job_executor.{h,cpp} Job execution\n\u2502   \u251c\u2500\u2500 worker_identity.{h,cpp} Ed25519 signing\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tests/                   Test suite\n\u2502   \u251c\u2500\u2500 unit/               Unit tests\n\u2502   \u2514\u2500\u2500 integration/        Integration tests\n\u251c\u2500\u2500 integrations/            Integrations\n\u251c\u2500\u2500 docs/                    Documentation\n\u2514\u2500\u2500 scripts/                 Helper scripts\n</code></pre>"},{"location":"development/building/#debugging","title":"Debugging","text":""},{"location":"development/building/#common-issues","title":"Common Issues","text":"<p>Permission Denied:</p> <pre><code># Namespaces require root\nsudo ./build/sandrun --port 8443\n</code></pre> <p>Seccomp Errors:</p> <pre><code># Check kernel support\ncat /proc/sys/kernel/seccomp\n# Should output: 2\n\n# Run without seccomp (debugging only)\n# Edit src/sandbox.cpp to disable seccomp\n</code></pre> <p>Memory Leaks:</p> <pre><code># Run with valgrind\nsudo valgrind --leak-check=full ./build/sandrun --port 8443\n</code></pre>"},{"location":"development/building/#gdb-debugging","title":"GDB Debugging","text":"<pre><code># Build with debug symbols\ncmake -B build -DCMAKE_BUILD_TYPE=Debug\ncmake --build build\n\n# Run with GDB\nsudo gdb ./build/sandrun\n\n# GDB commands:\n(gdb) break main\n(gdb) run --port 8443\n(gdb) backtrace\n(gdb) print variable_name\n</code></pre>"},{"location":"development/building/#contributing","title":"Contributing","text":""},{"location":"development/building/#development-cycle","title":"Development Cycle","text":"<ol> <li>Create Feature Branch</li> </ol> <pre><code>git checkout -b feature/my-feature\n</code></pre> <ol> <li>Write Tests First (TDD)</li> </ol> <pre><code># Add test to tests/unit/test_myfeature.cpp\n# Run and verify it fails\n./build/tests/unit_tests --gtest_filter=MyFeatureTest.*\n</code></pre> <ol> <li>Implement Feature</li> </ol> <pre><code># Implement in src/\n# Run tests again\n./build/tests/unit_tests --gtest_filter=MyFeatureTest.*\n</code></pre> <ol> <li>Run All Tests</li> </ol> <pre><code>./scripts/run_tests.sh\n</code></pre> <ol> <li>Commit</li> </ol> <pre><code>git add -A\ngit commit -m \"feat: Add my feature\n\n- Implemented X\n- Added tests\n- Updated docs\"\n</code></pre> <ol> <li>Submit PR</li> </ol> <pre><code>git push origin feature/my-feature\n# Create pull request on GitHub\n</code></pre>"},{"location":"development/building/#code-review-checklist","title":"Code Review Checklist","text":"<ul> <li>\u2705 Tests pass</li> <li>\u2705 Code follows style guide</li> <li>\u2705 No memory leaks (valgrind clean)</li> <li>\u2705 Documentation updated</li> <li>\u2705 No compiler warnings</li> </ul>"},{"location":"development/building/#release-process","title":"Release Process","text":""},{"location":"development/building/#version-bumping","title":"Version Bumping","text":"<ol> <li>Update version in <code>CMakeLists.txt</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Tag release:</li> </ol> <pre><code>git tag -a v1.0.0 -m \"Release v1.0.0\"\ngit push origin v1.0.0\n</code></pre>"},{"location":"development/building/#building-release","title":"Building Release","text":"<pre><code># Clean build\nrm -rf build\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n\n# Create tarball\ntar czf sandrun-v1.0.0-linux-x86_64.tar.gz \\\n  build/sandrun \\\n  README.md \\\n  LICENSE\n</code></pre>"},{"location":"development/building/#cicd","title":"CI/CD","text":""},{"location":"development/building/#github-actions","title":"GitHub Actions","text":"<p>See <code>.github/workflows/</code> for CI configuration.</p> <p>Tests run automatically on: - Every push - Every pull request - Release tags</p>"},{"location":"development/building/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Guide</li> <li>Architecture</li> <li>Contributing Guidelines</li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive guide to testing Sandrun.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 unit/                    Unit tests\n\u2502   \u251c\u2500\u2500 test_file_utils.cpp\n\u2502   \u251c\u2500\u2500 test_worker_identity.cpp\n\u2502   \u251c\u2500\u2500 test_sandbox.cpp\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 integration/             Integration tests\n    \u251c\u2500\u2500 test_job_verification.cpp\n    \u251c\u2500\u2500 test_worker_signing.cpp\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#quick-start","title":"Quick Start","text":"<pre><code># Build and run all tests\n./scripts/run_tests.sh\n</code></pre>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<pre><code># Build tests\ncmake --build build\n\n# Run all unit tests\n./build/tests/unit_tests\n\n# Run specific test suite\n./build/tests/unit_tests --gtest_filter=FileUtilsTest.*\n\n# Run specific test\n./build/tests/unit_tests --gtest_filter=FileUtilsTest.SHA256File\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<pre><code># Integration tests may require sudo\nsudo ./build/tests/integration_tests\n\n# Run specific integration test\nsudo ./build/tests/integration_tests --gtest_filter=JobVerificationTest.*\n</code></pre>"},{"location":"development/testing/#with-output","title":"With Output","text":"<pre><code># Verbose output\n./build/tests/unit_tests --gtest_color=yes\n\n# XML output (for CI)\n./build/tests/unit_tests --gtest_output=xml:test_results.xml\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code># Build with coverage flags\ncmake -B build-coverage \\\n  -DCMAKE_BUILD_TYPE=Debug \\\n  -DCMAKE_CXX_FLAGS=\"--coverage\"\n\ncmake --build build-coverage\n\n# Run tests\n./build-coverage/tests/unit_tests\n./build-coverage/tests/integration_tests\n\n# Generate report\nlcov --directory build-coverage --capture --output-file coverage.info\nlcov --remove coverage.info '/usr/*' '*/tests/*' --output-file coverage.info\ngenhtml coverage.info --output-directory coverage_report\n\n# View report\nfirefox coverage_report/index.html\n</code></pre>"},{"location":"development/testing/#coverage-targets","title":"Coverage Targets","text":"<ul> <li>Critical paths: 100% (crypto, verification, sandbox)</li> <li>Core features: 90%+ (job execution, HTTP server)</li> <li>Overall: 80%+</li> </ul>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#unit-test-example","title":"Unit Test Example","text":"<pre><code>#include &lt;gtest/gtest.h&gt;\n#include \"file_utils.h\"\n\nusing namespace sandrun;\n\nTEST(FileUtilsTest, SHA256File) {\n    // Create temporary file\n    std::string test_file = \"/tmp/test_sha256.txt\";\n    std::ofstream f(test_file);\n    f &lt;&lt; \"Hello, World!\";\n    f.close();\n\n    // Calculate hash\n    std::string hash = FileUtils::sha256_file(test_file);\n\n    // Verify against known SHA256\n    EXPECT_EQ(hash, \"dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f\");\n\n    // Cleanup\n    std::remove(test_file.c_str());\n}\n\nTEST(FileUtilsTest, SHA256FileNotFound) {\n    std::string hash = FileUtils::sha256_file(\"/nonexistent/file.txt\");\n    EXPECT_EQ(hash, \"\");\n}\n</code></pre>"},{"location":"development/testing/#integration-test-example","title":"Integration Test Example","text":"<pre><code>#include &lt;gtest/gtest.h&gt;\n#include \"sandbox.h\"\n#include \"job_executor.h\"\n\nTEST(SandboxIntegrationTest, PythonExecution) {\n    // Create test job\n    Job job;\n    job.job_id = \"test-job-123\";\n    job.entrypoint = \"test.py\";\n    job.interpreter = \"python3\";\n    job.working_dir = \"/tmp/test_sandbox\";\n\n    // Create job files\n    std::filesystem::create_directories(job.working_dir);\n    std::ofstream script(job.working_dir + \"/test.py\");\n    script &lt;&lt; \"print('Test passed!')\";\n    script.close();\n\n    // Execute in sandbox\n    Sandbox sandbox;\n    JobResult result = sandbox.execute(job);\n\n    // Verify results\n    EXPECT_EQ(result.exit_code, 0);\n    EXPECT_TRUE(result.output.find(\"Test passed!\") != std::string::npos);\n\n    // Cleanup\n    std::filesystem::remove_all(job.working_dir);\n}\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#1-unit-tests","title":"1. Unit Tests","text":"<p>File Utils (<code>test_file_utils.cpp</code>) - SHA256 hashing (35 tests) - File metadata extraction - Directory hashing - Glob pattern matching</p> <p>Worker Identity (<code>test_worker_identity.cpp</code>) - Keypair generation (30 tests) - PEM file I/O - Signing and verification - Tampering detection</p> <p>Sandbox (<code>test_sandbox.cpp</code>) - Namespace creation - Seccomp filtering - Resource limits - Cleanup</p>"},{"location":"development/testing/#2-integration-tests","title":"2. Integration Tests","text":"<p>Job Verification (<code>test_job_verification.cpp</code>) - End-to-end job hash calculation (19 tests) - Output file hashing - JSON response format - Verification workflows</p> <p>Worker Signing (<code>test_worker_signing.cpp</code>) - Job signature generation (20 tests) - Signature verification - Tampering detection - Cross-worker verification</p> <p>Pool Integration (<code>integrations/trusted-pool/test_pool.sh</code>) - Worker health checks - Job distribution - Status proxying - Output download</p>"},{"location":"development/testing/#test-data","title":"Test Data","text":""},{"location":"development/testing/#fixtures","title":"Fixtures","text":"<pre><code>class SandboxTestFixture : public ::testing::Test {\nprotected:\n    void SetUp() override {\n        // Create test environment\n        test_dir = \"/tmp/sandrun_test_\" + random_string();\n        std::filesystem::create_directories(test_dir);\n    }\n\n    void TearDown() override {\n        // Cleanup\n        std::filesystem::remove_all(test_dir);\n    }\n\n    std::string test_dir;\n};\n\nTEST_F(SandboxTestFixture, IsolatedExecution) {\n    // Test uses test_dir from fixture\n}\n</code></pre>"},{"location":"development/testing/#mock-data","title":"Mock Data","text":"<pre><code>// Mock job\nJob create_test_job() {\n    Job job;\n    job.job_id = \"test-abc123\";\n    job.entrypoint = \"main.py\";\n    job.interpreter = \"python3\";\n    job.timeout = 60;\n    job.memory_mb = 256;\n    return job;\n}\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y libseccomp-dev libcap-dev libssl-dev libgtest-dev\n\n      - name: Build\n        run: |\n          cmake -B build\n          cmake --build build\n\n      - name: Run tests\n        run: |\n          ./build/tests/unit_tests --gtest_output=xml:test_results.xml\n\n      - name: Upload results\n        uses: actions/upload-artifact@v3\n        with:\n          name: test-results\n          path: test_results.xml\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmarks","title":"Benchmarks","text":"<pre><code>#include &lt;benchmark/benchmark.h&gt;\n\nstatic void BM_SHA256(benchmark::State&amp; state) {\n    std::string data(state.range(0), 'x');\n\n    for (auto _ : state) {\n        FileUtils::sha256_string(data);\n    }\n\n    state.SetBytesProcessed(state.iterations() * data.size());\n}\n\nBENCHMARK(BM_SHA256)-&gt;Range(1&lt;&lt;10, 1&lt;&lt;20);  // 1KB to 1MB\n</code></pre>"},{"location":"development/testing/#load-testing","title":"Load Testing","text":"<pre><code># Concurrent job submissions\nfor i in {1..100}; do\n    curl -X POST http://localhost:8443/submit \\\n      -F \"files=@test.tar.gz\" \\\n      -F 'manifest={\"entrypoint\":\"test.py\"}' &amp;\ndone\n\nwait\n</code></pre>"},{"location":"development/testing/#debugging-failed-tests","title":"Debugging Failed Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Run with verbose flag\n./build/tests/unit_tests --gtest_color=yes --gtest_print_time=1\n</code></pre>"},{"location":"development/testing/#repeat-failed-test","title":"Repeat Failed Test","text":"<pre><code># Run specific failed test\n./build/tests/unit_tests --gtest_filter=FailedTest --gtest_repeat=10\n</code></pre>"},{"location":"development/testing/#valgrind-check","title":"Valgrind Check","text":"<pre><code># Check for memory leaks\nvalgrind --leak-check=full ./build/tests/unit_tests --gtest_filter=FailedTest\n</code></pre>"},{"location":"development/testing/#test-best-practices","title":"Test Best Practices","text":""},{"location":"development/testing/#1-test-behavior-not-implementation","title":"1. Test Behavior, Not Implementation","text":"<p>\u274c Bad: <pre><code>TEST(JobExecutorTest, UsesCorrectDataStructure) {\n    EXPECT_TRUE(executor.uses_vector());  // Testing implementation\n}\n</code></pre></p> <p>\u2705 Good: <pre><code>TEST(JobExecutorTest, ExecutesJobsInOrder) {\n    executor.submit(job1);\n    executor.submit(job2);\n    EXPECT_EQ(executor.next(), job1);  // Testing behavior\n}\n</code></pre></p>"},{"location":"development/testing/#2-isolated-tests","title":"2. Isolated Tests","text":"<p>Each test should be independent:</p> <pre><code>TEST(FileUtilsTest, Test1) {\n    // Create own temp file\n    // Test logic\n    // Clean up\n}\n\nTEST(FileUtilsTest, Test2) {\n    // Don't depend on Test1\n}\n</code></pre>"},{"location":"development/testing/#3-descriptive-names","title":"3. Descriptive Names","text":"<pre><code>// Good test names describe what they test\nTEST(SandboxTest, FailsWhenEntrypointMissing)\nTEST(SandboxTest, EnforcesMemoryLimit)\nTEST(SandboxTest, IsolatesNetworkAccess)\n</code></pre>"},{"location":"development/testing/#4-aaa-pattern","title":"4. AAA Pattern","text":"<pre><code>TEST(Example, DoSomething) {\n    // Arrange: Setup test data\n    Job job = create_test_job();\n\n    // Act: Perform action\n    Result result = execute(job);\n\n    // Assert: Verify outcome\n    EXPECT_EQ(result.exit_code, 0);\n}\n</code></pre>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Building Guide</li> <li>Architecture</li> <li>Contributing</li> </ul>"},{"location":"integrations/broker/","title":"Sandrun Broker - Simple Job Distribution","text":"<p>A lightweight job broker for distributing work across multiple sandrun nodes. No blockchain, no complexity - just a simple HTTP coordinator.</p>"},{"location":"integrations/broker/#architecture","title":"Architecture","text":"<pre><code>Client \u2192 Broker Server \u2192 Sandrun Nodes\n         (job queue)     (execute jobs)\n</code></pre>"},{"location":"integrations/broker/#quick-start","title":"Quick Start","text":""},{"location":"integrations/broker/#1-start-broker-server","title":"1. Start Broker Server","text":"<pre><code>cd server\npip install -r requirements.txt\npython broker.py --port 8000\n</code></pre>"},{"location":"integrations/broker/#2-start-sandrun-node-with-broker-client","title":"2. Start Sandrun Node with Broker Client","text":"<pre><code># Terminal 1: Start sandrun\nsudo sandrun --port 8443\n\n# Terminal 2: Connect to broker\ncd node_client\npython node.py --broker http://localhost:8000 --sandrun http://localhost:8443\n</code></pre>"},{"location":"integrations/broker/#3-submit-jobs","title":"3. Submit Jobs","text":"<pre><code>import requests\n\n# Submit job\nresponse = requests.post('http://localhost:8000/submit', json={\n    'code': 'print(\"Hello distributed world!\")',\n    'interpreter': 'python3',\n    'timeout': 60\n})\n\njob_id = response.json()['job_id']\n\n# Check status\nstatus = requests.get(f'http://localhost:8000/status/{job_id}')\nprint(status.json())\n\n# Get results\nresults = requests.get(f'http://localhost:8000/results/{job_id}')\nprint(results.json()['output'])\n</code></pre>"},{"location":"integrations/broker/#components","title":"Components","text":""},{"location":"integrations/broker/#broker-server-serverbrokerpy","title":"Broker Server (<code>server/broker.py</code>)","text":"<ul> <li>Maintains job queue in SQLite</li> <li>Tracks registered nodes</li> <li>Assigns jobs to available nodes</li> <li>Stores results temporarily</li> </ul>"},{"location":"integrations/broker/#node-client-node_clientnodepy","title":"Node Client (<code>node_client/node.py</code>)","text":"<ul> <li>Registers with broker</li> <li>Polls for available jobs</li> <li>Executes via local sandrun</li> <li>Returns results to broker</li> </ul>"},{"location":"integrations/broker/#api-endpoints","title":"API Endpoints","text":""},{"location":"integrations/broker/#broker-server","title":"Broker Server","text":"Endpoint Method Description <code>/submit</code> POST Submit new job <code>/status/{job_id}</code> GET Check job status <code>/results/{job_id}</code> GET Get job results <code>/nodes</code> GET List registered nodes <code>/register</code> POST Register node (internal) <code>/heartbeat</code> POST Node keepalive (internal) <code>/claim</code> POST Claim job (internal)"},{"location":"integrations/broker/#database-schema","title":"Database Schema","text":"<pre><code>-- Jobs table\nCREATE TABLE jobs (\n    id TEXT PRIMARY KEY,\n    code TEXT,\n    interpreter TEXT,\n    status TEXT,  -- 'pending', 'assigned', 'running', 'completed', 'failed'\n    node_id TEXT,\n    output TEXT,\n    error TEXT,\n    created_at TIMESTAMP,\n    completed_at TIMESTAMP\n);\n\n-- Nodes table\nCREATE TABLE nodes (\n    id TEXT PRIMARY KEY,\n    endpoint TEXT,\n    capabilities TEXT,  -- JSON\n    last_heartbeat TIMESTAMP,\n    jobs_completed INTEGER\n);\n</code></pre>"},{"location":"integrations/broker/#features","title":"Features","text":"<ul> <li>\u2705 Simple HTTP-based coordination</li> <li>\u2705 SQLite persistence (no external DB needed)</li> <li>\u2705 Automatic node failover</li> <li>\u2705 Basic load balancing</li> <li>\u2705 Result caching</li> <li>\u2705 No blockchain complexity</li> <li>\u2705 No cryptocurrency required</li> <li>\u2705 Works with existing sandrun</li> </ul>"},{"location":"integrations/broker/#configuration","title":"Configuration","text":""},{"location":"integrations/broker/#broker-server_1","title":"Broker Server","text":"<pre><code># server/config.py\nJOB_TIMEOUT = 300  # seconds\nRESULT_TTL = 3600  # seconds\nNODE_TIMEOUT = 60  # seconds before marking node dead\nMAX_RETRIES = 3\n</code></pre>"},{"location":"integrations/broker/#node-client","title":"Node Client","text":"<pre><code>// node_client/config.json\n{\n  \"broker_url\": \"http://localhost:8000\",\n  \"sandrun_url\": \"http://localhost:8443\",\n  \"poll_interval\": 5,\n  \"capabilities\": {\n    \"cpu_cores\": 4,\n    \"memory_gb\": 8,\n    \"gpu\": false,\n    \"interpreters\": [\"python3\", \"node\", \"bash\"]\n  }\n}\n</code></pre>"},{"location":"integrations/broker/#security-notes","title":"Security Notes","text":"<ul> <li>Broker should run behind HTTPS in production</li> <li>Use API keys for authentication (not implemented in demo)</li> <li>Sandrun provides the actual security isolation</li> <li>Broker is just a coordinator, not a security boundary</li> </ul>"},{"location":"integrations/broker/#future-enhancements-if-needed","title":"Future Enhancements (if needed)","text":"<ul> <li>[ ] Payment integration (Stripe/PayPal)</li> <li>[ ] Priority queues</li> <li>[ ] Geographic node selection</li> <li>[ ] Job result verification (multiple nodes)</li> <li>[ ] Web dashboard</li> <li>[ ] Metrics and monitoring</li> </ul>"},{"location":"integrations/mcp-server/","title":"Sandrun MCP Server","text":"<p>Give Claude (and other LLMs) the power to execute code safely!</p> <p>This MCP (Model Context Protocol) server allows AI assistants like Claude to execute Python, JavaScript, and Bash code in Sandrun's secure sandbox environment.</p>"},{"location":"integrations/mcp-server/#why-this-is-awesome","title":"Why This Is Awesome","text":""},{"location":"integrations/mcp-server/#for-users","title":"For Users:","text":"<ul> <li>Safe AI Code Execution: Claude can run code without risking your system</li> <li>Instant Results: LLM writes code \u2192 Sandrun executes \u2192 Results back to LLM</li> <li>No Docker Required: Lightweight Linux namespaces instead of containers</li> <li>Fair Resource Sharing: Built-in rate limiting and queuing</li> </ul>"},{"location":"integrations/mcp-server/#for-developers","title":"For Developers:","text":"<ul> <li>Drop-in Solution: Just add to Claude Desktop config</li> <li>Multiple Languages: Python, JavaScript/Node, Bash out of the box</li> <li>Async Support: Non-blocking execution with status polling</li> <li>Battle-Tested: Uses Sandrun's proven isolation</li> </ul>"},{"location":"integrations/mcp-server/#quick-start","title":"Quick Start","text":""},{"location":"integrations/mcp-server/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>cd integrations/mcp-server\npip install -e .\n</code></pre>"},{"location":"integrations/mcp-server/#2-start-sandrun","title":"2. Start Sandrun","text":"<pre><code># In terminal 1\nsudo ./build/sandrun --port 8443\n</code></pre>"},{"location":"integrations/mcp-server/#3-configure-claude-desktop","title":"3. Configure Claude Desktop","text":"<p>Add to <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> (Mac) or <code>%APPDATA%\\Claude\\claude_desktop_config.json</code> (Windows):</p> <pre><code>{\n  \"mcpServers\": {\n    \"sandrun\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/sandrun/integrations/mcp-server/sandrun_mcp.py\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"integrations/mcp-server/#4-restart-claude-desktop","title":"4. Restart Claude Desktop","text":"<p>The MCP server will start automatically when Claude launches.</p>"},{"location":"integrations/mcp-server/#usage-examples","title":"Usage Examples","text":"<p>Once configured, you can ask Claude:</p>"},{"location":"integrations/mcp-server/#example-1-data-analysis","title":"Example 1: Data Analysis","text":"<pre><code>\"Can you analyze this list of numbers and tell me the mean, median, and mode?\"\n</code></pre> <p>Claude will use <code>execute_python</code> to run NumPy/statistics code and return results.</p>"},{"location":"integrations/mcp-server/#example-2-file-processing","title":"Example 2: File Processing","text":"<pre><code>\"I need to process this CSV data and extract unique values from column 3\"\n</code></pre> <p>Claude writes Python/pandas code, executes it, and shows you the results.</p>"},{"location":"integrations/mcp-server/#example-3-algorithm-implementation","title":"Example 3: Algorithm Implementation","text":"<pre><code>\"Implement the Sieve of Eratosthenes for finding primes up to 1000\"\n</code></pre> <p>Claude writes the code, runs it via Sandrun, and shows both code and output.</p>"},{"location":"integrations/mcp-server/#example-4-text-processing","title":"Example 4: Text Processing","text":"<pre><code>\"Use bash tools to count word frequencies in this text\"\n</code></pre> <p>Claude uses <code>execute_bash</code> with grep/awk/sort to process text.</p>"},{"location":"integrations/mcp-server/#available-tools","title":"Available Tools","text":"<p>The MCP server exposes these tools to the LLM:</p>"},{"location":"integrations/mcp-server/#execute_python","title":"<code>execute_python</code>","text":"<p>Execute Python code in a secure sandbox. - Input: Python code string - Output: stdout, stderr, exit code, metrics - Limits: 512MB RAM, 5min timeout, no network</p>"},{"location":"integrations/mcp-server/#execute_javascript","title":"<code>execute_javascript</code>","text":"<p>Execute JavaScript/Node.js code. - Same security and limits as Python</p>"},{"location":"integrations/mcp-server/#execute_bash","title":"<code>execute_bash</code>","text":"<p>Execute Bash commands/scripts. - Standard Unix tools available - Same isolation and limits</p>"},{"location":"integrations/mcp-server/#check_job_status","title":"<code>check_job_status</code>","text":"<p>Check status of a submitted job. - Returns: queued | running | completed | failed</p>"},{"location":"integrations/mcp-server/#get_job_logs","title":"<code>get_job_logs</code>","text":"<p>Get execution output for a job. - Returns: stdout and stderr</p>"},{"location":"integrations/mcp-server/#security-features","title":"Security Features","text":"<p>All code executes with: - \u2705 Namespace Isolation: Separate PID, network, mount namespaces - \u2705 No Network Access: Completely airgapped execution - \u2705 Resource Limits: 512MB RAM, 5 minute timeout - \u2705 Seccomp Filtering: Only ~60 safe syscalls allowed - \u2705 Auto-Cleanup: All data destroyed after execution - \u2705 Rate Limiting: 10 CPU-seconds per minute per IP</p>"},{"location":"integrations/mcp-server/#configuration","title":"Configuration","text":""},{"location":"integrations/mcp-server/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>SANDRUN_URL</code>: Sandrun server URL (default: http://localhost:8443)</li> <li><code>SANDRUN_TIMEOUT</code>: Max wait time for jobs (default: 30 seconds)</li> </ul>"},{"location":"integrations/mcp-server/#custom-configuration","title":"Custom Configuration","text":"<p>Edit <code>sandrun_mcp.py</code> to customize: - Timeout values - Server URL - Tool descriptions - Response formatting</p>"},{"location":"integrations/mcp-server/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Claude       \u2502  \"Calculate fibonacci(100)\"\n\u2502 Desktop      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 MCP Protocol (stdio)\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sandrun_mcp  \u2502  Translates to HTTP API calls\n\u2502 .py          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 HTTP REST\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sandrun      \u2502  Executes in isolated sandbox\n\u2502 (C++ server) \u2502  Returns stdout/stderr\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/mcp-server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/mcp-server/#connection-refused","title":"\"Connection refused\"","text":"<ul> <li>Make sure Sandrun is running: <code>sudo ./build/sandrun --port 8443</code></li> <li>Check firewall settings</li> </ul>"},{"location":"integrations/mcp-server/#job-timeout","title":"\"Job timeout\"","text":"<ul> <li>Increase <code>SANDRUN_TIMEOUT</code> for longer-running code</li> <li>Check if Sandrun is under heavy load</li> </ul>"},{"location":"integrations/mcp-server/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<ul> <li>Too many requests from same IP</li> <li>Wait 1 minute or increase limits in Sandrun config</li> </ul>"},{"location":"integrations/mcp-server/#claude-doesnt-see-the-tools","title":"Claude doesn't see the tools","text":"<ul> <li>Check Claude Desktop config file syntax</li> <li>Restart Claude Desktop</li> <li>Check MCP server logs</li> </ul>"},{"location":"integrations/mcp-server/#development","title":"Development","text":""},{"location":"integrations/mcp-server/#running-standalone","title":"Running Standalone","text":"<pre><code># Test the MCP server directly\npython3 sandrun_mcp.py\n</code></pre> <p>Then interact via MCP protocol over stdin/stdout.</p>"},{"location":"integrations/mcp-server/#testing-tools","title":"Testing Tools","text":"<pre><code>import asyncio\nfrom sandrun_mcp import SandrunMCPServer\n\nasync def test():\n    server = SandrunMCPServer()\n    result = await server.execute_python(\"print('Hello!')\")\n    print(result)\n\nasyncio.run(test())\n</code></pre>"},{"location":"integrations/mcp-server/#use-cases","title":"Use Cases","text":""},{"location":"integrations/mcp-server/#1-code-learning","title":"1. Code Learning","text":"<p>Ask Claude to demonstrate algorithms, data structures, or language features with live execution.</p>"},{"location":"integrations/mcp-server/#2-data-analysis","title":"2. Data Analysis","text":"<p>Give Claude a dataset, ask for analysis, get immediate pandas/numpy results.</p>"},{"location":"integrations/mcp-server/#3-prototyping","title":"3. Prototyping","text":"<p>Quickly test ideas: \"Does this regex work?\" \"What's the output of this function?\"</p>"},{"location":"integrations/mcp-server/#4-teaching","title":"4. Teaching","text":"<p>Students can ask Claude questions and see working code examples executed.</p>"},{"location":"integrations/mcp-server/#5-debugging","title":"5. Debugging","text":"<p>\"Why isn't this code working?\" Claude can modify and test hypotheses.</p>"},{"location":"integrations/mcp-server/#comparison-to-alternatives","title":"Comparison to Alternatives","text":"Feature Sandrun MCP E2B Docker Cloud Functions Setup Time 1 min Account signup 5-10 min Account signup Cold Start &lt;100ms ~1s ~1s ~500ms Memory Overhead ~10MB ~50MB ~100MB Varies Network Isolation \u2705 \u274c Optional \u274c Cost Free Paid tiers Free Paid Privacy Complete Data sent to cloud Local Data sent to cloud"},{"location":"integrations/mcp-server/#contributing","title":"Contributing","text":"<p>Ideas for improvements: - Add more language support (Ruby, Go, Rust) - Streaming output support - File upload/download for multi-file projects - Custom resource limits per tool - Output visualization (plots, charts)</p>"},{"location":"integrations/mcp-server/#license","title":"License","text":"<p>Same as Sandrun main project (MIT).</p> <p>Now LLMs can safely execute code! \ud83c\udf89</p>"},{"location":"integrations/trusted-pool/","title":"Trusted Pool Coordinator","text":"<p>A simple pool coordinator that routes jobs to allowlisted workers. Workers are trusted based on their Ed25519 public keys.</p>"},{"location":"integrations/trusted-pool/#architecture","title":"Architecture","text":"<pre><code>Client \u2192 Pool Coordinator \u2192 Trusted Workers\n                \u2193\n         Allowlist (public keys)\n         Health checking\n         Load balancing\n</code></pre>"},{"location":"integrations/trusted-pool/#trust-model","title":"Trust Model","text":"<ul> <li>Workers are allowlisted by their Ed25519 public keys</li> <li>No result verification needed (trusted execution)</li> <li>Health checking ensures worker availability</li> <li>Load balancing distributes jobs across available workers</li> </ul> <p>This is simpler than the trustless pool because: - No consensus needed - No verification of results - No economic incentives (stake/slash) - Workers are pre-approved and trusted</p>"},{"location":"integrations/trusted-pool/#setup","title":"Setup","text":""},{"location":"integrations/trusted-pool/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>cd integrations/trusted-pool\npip install -r requirements.txt\n</code></pre>"},{"location":"integrations/trusted-pool/#2-configure-workers","title":"2. Configure Workers","text":"<p>Create <code>workers.json</code> with your trusted workers:</p> <pre><code>[\n  {\n    \"worker_id\": \"base64-encoded-ed25519-public-key\",\n    \"endpoint\": \"http://worker1.example.com:8443\",\n    \"max_concurrent_jobs\": 4\n  },\n  {\n    \"worker_id\": \"another-public-key-base64\",\n    \"endpoint\": \"http://worker2.example.com:8443\",\n    \"max_concurrent_jobs\": 4\n  }\n]\n</code></pre> <p>To get a worker's public key (worker_id):</p> <pre><code># On worker machine:\n./sandrun --generate-key /etc/sandrun/worker.pem\n\n# Output shows:\n# \u2705 Saved worker key to: /etc/sandrun/worker.pem\n#    Worker ID: &lt;base64-encoded-public-key&gt;\n</code></pre> <p>Add the worker ID to your <code>workers.json</code> allowlist.</p>"},{"location":"integrations/trusted-pool/#3-start-workers","title":"3. Start Workers","text":"<p>On each worker machine:</p> <pre><code>sudo ./sandrun --port 8443 --worker-key /etc/sandrun/worker.pem\n</code></pre>"},{"location":"integrations/trusted-pool/#4-start-pool-coordinator","title":"4. Start Pool Coordinator","text":"<pre><code>python coordinator.py --port 9000 --workers workers.json\n</code></pre>"},{"location":"integrations/trusted-pool/#usage","title":"Usage","text":""},{"location":"integrations/trusted-pool/#submit-job-to-pool","title":"Submit Job to Pool","text":"<p>Instead of submitting directly to a worker, submit to the pool coordinator:</p> <pre><code>curl -X POST http://pool.example.com:9000/submit \\\n  -F \"files=@project.tar.gz\" \\\n  -F 'manifest={\"entrypoint\":\"main.py\",\"interpreter\":\"python3\"}'\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"pool-a1b2c3d4e5f6\",\n  \"status\": \"queued\"\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#check-job-status","title":"Check Job Status","text":"<pre><code>curl http://pool.example.com:9000/status/pool-a1b2c3d4e5f6\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"pool-a1b2c3d4e5f6\",\n  \"pool_status\": \"running\",\n  \"worker_id\": \"base64-worker-public-key\",\n  \"worker_status\": {\n    \"job_id\": \"remote-job-id-on-worker\",\n    \"status\": \"running\",\n    \"execution_metadata\": {\n      \"cpu_seconds\": 1.23,\n      \"memory_peak_bytes\": 52428800\n    }\n  },\n  \"submitted_at\": 1234567890.123,\n  \"completed_at\": null\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#download-output","title":"Download Output","text":"<pre><code>curl http://pool.example.com:9000/outputs/pool-a1b2c3d4e5f6/results/output.png \\\n  -o output.png\n</code></pre>"},{"location":"integrations/trusted-pool/#check-pool-status","title":"Check Pool Status","text":"<pre><code>curl http://pool.example.com:9000/pool\n</code></pre> <p>Response: <pre><code>{\n  \"total_workers\": 3,\n  \"healthy_workers\": 2,\n  \"total_jobs\": 15,\n  \"queued_jobs\": 2,\n  \"workers\": [\n    {\n      \"worker_id\": \"worker-1-public-key\",\n      \"endpoint\": \"http://worker1.example.com:8443\",\n      \"is_healthy\": true,\n      \"active_jobs\": 3,\n      \"max_concurrent_jobs\": 4,\n      \"last_health_check\": 1234567890.123\n    }\n  ]\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#api-endpoints","title":"API Endpoints","text":""},{"location":"integrations/trusted-pool/#post-submit","title":"POST /submit","text":"<p>Submit a job to the pool.</p> <p>Request: - <code>files</code>: Tarball of project files (multipart/form-data) - <code>manifest</code>: Job manifest JSON</p> <p>Response: <pre><code>{\n  \"job_id\": \"pool-xxx\",\n  \"status\": \"queued\"\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#get-statusjob_id","title":"GET /status/{job_id}","text":"<p>Get job status.</p> <p>Response: <pre><code>{\n  \"job_id\": \"pool-xxx\",\n  \"pool_status\": \"running\",\n  \"worker_id\": \"worker-public-key\",\n  \"worker_status\": { ... },\n  \"submitted_at\": 1234567890.123,\n  \"completed_at\": null\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#get-outputsjob_idpath","title":"GET /outputs/{job_id}/{path}","text":"<p>Download output file.</p> <p>Response: Binary file content</p>"},{"location":"integrations/trusted-pool/#get-pool","title":"GET /pool","text":"<p>Get pool status.</p> <p>Response: <pre><code>{\n  \"total_workers\": 3,\n  \"healthy_workers\": 2,\n  \"total_jobs\": 10,\n  \"queued_jobs\": 1,\n  \"workers\": [ ... ]\n}\n</code></pre></p>"},{"location":"integrations/trusted-pool/#how-it-works","title":"How It Works","text":""},{"location":"integrations/trusted-pool/#job-flow","title":"Job Flow","text":"<ol> <li>Client submits job to pool coordinator</li> <li>Job enters queue with \"queued\" status</li> <li>Coordinator finds available worker (healthy, not overloaded)</li> <li>Job dispatched to worker via HTTP POST to worker's /submit endpoint</li> <li>Worker executes job in sandbox</li> <li>Client polls status via pool coordinator (proxied to worker)</li> <li>Client downloads outputs via pool coordinator (proxied from worker)</li> </ol>"},{"location":"integrations/trusted-pool/#health-checking","title":"Health Checking","text":"<ul> <li>Pool coordinator checks each worker every 30 seconds</li> <li>Health check: <code>GET http://worker:8443/health</code></li> <li>Expected response: <code>{\"status\":\"healthy\",\"worker_id\":\"...\"}</code></li> <li>Unhealthy workers are excluded from routing</li> </ul>"},{"location":"integrations/trusted-pool/#load-balancing","title":"Load Balancing","text":"<ul> <li>Jobs routed to worker with fewest active jobs</li> <li>Workers have <code>max_concurrent_jobs</code> limit (default: 4)</li> <li>If no workers available, job waits in queue</li> </ul>"},{"location":"integrations/trusted-pool/#failure-handling","title":"Failure Handling","text":"<ul> <li>If worker rejects job \u2192 job re-queued</li> <li>If worker fails health check \u2192 marked unhealthy, excluded from routing</li> <li>Jobs in progress on failed workers remain assigned (client can retry)</li> </ul>"},{"location":"integrations/trusted-pool/#differences-from-trustless-pool","title":"Differences from Trustless Pool","text":"Feature Trusted Pool Trustless Pool Worker authorization Allowlist (public keys) Open (anyone can join) Result verification None (trust workers) Hash comparison + consensus Economic model None Stake + slashing Complexity Simple (~200 lines) Complex (~1000+ lines) Use case Private cluster, known workers Public compute, anonymous workers"},{"location":"integrations/trusted-pool/#security-considerations","title":"Security Considerations","text":""},{"location":"integrations/trusted-pool/#worker-authentication","title":"Worker Authentication","text":"<p>Workers must be started with <code>--worker-key</code> to have an identity. The pool coordinator verifies worker identity during health checks:</p> <pre><code>if data.get(\"worker_id\") == worker.worker_id:\n    worker.is_healthy = True\n</code></pre> <p>This prevents: - Impersonation: Rogue server can't pretend to be allowlisted worker - Unauthorized workers: Only allowlisted workers receive jobs</p>"},{"location":"integrations/trusted-pool/#network-security","title":"Network Security","text":"<p>Since this is a trusted pool, you should:</p> <ol> <li>Use private network or VPN for worker communication</li> <li>Enable TLS on workers (add HTTPS support)</li> <li>Firewall workers to only accept from coordinator IP</li> <li>Restrict pool coordinator to authorized clients</li> </ol>"},{"location":"integrations/trusted-pool/#resource-limits","title":"Resource Limits","text":"<p>Workers enforce their own resource limits (as configured in sandrun). The pool coordinator adds: - max_concurrent_jobs: Prevent worker overload - Job queueing: Prevent coordinator overload - Health checks: Detect and exclude failed workers</p>"},{"location":"integrations/trusted-pool/#monitoring","title":"Monitoring","text":""},{"location":"integrations/trusted-pool/#logs","title":"Logs","text":"<p>The coordinator logs: - Job submissions and dispatching - Worker health status changes - Errors and warnings</p> <p>Example: <pre><code>INFO:__main__:Added trusted worker: a1b2c3d4e5f6... at http://worker1:8443\nINFO:__main__:Queued job pool-abc123\nINFO:__main__:Dispatched job pool-abc123 to a1b2c3d4e5f6... (remote: job-xyz789)\nWARNING:__main__:Health check failed for b2c3d4e5f6g7...: Connection refused\n</code></pre></p>"},{"location":"integrations/trusted-pool/#metrics","title":"Metrics","text":"<p>Check <code>/pool</code> endpoint for real-time metrics: - Total workers and healthy count - Total jobs and queue depth - Per-worker active job count</p>"},{"location":"integrations/trusted-pool/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements for production use:</p> <ol> <li>Persistent storage for job history (currently in-memory)</li> <li>Worker capacity discovery (auto-detect max_concurrent_jobs)</li> <li>Job priority queues (high/low priority jobs)</li> <li>Authentication for clients (API keys, OAuth)</li> <li>TLS support for encrypted communication</li> <li>Metrics export (Prometheus, Grafana)</li> <li>Job cancellation (cancel in-progress jobs)</li> <li>Worker drain mode (stop accepting new jobs for maintenance)</li> </ol>"},{"location":"integrations/trusted-pool/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/trusted-pool/#no-workers-available","title":"No workers available","text":"<pre><code>WARNING:__main__:No available workers for job pool-xxx\n</code></pre> <p>Causes: - All workers unhealthy (check worker logs) - All workers at max capacity (check <code>/pool</code> endpoint) - Workers not started with <code>--worker-key</code></p> <p>Solution: - Start more workers - Increase <code>max_concurrent_jobs</code> per worker - Check worker health endpoints directly</p>"},{"location":"integrations/trusted-pool/#jobs-stuck-in-queued-status","title":"Jobs stuck in \"queued\" status","text":"<p>Causes: - No healthy workers available - Worker endpoints incorrect in workers.json</p> <p>Solution: - Check <code>/pool</code> endpoint for worker health status - Verify worker endpoints are reachable - Check worker logs for errors</p>"},{"location":"integrations/trusted-pool/#worker-rejected-job","title":"Worker rejected job","text":"<pre><code>ERROR:__main__:Worker a1b2c3d4... rejected job: 400\n</code></pre> <p>Causes: - Invalid manifest format - Files too large for worker - Worker resource limits exceeded</p> <p>Solution: - Check worker logs for specific error - Verify manifest is valid JSON - Reduce job size or increase worker limits</p>"},{"location":"integrations/trusted-pool/#example-deployment","title":"Example Deployment","text":""},{"location":"integrations/trusted-pool/#3-worker-pool","title":"3-Worker Pool","text":"<pre><code># Worker 1\nsudo ./sandrun --port 8443 --worker-key /etc/sandrun/worker1.pem\n\n# Worker 2\nsudo ./sandrun --port 8443 --worker-key /etc/sandrun/worker2.pem\n\n# Worker 3\nsudo ./sandrun --port 8443 --worker-key /etc/sandrun/worker3.pem\n\n# Coordinator\npython coordinator.py --port 9000 --workers workers.json\n</code></pre> <p>workers.json: <pre><code>[\n  {\n    \"worker_id\": \"worker1-public-key-from-generate-key\",\n    \"endpoint\": \"http://192.168.1.101:8443\",\n    \"max_concurrent_jobs\": 4\n  },\n  {\n    \"worker_id\": \"worker2-public-key-from-generate-key\",\n    \"endpoint\": \"http://192.168.1.102:8443\",\n    \"max_concurrent_jobs\": 4\n  },\n  {\n    \"worker_id\": \"worker3-public-key-from-generate-key\",\n    \"endpoint\": \"http://192.168.1.103:8443\",\n    \"max_concurrent_jobs\": 4\n  }\n]\n</code></pre></p> <p>Now you can submit jobs to the pool at <code>http://coordinator-ip:9000/submit</code> and they will be automatically distributed across the 3 workers!</p>"},{"location":"tutorials/","title":"Sandrun Tutorials","text":"<p>Hands-on, practical tutorials that guide you through building real-world applications with Sandrun.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/#distributed-data-pipeline","title":"Distributed Data Pipeline","text":"<p>Level: Beginner to Advanced | Time: 45-60 minutes</p> <p>Learn to build a complete data processing pipeline from scratch. This comprehensive tutorial progressively teaches:</p> <ul> <li>Part 1: Simple data analysis with Python</li> <li>Part 2: Multi-file projects with dependencies</li> <li>Part 3: Using pre-built environments for performance</li> <li>Part 4: Processing multiple datasets</li> <li>Part 5: Distributed execution with trusted pools</li> <li>Part 6: Real-time monitoring with WebSocket streaming</li> <li>Part 7: Cryptographic result verification</li> <li>Part 8: Complete production pipeline</li> </ul> <p>What You'll Build: A distributed system that analyzes sales data across multiple workers with real-time monitoring and verified results.</p> <p>Prerequisites: - Sandrun server running locally - Python 3.8+ - Basic Python and pandas knowledge - curl and jq installed</p>"},{"location":"tutorials/#tutorial-philosophy","title":"Tutorial Philosophy","text":"<p>Our tutorials follow these principles:</p> <ol> <li>Hands-on and Practical - You build real projects, not toy examples</li> <li>Progressive Complexity - Start simple, gradually add advanced features</li> <li>Production-Ready - Learn patterns used in real deployments</li> <li>Complete Examples - All code provided, copy-paste friendly</li> <li>Checkpoint System - Regular validation of your progress</li> </ol>"},{"location":"tutorials/#coming-soon","title":"Coming Soon","text":"<p>We're planning tutorials on:</p> <ul> <li>Machine Learning with Sandrun - Train and deploy models safely</li> <li>Privacy-Preserving Analytics - Process sensitive data without persistence</li> <li>LLM Code Execution - Build AI assistants with MCP integration</li> <li>GPU Workloads - Deep learning and image processing pipelines</li> </ul>"},{"location":"tutorials/#contributing-tutorials","title":"Contributing Tutorials","text":"<p>Want to contribute a tutorial? We'd love to have it! Check out our contribution guidelines and open a pull request.</p> <p>Great tutorials: - Solve real problems users face - Include complete, working code - Explain the \"why\" not just the \"how\" - Build progressively from simple to advanced - Use MkDocs Material features (admonitions, tabs, etc.)</p>"},{"location":"tutorials/#feedback","title":"Feedback","text":"<p>Found an issue in a tutorial? Have suggestions for improvements? Please open an issue on GitHub.</p>"},{"location":"tutorials/distributed-data-pipeline/","title":"Building a Distributed Data Processing Pipeline","text":"<p>A hands-on tutorial that takes you from processing a single file to building a distributed pipeline with cryptographic verification.</p> <p>Tutorial Overview</p> <p>Level: Beginner to Advanced Time: 45-60 minutes What You'll Build: A distributed data processing system that analyzes sales data across multiple workers with real-time monitoring and verified results</p>"},{"location":"tutorials/distributed-data-pipeline/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this tutorial, you will:</p> <ol> <li>Execute single-file Python scripts in Sandrun</li> <li>Structure multi-file projects with dependencies</li> <li>Use pre-built environments for data science workflows</li> <li>Scale processing across multiple files</li> <li>Set up a trusted pool for distributed execution</li> <li>Monitor jobs with WebSocket streaming</li> <li>Verify results with cryptographic signatures</li> </ol>"},{"location":"tutorials/distributed-data-pipeline/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>[x] Sandrun server running locally (see Getting Started)</li> <li>[x] Python 3.8+ installed</li> <li>[x] Basic familiarity with Python and pandas</li> <li>[x] <code>curl</code> and <code>jq</code> installed for API interaction</li> <li>[x] (Optional) Multiple machines or VMs for distributed setup</li> </ul> <p>Quick verification: <pre><code># Check Sandrun is running\ncurl http://localhost:8443/health\n\n# Expected response:\n# {\"status\":\"healthy\"}\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#part-1-simple-data-analysis","title":"Part 1: Simple Data Analysis","text":"<p>Let's start by analyzing a single CSV file with basic Python.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-11-create-sample-data","title":"Step 1.1: Create Sample Data","text":"<pre><code># Create a sample sales dataset\ncat &gt; sales.csv &lt;&lt;'EOF'\ndate,product,quantity,revenue\n2025-01-15,Widget A,5,125.00\n2025-01-15,Widget B,3,90.00\n2025-01-16,Widget A,8,200.00\n2025-01-16,Widget C,2,150.00\n2025-01-17,Widget A,12,300.00\n2025-01-17,Widget B,7,210.00\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-12-write-analysis-script","title":"Step 1.2: Write Analysis Script","text":"<pre><code>cat &gt; analyze.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\nimport csv\nfrom collections import defaultdict\n\n# Read data\ndata = []\nwith open('sales.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    data = list(reader)\n\n# Calculate total revenue\ntotal_revenue = sum(float(row['revenue']) for row in data)\n\n# Revenue by product\nproduct_revenue = defaultdict(float)\nfor row in data:\n    product_revenue[row['product']] += float(row['revenue'])\n\n# Print results\nprint(\"=== Sales Analysis Report ===\")\nprint(f\"\\nTotal Revenue: ${total_revenue:.2f}\")\nprint(\"\\nRevenue by Product:\")\nfor product, revenue in sorted(product_revenue.items()):\n    print(f\"  {product}: ${revenue:.2f}\")\n\n# Save report\nwith open('report.txt', 'w') as f:\n    f.write(f\"Total Revenue: ${total_revenue:.2f}\\n\")\n    f.write(\"Revenue by Product:\\n\")\n    for product, revenue in sorted(product_revenue.items()):\n        f.write(f\"  {product}: ${revenue:.2f}\\n\")\n\nprint(\"\\n\u2713 Report saved to report.txt\")\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-13-create-job-manifest","title":"Step 1.3: Create Job Manifest","text":"<pre><code>cat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"analyze.py\",\n  \"interpreter\": \"python3\",\n  \"outputs\": [\"report.txt\"]\n}\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-14-submit-job","title":"Step 1.4: Submit Job","text":"<pre><code># Package files\ntar czf job.tar.gz analyze.py sales.csv\n\n# Submit to Sandrun\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F \"manifest=$(cat job.json)\")\n\n# Extract job ID\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n</code></pre> <p>Expected Output: <pre><code>Job submitted: job-a1b2c3d4e5f6\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-15-monitor-job","title":"Step 1.5: Monitor Job","text":"<pre><code># Check status\ncurl http://localhost:8443/status/$JOB_ID | jq\n\n# Get logs\ncurl http://localhost:8443/logs/$JOB_ID\n\n# Download report\ncurl http://localhost:8443/outputs/$JOB_ID/report.txt\n</code></pre> <p>Expected Logs: <pre><code>=== Sales Analysis Report ===\n\nTotal Revenue: $1075.00\n\nRevenue by Product:\n  Widget A: $625.00\n  Widget B: $300.00\n  Widget C: $150.00\n\n\u2713 Report saved to report.txt\n</code></pre></p> <p>Checkpoint 1</p> <p>You've successfully executed a simple Python script in Sandrun's isolated sandbox! The job ran without any local Python environment setup.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-2-multi-file-project-with-dependencies","title":"Part 2: Multi-File Project with Dependencies","text":"<p>Now let's structure our analysis as a proper project with modules and external dependencies.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-21-create-project-structure","title":"Step 2.1: Create Project Structure","text":"<pre><code>mkdir -p analytics-project/data\ncd analytics-project\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-22-write-modular-code","title":"Step 2.2: Write Modular Code","text":"<p>Create the main analysis module:</p> <pre><code>cat &gt; analyzer.py &lt;&lt;'EOF'\n\"\"\"Sales data analyzer with pandas\"\"\"\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend\nimport matplotlib.pyplot as plt\n\ndef load_data(filepath):\n    \"\"\"Load sales data from CSV\"\"\"\n    df = pd.read_csv(filepath)\n    df['date'] = pd.to_datetime(df['date'])\n    return df\n\ndef analyze_revenue(df):\n    \"\"\"Calculate revenue statistics\"\"\"\n    total = df['revenue'].sum()\n    by_product = df.groupby('product')['revenue'].sum()\n    by_date = df.groupby('date')['revenue'].sum()\n\n    return {\n        'total': total,\n        'by_product': by_product,\n        'by_date': by_date\n    }\n\ndef create_visualizations(df, output_dir='plots'):\n    \"\"\"Generate analysis plots\"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Revenue by product (bar chart)\n    product_revenue = df.groupby('product')['revenue'].sum()\n    plt.figure(figsize=(10, 6))\n    product_revenue.plot(kind='bar', color='steelblue')\n    plt.title('Revenue by Product')\n    plt.xlabel('Product')\n    plt.ylabel('Revenue ($)')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/revenue_by_product.png')\n    plt.close()\n\n    # Daily revenue trend (line chart)\n    daily_revenue = df.groupby('date')['revenue'].sum()\n    plt.figure(figsize=(10, 6))\n    daily_revenue.plot(kind='line', marker='o', color='green')\n    plt.title('Daily Revenue Trend')\n    plt.xlabel('Date')\n    plt.ylabel('Revenue ($)')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/daily_trend.png')\n    plt.close()\n\n    print(\"\u2713 Visualizations saved to plots/\")\n\ndef save_report(stats, filepath='report.txt'):\n    \"\"\"Save analysis report\"\"\"\n    with open(filepath, 'w') as f:\n        f.write(\"=== Sales Analysis Report ===\\n\\n\")\n        f.write(f\"Total Revenue: ${stats['total']:.2f}\\n\\n\")\n        f.write(\"Revenue by Product:\\n\")\n        for product, revenue in stats['by_product'].items():\n            f.write(f\"  {product}: ${revenue:.2f}\\n\")\n        f.write(\"\\nDaily Revenue:\\n\")\n        for date, revenue in stats['by_date'].items():\n            f.write(f\"  {date.date()}: ${revenue:.2f}\\n\")\n\n    print(f\"\u2713 Report saved to {filepath}\")\nEOF\n</code></pre> <p>Create the main script:</p> <pre><code>cat &gt; main.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"Main entry point for sales analysis\"\"\"\nimport sys\nimport argparse\nfrom analyzer import load_data, analyze_revenue, create_visualizations, save_report\n\ndef main():\n    parser = argparse.ArgumentParser(description='Analyze sales data')\n    parser.add_argument('--input', default='data/sales.csv', help='Input CSV file')\n    parser.add_argument('--output', default='report.txt', help='Output report file')\n    parser.add_argument('--plots', action='store_true', help='Generate plots')\n    args = parser.parse_args()\n\n    print(f\"Loading data from {args.input}...\")\n    df = load_data(args.input)\n    print(f\"\u2713 Loaded {len(df)} records\")\n\n    print(\"\\nAnalyzing revenue...\")\n    stats = analyze_revenue(df)\n    print(f\"\u2713 Total revenue: ${stats['total']:.2f}\")\n\n    if args.plots:\n        print(\"\\nGenerating visualizations...\")\n        create_visualizations(df)\n\n    print(f\"\\nSaving report to {args.output}...\")\n    save_report(stats, args.output)\n\n    print(\"\\n\ud83c\udf89 Analysis complete!\")\n\nif __name__ == '__main__':\n    main()\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-23-add-dependencies","title":"Step 2.3: Add Dependencies","text":"<pre><code>cat &gt; requirements.txt &lt;&lt;'EOF'\npandas==2.0.3\nmatplotlib==3.7.2\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-24-copy-data","title":"Step 2.4: Copy Data","text":"<pre><code>cp ../sales.csv data/\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-25-create-enhanced-manifest","title":"Step 2.5: Create Enhanced Manifest","text":"<pre><code>cat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"args\": [\"--input\", \"data/sales.csv\", \"--plots\"],\n  \"requirements\": \"requirements.txt\",\n  \"outputs\": [\"report.txt\", \"plots/*.png\"],\n  \"timeout\": 300,\n  \"memory_mb\": 512\n}\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-26-submit-project","title":"Step 2.6: Submit Project","text":"<pre><code># Package entire project\ntar czf ../analytics-project.tar.gz .\n\n# Submit\ncd ..\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@analytics-project.tar.gz\" \\\n  -F \"manifest=$(cat analytics-project/job.json)\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Wait for completion (this may take 30-60 seconds for pip install)\necho \"Installing dependencies...\"\nsleep 45\n\n# Check status\ncurl http://localhost:8443/status/$JOB_ID | jq '.status'\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-27-download-results","title":"Step 2.7: Download Results","text":"<pre><code># List available outputs\ncurl http://localhost:8443/outputs/$JOB_ID | jq\n\n# Download all outputs as tarball\ncurl http://localhost:8443/download/$JOB_ID -o results.tar.gz\ntar xzf results.tar.gz\n\n# View files\nls -R plots/\ncat report.txt\n</code></pre> <p>Dependency Installation</p> <p>Sandrun installs dependencies in the sandbox on first use. This adds startup time but ensures complete isolation. For faster execution, use pre-built environments (next section).</p> <p>Checkpoint 2</p> <p>You've built a modular project with external dependencies! Notice how Sandrun automatically installed pandas and matplotlib without any local setup.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-3-using-pre-built-environments","title":"Part 3: Using Pre-Built Environments","text":"<p>Installing dependencies on every job is slow. Let's use Sandrun's pre-built environments for faster execution.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-31-list-available-environments","title":"Step 3.1: List Available Environments","text":"<pre><code>curl http://localhost:8443/environments | jq\n</code></pre> <p>Response: <pre><code>{\n  \"environments\": [\n    {\n      \"name\": \"ml-basic\",\n      \"packages\": [\"numpy\", \"pandas\", \"scikit-learn\", \"matplotlib\"],\n      \"python_version\": \"3.10.12\"\n    },\n    {\n      \"name\": \"data-science\",\n      \"packages\": [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"jupyter\"],\n      \"python_version\": \"3.10.12\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-32-update-manifest","title":"Step 3.2: Update Manifest","text":"<pre><code>cd analytics-project\n\n# Use ml-basic environment (has pandas and matplotlib pre-installed)\ncat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"args\": [\"--input\", \"data/sales.csv\", \"--plots\"],\n  \"outputs\": [\"report.txt\", \"plots/*.png\"],\n  \"timeout\": 300,\n  \"memory_mb\": 512\n}\nEOF\n</code></pre> <p>Environment Benefits</p> <p>By specifying <code>\"environment\": \"ml-basic\"</code>, we skip pip installation entirely. The job starts instantly because pandas and matplotlib are already available.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-33-submit-with-environment","title":"Step 3.3: Submit with Environment","text":"<pre><code>tar czf ../analytics-env.tar.gz .\ncd ..\n\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@analytics-env.tar.gz\" \\\n  -F \"manifest=$(cat analytics-project/job.json)\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Check execution time (should be much faster)\nsleep 5\ncurl http://localhost:8443/status/$JOB_ID | jq '.execution_metadata'\n</code></pre> <p>Expected Response: <pre><code>{\n  \"cpu_seconds\": 0.8,\n  \"memory_peak_bytes\": 45678912,\n  \"exit_code\": 0,\n  \"runtime_seconds\": 2.3\n}\n</code></pre></p> <p>Checkpoint 3</p> <p>Job execution is now ~20x faster! From 45 seconds (with pip install) to 2-3 seconds (pre-built environment). This makes Sandrun practical for high-volume batch processing.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-4-processing-multiple-files","title":"Part 4: Processing Multiple Files","text":"<p>Real-world pipelines process multiple datasets. Let's extend our analysis to handle multiple files.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-41-generate-multiple-datasets","title":"Step 4.1: Generate Multiple Datasets","text":"<pre><code># Create datasets for different regions\nmkdir -p multi-region/data\n\ncat &gt; multi-region/data/sales_north.csv &lt;&lt;'EOF'\ndate,product,quantity,revenue\n2025-01-15,Widget A,10,250.00\n2025-01-16,Widget B,5,150.00\n2025-01-17,Widget A,15,375.00\nEOF\n\ncat &gt; multi-region/data/sales_south.csv &lt;&lt;'EOF'\ndate,product,quantity,revenue\n2025-01-15,Widget C,8,240.00\n2025-01-16,Widget A,6,150.00\n2025-01-17,Widget B,9,270.00\nEOF\n\ncat &gt; multi-region/data/sales_east.csv &lt;&lt;'EOF'\ndate,product,quantity,revenue\n2025-01-15,Widget A,12,300.00\n2025-01-16,Widget C,4,120.00\n2025-01-17,Widget A,8,200.00\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-42-create-batch-processor","title":"Step 4.2: Create Batch Processor","text":"<pre><code>cd multi-region\n\ncat &gt; batch_analyze.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"Batch analysis across multiple datasets\"\"\"\nimport os\nimport glob\nimport pandas as pd\nimport json\n\ndef process_region(filepath):\n    \"\"\"Process a single region's data\"\"\"\n    region = os.path.basename(filepath).replace('sales_', '').replace('.csv', '')\n    df = pd.read_csv(filepath)\n\n    total_revenue = df['revenue'].sum()\n    total_quantity = df['quantity'].sum()\n    unique_products = df['product'].nunique()\n\n    return {\n        'region': region,\n        'total_revenue': float(total_revenue),\n        'total_quantity': int(total_quantity),\n        'unique_products': unique_products,\n        'records': len(df)\n    }\n\ndef main():\n    print(\"Finding data files...\")\n    files = glob.glob('data/sales_*.csv')\n    print(f\"\u2713 Found {len(files)} region files\")\n\n    # Process each region\n    results = []\n    for filepath in sorted(files):\n        print(f\"\\nProcessing {os.path.basename(filepath)}...\")\n        result = process_region(filepath)\n        results.append(result)\n        print(f\"  Revenue: ${result['total_revenue']:.2f}\")\n        print(f\"  Products: {result['unique_products']}\")\n\n    # Calculate aggregates\n    total_revenue = sum(r['total_revenue'] for r in results)\n    total_quantity = sum(r['total_quantity'] for r in results)\n\n    # Create summary\n    summary = {\n        'regions': results,\n        'totals': {\n            'revenue': total_revenue,\n            'quantity': total_quantity,\n            'regions_processed': len(results)\n        }\n    }\n\n    # Save JSON report\n    with open('summary.json', 'w') as f:\n        json.dump(summary, f, indent=2)\n\n    # Save text report\n    with open('summary.txt', 'w') as f:\n        f.write(\"=== Multi-Region Sales Summary ===\\n\\n\")\n        f.write(f\"Regions Processed: {len(results)}\\n\")\n        f.write(f\"Total Revenue: ${total_revenue:.2f}\\n\")\n        f.write(f\"Total Quantity: {total_quantity}\\n\\n\")\n        f.write(\"By Region:\\n\")\n        for r in results:\n            f.write(f\"  {r['region'].upper()}: ${r['total_revenue']:.2f} ({r['total_quantity']} units)\\n\")\n\n    print(f\"\\n\u2713 Summary saved\")\n    print(f\"  Total revenue across all regions: ${total_revenue:.2f}\")\n    print(\"\\n\ud83c\udf89 Batch processing complete!\")\n\nif __name__ == '__main__':\n    main()\nEOF\n\ncat &gt; job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"batch_analyze.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"outputs\": [\"summary.json\", \"summary.txt\"],\n  \"timeout\": 300,\n  \"memory_mb\": 512\n}\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-43-submit-batch-job","title":"Step 4.3: Submit Batch Job","text":"<pre><code>tar czf ../multi-region.tar.gz .\ncd ..\n\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@multi-region.tar.gz\" \\\n  -F \"manifest=$(cat multi-region/job.json)\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Wait and download\nsleep 5\ncurl http://localhost:8443/outputs/$JOB_ID/summary.json | jq\n</code></pre> <p>Expected Output: <pre><code>{\n  \"regions\": [\n    {\n      \"region\": \"east\",\n      \"total_revenue\": 620.0,\n      \"total_quantity\": 24,\n      \"unique_products\": 2,\n      \"records\": 3\n    },\n    {\n      \"region\": \"north\",\n      \"total_revenue\": 775.0,\n      \"total_quantity\": 30,\n      \"unique_products\": 2,\n      \"records\": 3\n    },\n    {\n      \"region\": \"south\",\n      \"total_revenue\": 660.0,\n      \"total_quantity\": 23,\n      \"unique_products\": 3,\n      \"records\": 3\n    }\n  ],\n  \"totals\": {\n    \"revenue\": 2055.0,\n    \"quantity\": 77,\n    \"regions_processed\": 3\n  }\n}\n</code></pre></p> <p>Checkpoint 4</p> <p>You've processed multiple datasets in a single job! However, all processing still happens sequentially on one worker. Let's distribute the work next.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-5-distributed-processing-with-trusted-pool","title":"Part 5: Distributed Processing with Trusted Pool","text":"<p>Now let's scale horizontally by distributing jobs across multiple workers using Sandrun's trusted pool coordinator.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-51-generate-worker-identities","title":"Step 5.1: Generate Worker Identities","text":"<p>First, we need to create worker identities on each machine:</p> <pre><code># On Worker 1\nsudo ./build/sandrun --generate-key /etc/sandrun/worker1.pem\n\n# Output:\n# \u2705 Saved worker key to: /etc/sandrun/worker1.pem\n#    Worker ID: J7X8K3mNqR4tUvWxYz9A2bCdEfGhIjKlMnOpQrStUvWx==\n\n# On Worker 2\nsudo ./build/sandrun --generate-key /etc/sandrun/worker2.pem\n\n# On Worker 3\nsudo ./build/sandrun --generate-key /etc/sandrun/worker3.pem\n</code></pre> <p>Worker Identity</p> <p>Each worker gets an Ed25519 key pair. The public key (Worker ID) is used to identify and trust the worker. The private key signs all job results for verification.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-52-configure-pool","title":"Step 5.2: Configure Pool","text":"<pre><code>cd /path/to/sandrun/integrations/trusted-pool\n\n# Create worker configuration\ncat &gt; workers.json &lt;&lt;'EOF'\n[\n  {\n    \"worker_id\": \"J7X8K3mNqR4tUvWxYz9A2bCdEfGhIjKlMnOpQrStUvWx==\",\n    \"endpoint\": \"http://192.168.1.101:8443\",\n    \"max_concurrent_jobs\": 4\n  },\n  {\n    \"worker_id\": \"K8Y9L4nOqS5uVwXyZa0B3cDeEfGhIjKlMnOpQrStUvWx==\",\n    \"endpoint\": \"http://192.168.1.102:8443\",\n    \"max_concurrent_jobs\": 4\n  },\n  {\n    \"worker_id\": \"L9Z0M5oPrT6vWxYza1C4dEfGhIjKlMnOpQrStUvWxYz==\",\n    \"endpoint\": \"http://192.168.1.103:8443\",\n    \"max_concurrent_jobs\": 4\n  }\n]\nEOF\n</code></pre> <p>Local Testing</p> <p>Don't have multiple machines? Run workers on different ports locally: <pre><code># Terminal 1\nsudo ./build/sandrun --port 8443 --worker-key worker1.pem\n\n# Terminal 2\nsudo ./build/sandrun --port 8444 --worker-key worker2.pem\n\n# Terminal 3\nsudo ./build/sandrun --port 8445 --worker-key worker3.pem\n</code></pre></p> <p>Then use <code>http://localhost:8443</code>, <code>http://localhost:8444</code>, <code>http://localhost:8445</code> in workers.json.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-53-start-workers","title":"Step 5.3: Start Workers","text":"<pre><code># On each worker machine (or in separate terminals if local)\nsudo ./build/sandrun --port 8443 --worker-key /etc/sandrun/worker1.pem\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-54-start-pool-coordinator","title":"Step 5.4: Start Pool Coordinator","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Start coordinator\npython coordinator.py --port 9000 --workers workers.json\n</code></pre> <p>Expected Output: <pre><code>INFO:__main__:Added trusted worker: J7X8K3mNqR4t... at http://192.168.1.101:8443\nINFO:__main__:Added trusted worker: K8Y9L4nOqS5u... at http://192.168.1.102:8443\nINFO:__main__:Added trusted worker: L9Z0M5oPrT6v... at http://192.168.1.103:8443\nINFO:__main__:Starting health checker (interval: 30s)\nINFO:__main__:Pool coordinator listening on port 9000\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-55-verify-pool-status","title":"Step 5.5: Verify Pool Status","text":"<pre><code>curl http://localhost:9000/pool | jq\n</code></pre> <p>Response: <pre><code>{\n  \"total_workers\": 3,\n  \"healthy_workers\": 3,\n  \"total_jobs\": 0,\n  \"queued_jobs\": 0,\n  \"workers\": [\n    {\n      \"worker_id\": \"J7X8K3mNqR4t...\",\n      \"endpoint\": \"http://192.168.1.101:8443\",\n      \"is_healthy\": true,\n      \"active_jobs\": 0,\n      \"max_concurrent_jobs\": 4\n    }\n  ]\n}\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-56-submit-jobs-to-pool","title":"Step 5.6: Submit Jobs to Pool","text":"<p>Now submit multiple jobs - they'll be automatically distributed:</p> <pre><code># Submit 9 jobs (3 per worker)\nfor i in {1..9}; do\n  echo \"Submitting job $i...\"\n  RESPONSE=$(curl -s -X POST http://localhost:9000/submit \\\n    -F \"files=@analytics-project.tar.gz\" \\\n    -F \"manifest=$(cat analytics-project/job.json)\")\n\n  JOB_ID=$(echo $RESPONSE | jq -r '.job_id')\n  echo \"  Job $i: $JOB_ID\"\n\n  # Save job IDs\n  echo $JOB_ID &gt;&gt; pool_jobs.txt\ndone\n\necho \"\u2713 Submitted 9 jobs to pool\"\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-57-monitor-distribution","title":"Step 5.7: Monitor Distribution","text":"<pre><code># Check pool status\ncurl http://localhost:9000/pool | jq '.workers[] | {worker_id: .worker_id[0:16], active_jobs: .active_jobs}'\n</code></pre> <p>Response (jobs distributed across workers): <pre><code>{\"worker_id\": \"J7X8K3mNqR4t\", \"active_jobs\": 3}\n{\"worker_id\": \"K8Y9L4nOqS5u\", \"active_jobs\": 3}\n{\"worker_id\": \"L9Z0M5oPrT6v\", \"active_jobs\": 3}\n</code></pre></p> <p>Checkpoint 5</p> <p>Jobs are now distributed across 3 workers! Each worker processes 3 jobs concurrently, giving you 9x parallelism. The pool coordinator automatically load balances based on worker availability.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-6-real-time-monitoring-with-websocket","title":"Part 6: Real-Time Monitoring with WebSocket","text":"<p>For long-running jobs, you want to see progress in real-time. Let's use WebSocket streaming.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-61-create-long-running-job","title":"Step 6.1: Create Long-Running Job","text":"<pre><code>mkdir -p streaming-demo\n\ncat &gt; streaming-demo/process.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"Long-running job with progress updates\"\"\"\nimport time\nimport sys\n\ndef process_batch(batch_num, items):\n    \"\"\"Simulate processing a batch\"\"\"\n    print(f\"[Batch {batch_num}] Processing {items} items...\")\n    sys.stdout.flush()\n\n    for i in range(items):\n        time.sleep(0.5)  # Simulate work\n        if (i + 1) % 5 == 0:\n            print(f\"  Progress: {i+1}/{items} items processed\")\n            sys.stdout.flush()\n\n    print(f\"[Batch {batch_num}] \u2713 Complete ({items} items)\")\n    sys.stdout.flush()\n\ndef main():\n    print(\"=== Starting Data Processing Pipeline ===\\n\")\n    sys.stdout.flush()\n\n    batches = [\n        (1, 10),\n        (2, 15),\n        (3, 8),\n    ]\n\n    for batch_num, items in batches:\n        process_batch(batch_num, items)\n        print()\n        sys.stdout.flush()\n\n    print(\"\ud83c\udf89 All batches processed successfully!\")\n    sys.stdout.flush()\n\nif __name__ == '__main__':\n    main()\nEOF\n\ncat &gt; streaming-demo/job.json &lt;&lt;'EOF'\n{\n  \"entrypoint\": \"process.py\",\n  \"interpreter\": \"python3\",\n  \"timeout\": 300\n}\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-62-create-websocket-client","title":"Step 6.2: Create WebSocket Client","text":"<pre><code>cat &gt; stream_logs.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"WebSocket client to stream job logs in real-time\"\"\"\nimport sys\nimport asyncio\nimport websockets\n\nasync def stream_logs(job_id):\n    uri = f\"ws://localhost:8443/logs/{job_id}/stream\"\n\n    print(f\"Connecting to {uri}...\")\n    print(\"-\" * 60)\n\n    try:\n        async with websockets.connect(uri) as ws:\n            async for message in ws:\n                print(message, end='', flush=True)\n    except websockets.exceptions.ConnectionClosed:\n        print(\"\\n\" + \"-\" * 60)\n        print(\"\u2713 Stream closed (job completed)\")\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print(\"Usage: python stream_logs.py &lt;job_id&gt;\")\n        sys.exit(1)\n\n    job_id = sys.argv[1]\n    asyncio.run(stream_logs(job_id))\nEOF\n\nchmod +x stream_logs.py\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-63-submit-and-stream","title":"Step 6.3: Submit and Stream","text":"<pre><code># Submit job\ncd streaming-demo\ntar czf ../streaming.tar.gz .\ncd ..\n\nRESPONSE=$(curl -s -X POST http://localhost:8443/submit \\\n  -F \"files=@streaming.tar.gz\" \\\n  -F \"manifest=$(cat streaming-demo/job.json)\")\n\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\necho \"Job submitted: $JOB_ID\"\n\n# Stream logs in real-time\npython stream_logs.py $JOB_ID\n</code></pre> <p>Live Output: <pre><code>Connecting to ws://localhost:8443/logs/job-abc123/stream...\n------------------------------------------------------------\n=== Starting Data Processing Pipeline ===\n\n[Batch 1] Processing 10 items...\n  Progress: 5/10 items processed\n  Progress: 10/10 items processed\n[Batch 1] \u2713 Complete (10 items)\n\n[Batch 2] Processing 15 items...\n  Progress: 5/15 items processed\n  Progress: 10/15 items processed\n  Progress: 15/15 items processed\n[Batch 2] \u2713 Complete (15 items)\n\n[Batch 3] Processing 8 items...\n  Progress: 5/8 items processed\n  Progress: 8/8 items processed\n[Batch 3] \u2713 Complete (8 items)\n\n\ud83c\udf89 All batches processed successfully!\n------------------------------------------------------------\n\u2713 Stream closed (job completed)\n</code></pre></p> <p>Pool Streaming</p> <p>WebSocket streaming also works through the pool coordinator: <pre><code># Submit to pool\nRESPONSE=$(curl -s -X POST http://localhost:9000/submit ...)\n\n# Stream from pool (it proxies to the worker)\npython stream_logs.py $POOL_JOB_ID\n</code></pre></p> <p>Checkpoint 6</p> <p>You can now monitor long-running jobs in real-time! This is essential for debugging and progress tracking in production pipelines.</p>"},{"location":"tutorials/distributed-data-pipeline/#part-7-cryptographic-result-verification","title":"Part 7: Cryptographic Result Verification","text":"<p>When running untrusted workers or processing sensitive data, you need to verify results weren't tampered with.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-71-understand-worker-signatures","title":"Step 7.1: Understand Worker Signatures","text":"<p>Every job executed by a worker with <code>--worker-key</code> includes a cryptographic signature:</p> <pre><code># Check job status\ncurl http://localhost:8443/status/$JOB_ID | jq '.worker_metadata'\n</code></pre> <p>Response: <pre><code>{\n  \"worker_id\": \"J7X8K3mNqR4tUvWxYz9A2bCdEfGhIjKlMnOpQrStUvWx==\",\n  \"signature\": \"k8mN9qR2tVwX...base64-signature...\",\n  \"signature_algorithm\": \"Ed25519\",\n  \"signed_data\": {\n    \"job_id\": \"job-abc123\",\n    \"exit_code\": 0,\n    \"cpu_seconds\": 1.23,\n    \"memory_peak_bytes\": 52428800,\n    \"output_hash\": \"sha256:a3f2b8d1c7e5...\"\n  }\n}\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-72-extract-worker-public-key","title":"Step 7.2: Extract Worker Public Key","text":"<pre><code># Get worker public key from worker endpoint\ncurl http://localhost:8443/health | jq -r '.worker_id' &gt; worker_pubkey.txt\n\ncat worker_pubkey.txt\n# J7X8K3mNqR4tUvWxYz9A2bCdEfGhIjKlMnOpQrStUvWx==\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-73-create-verification-script","title":"Step 7.3: Create Verification Script","text":"<pre><code>cat &gt; verify_result.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"Verify job result signature\"\"\"\nimport sys\nimport json\nimport base64\nimport hashlib\nfrom cryptography.hazmat.primitives.asymmetric import ed25519\nfrom cryptography.hazmat.primitives import serialization\n\ndef load_worker_pubkey(pubkey_b64):\n    \"\"\"Load Ed25519 public key from base64\"\"\"\n    pubkey_bytes = base64.b64decode(pubkey_b64)\n    return ed25519.Ed25519PublicKey.from_public_bytes(pubkey_bytes)\n\ndef verify_signature(pubkey, signed_data, signature_b64):\n    \"\"\"Verify Ed25519 signature\"\"\"\n    signature = base64.b64decode(signature_b64)\n    message = json.dumps(signed_data, sort_keys=True).encode()\n\n    try:\n        pubkey.verify(signature, message)\n        return True\n    except Exception:\n        return False\n\ndef main(job_id, worker_pubkey_b64):\n    # Fetch job status\n    import subprocess\n    result = subprocess.run(\n        ['curl', '-s', f'http://localhost:8443/status/{job_id}'],\n        capture_output=True,\n        text=True\n    )\n\n    status = json.loads(result.stdout)\n\n    if 'worker_metadata' not in status:\n        print(\"\u274c No worker metadata (job not executed by signed worker)\")\n        sys.exit(1)\n\n    metadata = status['worker_metadata']\n\n    # Verify worker identity\n    if metadata['worker_id'] != worker_pubkey_b64:\n        print(\"\u274c Worker ID mismatch!\")\n        print(f\"  Expected: {worker_pubkey_b64}\")\n        print(f\"  Got: {metadata['worker_id']}\")\n        sys.exit(1)\n\n    print(f\"\u2713 Worker identity verified: {metadata['worker_id'][:16]}...\")\n\n    # Verify signature\n    pubkey = load_worker_pubkey(worker_pubkey_b64)\n    signed_data = metadata['signed_data']\n    signature = metadata['signature']\n\n    if verify_signature(pubkey, signed_data, signature):\n        print(\"\u2713 Signature valid (result authenticated)\")\n        print(f\"\\nSigned Data:\")\n        print(f\"  Job ID: {signed_data['job_id']}\")\n        print(f\"  Exit Code: {signed_data['exit_code']}\")\n        print(f\"  CPU Time: {signed_data['cpu_seconds']}s\")\n        print(f\"  Memory Peak: {signed_data['memory_peak_bytes'] / 1024 / 1024:.1f} MB\")\n        print(f\"  Output Hash: {signed_data['output_hash']}\")\n        print(\"\\n\ud83c\udf89 Result verified successfully!\")\n    else:\n        print(\"\u274c Invalid signature (result may be tampered)\")\n        sys.exit(1)\n\nif __name__ == '__main__':\n    if len(sys.argv) != 3:\n        print(\"Usage: python verify_result.py &lt;job_id&gt; &lt;worker_pubkey_base64&gt;\")\n        sys.exit(1)\n\n    main(sys.argv[1], sys.argv[2])\nEOF\n\n# Install cryptography library\npip install cryptography\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-74-verify-a-job-result","title":"Step 7.4: Verify a Job Result","text":"<pre><code># Get worker public key\nWORKER_PUBKEY=$(curl -s http://localhost:8443/health | jq -r '.worker_id')\n\n# Verify job result\npython verify_result.py $JOB_ID $WORKER_PUBKEY\n</code></pre> <p>Output: <pre><code>\u2713 Worker identity verified: J7X8K3mNqR4t...\n\u2713 Signature valid (result authenticated)\n\nSigned Data:\n  Job ID: job-abc123\n  Exit Code: 0\n  CPU Time: 1.23s\n  Memory Peak: 50.0 MB\n  Output Hash: sha256:a3f2b8d1c7e5...\n\n\ud83c\udf89 Result verified successfully!\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-75-verify-pool-job","title":"Step 7.5: Verify Pool Job","text":"<p>For pool jobs, the coordinator tracks which worker executed the job:</p> <pre><code># Get pool job status\ncurl http://localhost:9000/status/$POOL_JOB_ID | jq '{pool_job_id: .job_id, worker_id: .worker_id, remote_job_id: .worker_status.job_id}'\n\n# Get worker public key from pool\nWORKER_ID=$(curl -s http://localhost:9000/status/$POOL_JOB_ID | jq -r '.worker_id')\n\n# Fetch actual job status from worker\ncurl http://localhost:8443/status/$REMOTE_JOB_ID | jq '.worker_metadata'\n\n# Verify\npython verify_result.py $REMOTE_JOB_ID $WORKER_ID\n</code></pre> <p>Trust Model</p> <p>The trusted pool coordinator doesn't verify signatures itself - it trusts allowlisted workers. You should verify signatures if:</p> <ul> <li>Running a public pool open to anyone</li> <li>Processing sensitive data requiring audit trails</li> <li>Implementing compliance requirements</li> </ul> <p>For private clusters with trusted workers, signature verification is optional.</p> <p>Checkpoint 7</p> <p>You can now cryptographically verify that results came from specific workers and weren't tampered with. This enables trustless distributed computing!</p>"},{"location":"tutorials/distributed-data-pipeline/#part-8-complete-pipeline-example","title":"Part 8: Complete Pipeline Example","text":"<p>Let's tie everything together: distributed processing with monitoring and verification.</p>"},{"location":"tutorials/distributed-data-pipeline/#step-81-create-distributed-data-pipeline","title":"Step 8.1: Create Distributed Data Pipeline","text":"<pre><code>mkdir -p complete-pipeline\n\ncat &gt; complete-pipeline/pipeline.py &lt;&lt;'EOF'\n#!/usr/bin/env python3\n\"\"\"Complete data pipeline with progress tracking\"\"\"\nimport sys\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef log(message):\n    \"\"\"Log with timestamp and flush\"\"\"\n    timestamp = datetime.now().strftime('%H:%M:%S')\n    print(f\"[{timestamp}] {message}\", flush=True)\n\ndef process_dataset(filepath):\n    \"\"\"Process a single dataset\"\"\"\n    log(f\"Loading {filepath}...\")\n    df = pd.read_csv(filepath)\n\n    log(f\"  Records: {len(df)}\")\n\n    # Data validation\n    log(\"  Validating data...\")\n    assert not df.isnull().any().any(), \"Found null values\"\n    assert all(df['revenue'] &gt;= 0), \"Found negative revenue\"\n\n    # Calculate metrics\n    log(\"  Calculating metrics...\")\n    metrics = {\n        'total_revenue': float(df['revenue'].sum()),\n        'avg_revenue': float(df['revenue'].mean()),\n        'total_quantity': int(df['quantity'].sum()),\n        'unique_products': int(df['product'].nunique()),\n        'date_range': {\n            'start': df['date'].min(),\n            'end': df['date'].max()\n        }\n    }\n\n    log(f\"  \u2713 Revenue: ${metrics['total_revenue']:.2f}\")\n    return metrics\n\ndef main():\n    log(\"=== Data Pipeline Starting ===\")\n\n    # Discover input files\n    import glob\n    files = sorted(glob.glob('data/*.csv'))\n    log(f\"Found {len(files)} data files\")\n\n    # Process each file\n    results = {}\n    for filepath in files:\n        dataset_name = filepath.split('/')[-1].replace('.csv', '')\n        log(f\"\\n--- Processing {dataset_name} ---\")\n\n        try:\n            metrics = process_dataset(filepath)\n            results[dataset_name] = {\n                'status': 'success',\n                'metrics': metrics\n            }\n        except Exception as e:\n            log(f\"  \u274c Error: {e}\")\n            results[dataset_name] = {\n                'status': 'failed',\n                'error': str(e)\n            }\n\n    # Save results\n    log(\"\\n--- Saving Results ---\")\n\n    output = {\n        'timestamp': datetime.now().isoformat(),\n        'datasets_processed': len(results),\n        'datasets': results,\n        'summary': {\n            'successful': sum(1 for r in results.values() if r['status'] == 'success'),\n            'failed': sum(1 for r in results.values() if r['status'] == 'failed'),\n            'total_revenue': sum(\n                r['metrics']['total_revenue']\n                for r in results.values()\n                if r['status'] == 'success'\n            )\n        }\n    }\n\n    with open('pipeline_results.json', 'w') as f:\n        json.dump(output, f, indent=2)\n\n    log(f\"\u2713 Results saved to pipeline_results.json\")\n    log(f\"\\n=== Pipeline Complete ===\")\n    log(f\"  Processed: {output['summary']['successful']}/{len(results)} datasets\")\n    log(f\"  Total Revenue: ${output['summary']['total_revenue']:.2f}\")\n\nif __name__ == '__main__':\n    main()\nEOF\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-82-create-submission-script","title":"Step 8.2: Create Submission Script","text":"<pre><code>cat &gt; submit_pipeline.sh &lt;&lt;'EOF'\n#!/bin/bash\nset -e\n\necho \"=== Distributed Pipeline Submission ===\"\necho\n\n# Configuration\nPOOL_URL=\"http://localhost:9000\"\nPROJECT_DIR=\"complete-pipeline\"\n\n# Create manifest\ncat &gt; $PROJECT_DIR/job.json &lt;&lt;MANIFEST\n{\n  \"entrypoint\": \"pipeline.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"outputs\": [\"pipeline_results.json\"],\n  \"timeout\": 600,\n  \"memory_mb\": 1024\n}\nMANIFEST\n\n# Package project\necho \"Packaging project...\"\ntar czf pipeline.tar.gz -C $PROJECT_DIR .\n\n# Submit multiple jobs\nJOB_IDS=()\nNUM_JOBS=5\n\necho \"Submitting $NUM_JOBS jobs to pool...\"\nfor i in $(seq 1 $NUM_JOBS); do\n    RESPONSE=$(curl -s -X POST $POOL_URL/submit \\\n        -F \"files=@pipeline.tar.gz\" \\\n        -F \"manifest=$(cat $PROJECT_DIR/job.json)\")\n\n    JOB_ID=$(echo $RESPONSE | jq -r '.job_id')\n    JOB_IDS+=($JOB_ID)\n    echo \"  Job $i: $JOB_ID\"\ndone\n\necho\necho \"\u2713 Submitted $NUM_JOBS jobs\"\necho\n\n# Monitor pool status\necho \"Monitoring pool distribution...\"\nsleep 2\ncurl -s $POOL_URL/pool | jq '{\n    total_workers: .total_workers,\n    healthy_workers: .healthy_workers,\n    queued_jobs: .queued_jobs,\n    worker_loads: [.workers[] | {\n        worker: .worker_id[0:16],\n        active: .active_jobs,\n        max: .max_concurrent_jobs\n    }]\n}'\n\necho\necho \"Job IDs:\"\nprintf '%s\\n' \"${JOB_IDS[@]}\"\necho\necho \"To stream a job's logs:\"\necho \"  python stream_logs.py &lt;job_id&gt;\"\necho\necho \"To verify a job's results:\"\necho \"  python verify_result.py &lt;job_id&gt; &lt;worker_pubkey&gt;\"\nEOF\n\nchmod +x submit_pipeline.sh\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-83-copy-sample-data","title":"Step 8.3: Copy Sample Data","text":"<pre><code>cp -r multi-region/data complete-pipeline/\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-84-run-complete-pipeline","title":"Step 8.4: Run Complete Pipeline","text":"<pre><code># Submit jobs\n./submit_pipeline.sh\n</code></pre> <p>Output: <pre><code>=== Distributed Pipeline Submission ===\n\nPackaging project...\nSubmitting 5 jobs to pool...\n  Job 1: pool-a1b2c3d4\n  Job 2: pool-e5f6g7h8\n  Job 3: pool-i9j0k1l2\n  Job 4: pool-m3n4o5p6\n  Job 5: pool-q7r8s9t0\n\n\u2713 Submitted 5 jobs\n\nMonitoring pool distribution...\n{\n  \"total_workers\": 3,\n  \"healthy_workers\": 3,\n  \"queued_jobs\": 2,\n  \"worker_loads\": [\n    {\"worker\": \"J7X8K3mNqR4t\", \"active\": 2, \"max\": 4},\n    {\"worker\": \"K8Y9L4nOqS5u\", \"active\": 2, \"max\": 4},\n    {\"worker\": \"L9Z0M5oPrT6v\", \"active\": 1, \"max\": 4}\n  ]\n}\n\nJob IDs:\npool-a1b2c3d4\npool-e5f6g7h8\npool-i9j0k1l2\npool-m3n4o5p6\npool-q7r8s9t0\n\nTo stream a job's logs:\n  python stream_logs.py &lt;job_id&gt;\n\nTo verify a job's results:\n  python verify_result.py &lt;job_id&gt; &lt;worker_pubkey&gt;\n</code></pre></p>"},{"location":"tutorials/distributed-data-pipeline/#step-85-monitor-job-progress","title":"Step 8.5: Monitor Job Progress","text":"<pre><code># Stream first job\npython stream_logs.py pool-a1b2c3d4\n</code></pre>"},{"location":"tutorials/distributed-data-pipeline/#step-86-collect-and-verify-results","title":"Step 8.6: Collect and Verify Results","text":"<pre><code>cat &gt; collect_results.sh &lt;&lt;'EOF'\n#!/bin/bash\n\nPOOL_URL=\"http://localhost:9000\"\nJOB_IDS_FILE=\"pipeline_jobs.txt\"\n\necho \"=== Collecting Pipeline Results ===\"\necho\n\nmkdir -p results\n\nwhile IFS= read -r JOB_ID; do\n    echo \"Checking $JOB_ID...\"\n\n    # Get status\n    STATUS=$(curl -s $POOL_URL/status/$JOB_ID | jq -r '.pool_status')\n\n    if [ \"$STATUS\" = \"completed\" ]; then\n        echo \"  \u2713 Completed\"\n\n        # Download results\n        curl -s $POOL_URL/outputs/$JOB_ID/pipeline_results.json \\\n            -o results/${JOB_ID}_results.json\n\n        # Show summary\n        cat results/${JOB_ID}_results.json | jq '.summary'\n\n        # Verify signature (if available)\n        WORKER_ID=$(curl -s $POOL_URL/status/$JOB_ID | jq -r '.worker_id')\n        REMOTE_JOB=$(curl -s $POOL_URL/status/$JOB_ID | jq -r '.worker_status.job_id')\n\n        echo \"  Worker: ${WORKER_ID:0:16}...\"\n        echo\n    else\n        echo \"  Status: $STATUS\"\n        echo\n    fi\ndone &lt; \"$JOB_IDS_FILE\"\n\necho \"\u2713 Results collected in results/\"\nEOF\n\nchmod +x collect_results.sh\n\n# Save job IDs for collection\n./submit_pipeline.sh | grep \"pool-\" &gt; pipeline_jobs.txt\n\n# Wait for completion\nsleep 10\n\n# Collect results\n./collect_results.sh\n</code></pre> <p>Checkpoint 8 - Complete!</p> <p>You've built a production-ready distributed data processing pipeline with:</p> <ul> <li>\u2705 Distributed execution across multiple workers</li> <li>\u2705 Real-time progress monitoring via WebSocket</li> <li>\u2705 Cryptographic result verification</li> <li>\u2705 Automated job submission and collection</li> <li>\u2705 Error handling and status tracking</li> </ul>"},{"location":"tutorials/distributed-data-pipeline/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>Congratulations! You've learned to build distributed data pipelines with Sandrun.</p>"},{"location":"tutorials/distributed-data-pipeline/#what-youve-accomplished","title":"What You've Accomplished","text":"<ol> <li>Basic Execution - Ran Python scripts in isolated sandboxes</li> <li>Project Structure - Organized multi-file projects with dependencies</li> <li>Performance Optimization - Used pre-built environments for instant execution</li> <li>Batch Processing - Processed multiple datasets in parallel</li> <li>Distributed Computing - Scaled across worker pools with load balancing</li> <li>Real-Time Monitoring - Streamed logs via WebSocket</li> <li>Security - Verified results with cryptographic signatures</li> <li>Production Pipeline - Automated end-to-end workflows</li> </ol>"},{"location":"tutorials/distributed-data-pipeline/#key-takeaways","title":"Key Takeaways","text":"<p>Best Practices</p> <ul> <li>Use pre-built environments for faster execution (20x speedup)</li> <li>Structure projects with manifest files for reproducibility</li> <li>Monitor with WebSocket for long-running jobs</li> <li>Verify signatures for untrusted workers or compliance needs</li> <li>Distribute work across pools for horizontal scaling</li> <li>Handle errors gracefully with status checking and retries</li> </ul>"},{"location":"tutorials/distributed-data-pipeline/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>From this tutorial:</p> Metric Single Worker 3-Worker Pool Job Throughput 1 job/3s 3 jobs/3s Parallel Capacity 2 concurrent 12 concurrent Total Throughput 600 jobs/hour 1800 jobs/hour"},{"location":"tutorials/distributed-data-pipeline/#next-steps","title":"Next Steps","text":"<p>Ready to go deeper? Check out:</p> <ul> <li>API Reference - Complete endpoint documentation</li> <li>Job Manifest - Advanced configuration options</li> <li>MCP Integration - Give Claude AI code execution</li> <li>Security Model - Understand isolation guarantees</li> <li>Troubleshooting - Debug common issues</li> </ul>"},{"location":"tutorials/distributed-data-pipeline/#real-world-applications","title":"Real-World Applications","text":"<p>Apply what you've learned to:</p> <ul> <li>Data Science - Process large datasets in parallel</li> <li>CI/CD - Run test suites in isolated environments</li> <li>ML Training - Distribute training jobs with GPU workers</li> <li>LLM Integration - Give AI assistants safe code execution</li> <li>Privacy Computing - Process sensitive data without persistence</li> </ul>"},{"location":"tutorials/distributed-data-pipeline/#community","title":"Community","text":"<p>Questions or want to share your pipeline?</p> <ul> <li>GitHub: github.com/yourusername/sandrun</li> <li>Discussions: Share use cases and get help</li> <li>Issues: Report bugs or request features</li> </ul> <p>You're now ready to build production data pipelines with Sandrun! \ud83c\udf89</p>"},{"location":"tutorials/quick-reference/","title":"Tutorial Quick Reference","text":"<p>Quick command reference for the Distributed Data Pipeline Tutorial.</p>"},{"location":"tutorials/quick-reference/#basic-commands","title":"Basic Commands","text":""},{"location":"tutorials/quick-reference/#submit-job","title":"Submit Job","text":"<pre><code>tar czf job.tar.gz &lt;files&gt;\ncurl -X POST http://localhost:8443/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F \"manifest=$(cat job.json)\"\n</code></pre>"},{"location":"tutorials/quick-reference/#check-status","title":"Check Status","text":"<pre><code>curl http://localhost:8443/status/&lt;job_id&gt; | jq\n</code></pre>"},{"location":"tutorials/quick-reference/#get-logs","title":"Get Logs","text":"<pre><code>curl http://localhost:8443/logs/&lt;job_id&gt;\n</code></pre>"},{"location":"tutorials/quick-reference/#download-output","title":"Download Output","text":"<pre><code>curl http://localhost:8443/outputs/&lt;job_id&gt;/&lt;filename&gt; -o &lt;filename&gt;\n</code></pre>"},{"location":"tutorials/quick-reference/#job-manifest-templates","title":"Job Manifest Templates","text":""},{"location":"tutorials/quick-reference/#basic-python-job","title":"Basic Python Job","text":"<pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\"\n}\n</code></pre>"},{"location":"tutorials/quick-reference/#with-dependencies","title":"With Dependencies","text":"<pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"requirements\": \"requirements.txt\",\n  \"outputs\": [\"results/\", \"*.png\"]\n}\n</code></pre>"},{"location":"tutorials/quick-reference/#with-pre-built-environment","title":"With Pre-Built Environment","text":"<pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"outputs\": [\"report.txt\"]\n}\n</code></pre>"},{"location":"tutorials/quick-reference/#full-configuration","title":"Full Configuration","text":"<pre><code>{\n  \"entrypoint\": \"main.py\",\n  \"interpreter\": \"python3\",\n  \"environment\": \"ml-basic\",\n  \"args\": [\"--input\", \"data.csv\"],\n  \"outputs\": [\"results/\", \"*.json\"],\n  \"timeout\": 600,\n  \"memory_mb\": 1024\n}\n</code></pre>"},{"location":"tutorials/quick-reference/#pool-commands","title":"Pool Commands","text":""},{"location":"tutorials/quick-reference/#check-pool-status","title":"Check Pool Status","text":"<pre><code>curl http://localhost:9000/pool | jq\n</code></pre>"},{"location":"tutorials/quick-reference/#submit-to-pool","title":"Submit to Pool","text":"<pre><code>curl -X POST http://localhost:9000/submit \\\n  -F \"files=@job.tar.gz\" \\\n  -F \"manifest=$(cat job.json)\"\n</code></pre>"},{"location":"tutorials/quick-reference/#get-pool-job-status","title":"Get Pool Job Status","text":"<pre><code>curl http://localhost:9000/status/&lt;pool_job_id&gt; | jq\n</code></pre>"},{"location":"tutorials/quick-reference/#websocket-streaming","title":"WebSocket Streaming","text":""},{"location":"tutorials/quick-reference/#python-client","title":"Python Client","text":"<pre><code>import asyncio\nimport websockets\n\nasync def stream_logs(job_id):\n    uri = f\"ws://localhost:8443/logs/{job_id}/stream\"\n    async with websockets.connect(uri) as ws:\n        async for message in ws:\n            print(message, end='')\n\nasyncio.run(stream_logs(\"job-abc123\"))\n</code></pre>"},{"location":"tutorials/quick-reference/#javascript-client","title":"JavaScript Client","text":"<pre><code>const ws = new WebSocket('ws://localhost:8443/logs/job-abc123/stream');\nws.onmessage = (event) =&gt; console.log(event.data);\n</code></pre>"},{"location":"tutorials/quick-reference/#worker-setup","title":"Worker Setup","text":""},{"location":"tutorials/quick-reference/#generate-key","title":"Generate Key","text":"<pre><code>sudo ./build/sandrun --generate-key /etc/sandrun/worker.pem\n</code></pre>"},{"location":"tutorials/quick-reference/#start-worker","title":"Start Worker","text":"<pre><code>sudo ./build/sandrun --port 8443 --worker-key /etc/sandrun/worker.pem\n</code></pre>"},{"location":"tutorials/quick-reference/#start-pool-coordinator","title":"Start Pool Coordinator","text":"<pre><code>python coordinator.py --port 9000 --workers workers.json\n</code></pre>"},{"location":"tutorials/quick-reference/#useful-aliases","title":"Useful Aliases","text":"<p>Add these to your <code>~/.bashrc</code>:</p> <pre><code># Sandrun aliases\nalias sandrun-submit='curl -X POST http://localhost:8443/submit'\nalias sandrun-status='curl -s http://localhost:8443/status/'\nalias sandrun-logs='curl http://localhost:8443/logs/'\n\n# With jq formatting\nalias ss='sandrun-status \"$1\" | jq'\nalias sl='sandrun-logs \"$1\"'\n\n# Pool aliases\nalias pool-status='curl -s http://localhost:9000/pool | jq'\nalias pool-submit='curl -X POST http://localhost:9000/submit'\n</code></pre>"},{"location":"tutorials/quick-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"tutorials/quick-reference/#extract-job-id","title":"Extract Job ID","text":"<pre><code>RESPONSE=$(curl -s -X POST http://localhost:8443/submit ...)\nJOB_ID=$(echo $RESPONSE | jq -r '.job_id')\n</code></pre>"},{"location":"tutorials/quick-reference/#wait-for-completion","title":"Wait for Completion","text":"<pre><code>while true; do\n    STATUS=$(curl -s http://localhost:8443/status/$JOB_ID | jq -r '.status')\n    [ \"$STATUS\" = \"completed\" ] &amp;&amp; break\n    sleep 2\ndone\n</code></pre>"},{"location":"tutorials/quick-reference/#batch-submit","title":"Batch Submit","text":"<pre><code>for file in data/*.csv; do\n    # Create job for each file\n    tar czf \"job_$file.tar.gz\" \"$file\" process.py job.json\n    curl -X POST http://localhost:8443/submit \\\n        -F \"files=@job_$file.tar.gz\" \\\n        -F \"manifest=$(cat job.json)\"\ndone\n</code></pre>"},{"location":"tutorials/quick-reference/#environments","title":"Environments","text":"Name Packages Use Case <code>ml-basic</code> NumPy, Pandas, Scikit-learn Traditional ML <code>vision</code> PyTorch, OpenCV, Pillow Computer vision <code>nlp</code> PyTorch, Transformers NLP/LLMs <code>data-science</code> Pandas, Matplotlib, Jupyter Data analysis <code>scientific</code> NumPy, SciPy, SymPy Scientific computing"},{"location":"tutorials/quick-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/quick-reference/#check-server-health","title":"Check Server Health","text":"<pre><code>curl http://localhost:8443/health\n</code></pre>"},{"location":"tutorials/quick-reference/#check-quota","title":"Check Quota","text":"<pre><code>curl http://localhost:8443/stats | jq '.your_quota'\n</code></pre>"},{"location":"tutorials/quick-reference/#verify-worker-identity","title":"Verify Worker Identity","text":"<pre><code>curl http://localhost:8443/health | jq -r '.worker_id'\n</code></pre>"},{"location":"tutorials/quick-reference/#list-available-environments","title":"List Available Environments","text":"<pre><code>curl http://localhost:8443/environments | jq\n</code></pre>"},{"location":"tutorials/quick-reference/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use pre-built environments - 20x faster than pip install</li> <li>Specify outputs - Only download what you need</li> <li>Increase memory - For large datasets (up to 2048MB)</li> <li>Distribute work - Use pool for parallel processing</li> <li>Stream logs - Monitor long-running jobs in real-time</li> </ol>"},{"location":"tutorials/quick-reference/#security","title":"Security","text":""},{"location":"tutorials/quick-reference/#verify-result-signature","title":"Verify Result Signature","text":"<pre><code>from cryptography.hazmat.primitives.asymmetric import ed25519\nimport base64, json\n\ndef verify(pubkey_b64, signed_data, signature_b64):\n    pubkey_bytes = base64.b64decode(pubkey_b64)\n    pubkey = ed25519.Ed25519PublicKey.from_public_bytes(pubkey_bytes)\n    signature = base64.b64decode(signature_b64)\n    message = json.dumps(signed_data, sort_keys=True).encode()\n    pubkey.verify(signature, message)\n</code></pre>"},{"location":"tutorials/quick-reference/#resources","title":"Resources","text":"<ul> <li>Full Tutorial - Complete walkthrough</li> <li>API Reference - Endpoint documentation</li> <li>Job Manifest - Configuration options</li> <li>Troubleshooting - Common issues</li> </ul> <p>Print this page for quick reference during the tutorial!</p>"}]}